# Iteration 3 Log

## Metadata

- **Task ID**: client-onboarding-v2-c3y.105
- **Task Title**: Write unit tests for change detection
- **Description**: ERROR LOGGING REQUIREMENTS:
- Ensure test failures include full assertion context
- Log test setup/teardown at DEBUG level
- Capture and display logs from failed tests
- Include timing information in ...
- **Status**: completed
- **Task Completed**: Yes
- **Promise Detected**: Yes
- **Started At**: 2026-02-01T20:00:00.050Z
- **Ended At**: 2026-02-01T20:05:07.720Z
- **Duration**: 5m 7s
- **Agent**: claude
- **Sandbox Mode**: auto
- **Sandbox Network**: Enabled
--- RAW OUTPUT ---
{"type":"system","subtype":"init","cwd":"/Users/mike/Projects/client-onboarding-v2","session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","tools":["Task","TaskOutput","Bash","Glob","Grep","ExitPlanMode","Read","Edit","Write","NotebookEdit","WebFetch","TodoWrite","WebSearch","TaskStop","AskUserQuestion","Skill","EnterPlanMode","ToolSearch"],"mcp_servers":[],"model":"claude-opus-4-5-20251101","permissionMode":"bypassPermissions","slash_commands":["keybindings-help","frontend-design","openspec-apply-change","openspec-explore","openspec-verify-change","openspec-new-change","openspec-archive-change","openspec-onboard","openspec-bulk-archive-change","openspec-continue-change","openspec-sync-specs","openspec-ff-change","adapt","bolder","harden","extract","polish","quieter","critique","colorize","audit","clarify","simplify","onboard","optimize","animate","teach-impeccable","delight","normalize","opsx:explore","opsx:new","opsx:archive","opsx:bulk-archive","opsx:continue","opsx:apply","opsx:verify","opsx:onboard","opsx:sync","opsx:ff","ralph-loop:help","ralph-loop:cancel-ralph","ralph-loop:ralph-loop","compact","context","cost","init","pr-comments","release-notes","review","security-review"],"apiKeySource":"none","claude_code_version":"2.1.29","output_style":"default","agents":["Bash","general-purpose","statusline-setup","Explore","Plan"],"skills":["keybindings-help","frontend-design","openspec-apply-change","openspec-explore","openspec-verify-change","openspec-new-change","openspec-archive-change","openspec-onboard","openspec-bulk-archive-change","openspec-continue-change","openspec-sync-specs","openspec-ff-change"],"plugins":[{"name":"ralph-loop","path":"/Users/mike/.claude/plugins/cache/claude-plugins-official/ralph-loop/27d2b86d72da"}],"uuid":"dfbf2b04-080f-4db4-89c0-9ed71a9dd42f"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_013aSsf4zNGdgbsuVyYH7rij","type":"message","role":"assistant","content":[{"type":"text","text":"I'll start by studying the PRD context and progress file to understand the bigger picture, then implement the unit tests for change detection."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":7145,"cache_read_input_tokens":15267,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7145},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"f87025cc-f560-476e-b163-a71697f618eb"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_013aSsf4zNGdgbsuVyYH7rij","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EPkG92RXNc7Ee9cNnmHZkX","name":"Read","input":{"file_path":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":7145,"cache_read_input_tokens":15267,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7145},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"2cc43a8c-45f5-418b-ae49-64bbf9810ece"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EPkG92RXNc7Ee9cNnmHZkX","type":"tool_result","content":"     1→# Ralph Progress Log\n     2→\n     3→This file tracks progress across iterations. Agents update this file\n     4→after each iteration and it's included in prompts for context.\n     5→\n     6→## Codebase Patterns (Study These First)\n     7→\n     8→### Testing Patterns\n     9→- **Test file structure**: Tests organized by service/util in `tests/services/` and `tests/utils/`\n    10→- **Fixture pattern**: Per-class fixtures with `@pytest.fixture`, mock external clients with `AsyncMock`\n    11→- **Async testing**: Use `@pytest.mark.asyncio` decorator (auto mode enabled in pyproject.toml)\n    12→- **Mocking strategy**: Patch at module import source (`app.services.xxx.get_xxx_service`) not at usage point\n    13→- **Singleton testing**: Reset module-level `_xxx_service = None` before and after tests\n    14→- **Error logging verification**: Use `caplog` fixture with `caplog.at_level(logging.DEBUG/INFO)`\n    15→- **Coverage target**: 80% minimum, tests achieve 87% overall\n    16→\n    17→- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n    18→\n    19→---\n    20→\n    21→## 2026-02-01 - client-onboarding-v2-c3y.72\n    22→- What was implemented: Comprehensive unit tests for brand config synthesis service\n    23→- Files changed:\n    24→  - `backend/tests/services/test_brand_config.py` - New test file with 50 tests covering:\n    25→    - SynthesisResult dataclass validation\n    26→    - Exception classes (BrandConfigServiceError, ValidationError, NotFoundError, SynthesisError)\n    27→    - Schema merging (_merge_v2_schemas) with partial overrides\n    28→    - Document parsing (_parse_documents) with base64, failures, exceptions\n    29→    - V2 schema synthesis (synthesize_v2_schema) success, validation, Claude errors, JSON parsing\n    30→    - Synthesize and save (synthesize_and_save) create/update flows, DB errors\n    31→    - CRUD operations (get, list, update, delete) with ownership validation\n    32→    - Service factory function (get_brand_config_service)\n    33→    - Service initialization and helper methods\n    34→- **Learnings:**\n    35→  - Patterns discovered:\n    36→    - Brand config service uses dependency injection for Claude client and document parser\n    37→    - Synthesis result includes token usage metrics (input_tokens, output_tokens, request_id)\n    38→    - Schema merging uses deep merge for nested dicts (colors, typography, etc.)\n    39→    - Test coverage achieved: 93% for app.services.brand_config\n    40→  - Gotchas encountered:\n    41→    - Service logs use `extra={\"filename\": ...}` which conflicts with Python's reserved LogRecord attribute\n    42→    - Must patch `app.services.brand_config.logger` in tests that trigger warning/debug paths using filename\n    43→    - BrandConfigSynthesisRequest requires brand_name, tests with empty string need to use pydantic validation path\n    44→    - Markdown code block handling (`\\`\\`\\`json`) is tested for Claude responses\n    45→\n    46→---\n    47→\n    48→## 2026-02-01 - client-onboarding-v2-c3y.63\n    49→- What was implemented: Verified and fixed existing comprehensive unit tests for fan-out and categorization logic\n    50→- Files changed:\n    51→  - `backend/tests/utils/test_url_categorizer.py` - Fixed edge case test for relative paths without leading `/`\n    52→- **Learnings:**\n    53→  - Patterns discovered:\n    54→    - Tests already existed and were comprehensive (208 total tests for these modules)\n    55→    - Fan-out tests in `test_paa_enrichment.py` - TestEnrichKeywordFanout class\n    56→    - Categorization tests in `test_category_service.py`, `test_paa_categorization.py`, `test_url_categorizer.py`\n    57→    - Test docstrings include ERROR LOGGING REQUIREMENTS verification notes\n    58→  - Gotchas encountered:\n    59→    - `urlparse` treats relative paths without leading `/` ambiguously - they should have `/` prefix\n    60→    - Use `python3` directly (no poetry wrapper needed in this environment)\n    61→    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n    62→---\n    63→\n    64→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"ebf1ee98-32a0-4100-a236-0a11754ae3d5","tool_use_result":{"type":"text","file":{"filePath":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md","content":"# Ralph Progress Log\n\nThis file tracks progress across iterations. Agents update this file\nafter each iteration and it's included in prompts for context.\n\n## Codebase Patterns (Study These First)\n\n### Testing Patterns\n- **Test file structure**: Tests organized by service/util in `tests/services/` and `tests/utils/`\n- **Fixture pattern**: Per-class fixtures with `@pytest.fixture`, mock external clients with `AsyncMock`\n- **Async testing**: Use `@pytest.mark.asyncio` decorator (auto mode enabled in pyproject.toml)\n- **Mocking strategy**: Patch at module import source (`app.services.xxx.get_xxx_service`) not at usage point\n- **Singleton testing**: Reset module-level `_xxx_service = None` before and after tests\n- **Error logging verification**: Use `caplog` fixture with `caplog.at_level(logging.DEBUG/INFO)`\n- **Coverage target**: 80% minimum, tests achieve 87% overall\n\n- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.72\n- What was implemented: Comprehensive unit tests for brand config synthesis service\n- Files changed:\n  - `backend/tests/services/test_brand_config.py` - New test file with 50 tests covering:\n    - SynthesisResult dataclass validation\n    - Exception classes (BrandConfigServiceError, ValidationError, NotFoundError, SynthesisError)\n    - Schema merging (_merge_v2_schemas) with partial overrides\n    - Document parsing (_parse_documents) with base64, failures, exceptions\n    - V2 schema synthesis (synthesize_v2_schema) success, validation, Claude errors, JSON parsing\n    - Synthesize and save (synthesize_and_save) create/update flows, DB errors\n    - CRUD operations (get, list, update, delete) with ownership validation\n    - Service factory function (get_brand_config_service)\n    - Service initialization and helper methods\n- **Learnings:**\n  - Patterns discovered:\n    - Brand config service uses dependency injection for Claude client and document parser\n    - Synthesis result includes token usage metrics (input_tokens, output_tokens, request_id)\n    - Schema merging uses deep merge for nested dicts (colors, typography, etc.)\n    - Test coverage achieved: 93% for app.services.brand_config\n  - Gotchas encountered:\n    - Service logs use `extra={\"filename\": ...}` which conflicts with Python's reserved LogRecord attribute\n    - Must patch `app.services.brand_config.logger` in tests that trigger warning/debug paths using filename\n    - BrandConfigSynthesisRequest requires brand_name, tests with empty string need to use pydantic validation path\n    - Markdown code block handling (`\\`\\`\\`json`) is tested for Claude responses\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.63\n- What was implemented: Verified and fixed existing comprehensive unit tests for fan-out and categorization logic\n- Files changed:\n  - `backend/tests/utils/test_url_categorizer.py` - Fixed edge case test for relative paths without leading `/`\n- **Learnings:**\n  - Patterns discovered:\n    - Tests already existed and were comprehensive (208 total tests for these modules)\n    - Fan-out tests in `test_paa_enrichment.py` - TestEnrichKeywordFanout class\n    - Categorization tests in `test_category_service.py`, `test_paa_categorization.py`, `test_url_categorizer.py`\n    - Test docstrings include ERROR LOGGING REQUIREMENTS verification notes\n  - Gotchas encountered:\n    - `urlparse` treats relative paths without leading `/` ambiguously - they should have `/` prefix\n    - Use `python3` directly (no poetry wrapper needed in this environment)\n    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n","numLines":64,"startLine":1,"totalLines":64}}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_013aSsf4zNGdgbsuVyYH7rij","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BBTwBY3TGKnsdy8BExtxq7","name":"Read","input":{"file_path":"/Users/mike/Projects/client-onboarding-v2/prd.json"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":7145,"cache_read_input_tokens":15267,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":7145},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"e0514563-c0ee-49fd-82d1-fe22393ae164"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BBTwBY3TGKnsdy8BExtxq7","type":"tool_result","content":"     1→{\n     2→  \"name\": \"Client Onboarding V2\",\n     3→  \"description\": \"Ready tasks batch\",\n     4→  \"userStories\": [\n     5→    {\n     6→      \"id\": \"client-onboarding-v2-c3y.63\",\n     7→      \"title\": \"Write unit tests for fan-out and categorization logic\",\n     8→      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Ensure test failures include full assertion context\\n- Log test setup/teardown at DEBUG level\\n- Capture and display logs from failed tests\\n- Include timing information in test reports\\n- Log mock/stub invocations for debugging\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Tests must work with DATABASE_URL env var\\n- Use test database, not production\\n- CI should mirror Railway environment where possible\",\n     9→      \"acceptanceCriteria\": [],\n    10→      \"passes\": true,\n    11→      \"completionNotes\": \"Completed by agent\"\n    12→    },\n    13→    {\n    14→      \"id\": \"client-onboarding-v2-c3y.72\",\n    15→      \"title\": \"Write unit tests for brand config synthesis\",\n    16→      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Ensure test failures include full assertion context\\n- Log test setup/teardown at DEBUG level\\n- Capture and display logs from failed tests\\n- Include timing information in test reports\\n- Log mock/stub invocations for debugging\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Tests must work with DATABASE_URL env var\\n- Use test database, not production\\n- CI should mirror Railway environment where possible\",\n    17→      \"acceptanceCriteria\": [],\n    18→      \"passes\": true,\n    19→      \"completionNotes\": \"Completed by agent\"\n    20→    },\n    21→    {\n    22→      \"id\": \"client-onboarding-v2-c3y.105\",\n    23→      \"title\": \"Write unit tests for change detection\",\n    24→      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Ensure test failures include full assertion context\\n- Log test setup/teardown at DEBUG level\\n- Capture and display logs from failed tests\\n- Include timing information in test reports\\n- Log mock/stub invocations for debugging\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Tests must work with DATABASE_URL env var\\n- Use test database, not production\\n- CI should mirror Railway environment where possible\",\n    25→      \"acceptanceCriteria\": [],\n    26→      \"passes\": false\n    27→    },\n    28→    {\n    29→      \"id\": \"client-onboarding-v2-c3y.114\",\n    30→      \"title\": \"Create ProjectCard component for project list\",\n    31→      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Implement React Error Boundaries for all route components\\n- Log caught errors to console with component stack\\n- Set up global window.onerror and unhandledrejection handlers\\n- Log API errors with endpoint, status, response body\\n- Include user action context in error logs\\n- Set up error reporting service integration point (e.g., Sentry stub)\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Build as static assets (npm run build)\\n- API URL via VITE_API_URL environment variable\\n- Configure for production build (NODE_ENV=production)\\n- Can deploy to Railway static hosting or separate service\\n- Use relative paths or env-configured absolute URLs\",\n    32→      \"acceptanceCriteria\": [],\n    33→      \"passes\": false\n    34→    }\n    35→  ],\n    36→  \"metadata\": {\n    37→    \"updatedAt\": \"2026-02-01T20:00:00.050Z\"\n    38→  }\n    39→}\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"8529910a-530c-489e-b37c-5ecd7494231b","tool_use_result":{"type":"text","file":{"filePath":"/Users/mike/Projects/client-onboarding-v2/prd.json","content":"{\n  \"name\": \"Client Onboarding V2\",\n  \"description\": \"Ready tasks batch\",\n  \"userStories\": [\n    {\n      \"id\": \"client-onboarding-v2-c3y.63\",\n      \"title\": \"Write unit tests for fan-out and categorization logic\",\n      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Ensure test failures include full assertion context\\n- Log test setup/teardown at DEBUG level\\n- Capture and display logs from failed tests\\n- Include timing information in test reports\\n- Log mock/stub invocations for debugging\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Tests must work with DATABASE_URL env var\\n- Use test database, not production\\n- CI should mirror Railway environment where possible\",\n      \"acceptanceCriteria\": [],\n      \"passes\": true,\n      \"completionNotes\": \"Completed by agent\"\n    },\n    {\n      \"id\": \"client-onboarding-v2-c3y.72\",\n      \"title\": \"Write unit tests for brand config synthesis\",\n      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Ensure test failures include full assertion context\\n- Log test setup/teardown at DEBUG level\\n- Capture and display logs from failed tests\\n- Include timing information in test reports\\n- Log mock/stub invocations for debugging\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Tests must work with DATABASE_URL env var\\n- Use test database, not production\\n- CI should mirror Railway environment where possible\",\n      \"acceptanceCriteria\": [],\n      \"passes\": true,\n      \"completionNotes\": \"Completed by agent\"\n    },\n    {\n      \"id\": \"client-onboarding-v2-c3y.105\",\n      \"title\": \"Write unit tests for change detection\",\n      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Ensure test failures include full assertion context\\n- Log test setup/teardown at DEBUG level\\n- Capture and display logs from failed tests\\n- Include timing information in test reports\\n- Log mock/stub invocations for debugging\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Tests must work with DATABASE_URL env var\\n- Use test database, not production\\n- CI should mirror Railway environment where possible\",\n      \"acceptanceCriteria\": [],\n      \"passes\": false\n    },\n    {\n      \"id\": \"client-onboarding-v2-c3y.114\",\n      \"title\": \"Create ProjectCard component for project list\",\n      \"description\": \"ERROR LOGGING REQUIREMENTS:\\n- Implement React Error Boundaries for all route components\\n- Log caught errors to console with component stack\\n- Set up global window.onerror and unhandledrejection handlers\\n- Log API errors with endpoint, status, response body\\n- Include user action context in error logs\\n- Set up error reporting service integration point (e.g., Sentry stub)\\n\\nRAILWAY DEPLOYMENT REQUIREMENTS:\\n- Build as static assets (npm run build)\\n- API URL via VITE_API_URL environment variable\\n- Configure for production build (NODE_ENV=production)\\n- Can deploy to Railway static hosting or separate service\\n- Use relative paths or env-configured absolute URLs\",\n      \"acceptanceCriteria\": [],\n      \"passes\": false\n    }\n  ],\n  \"metadata\": {\n    \"updatedAt\": \"2026-02-01T20:00:00.050Z\"\n  }\n}","numLines":39,"startLine":1,"totalLines":39}}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01AMgFFXFT1nrQfZdwCxobdF","type":"message","role":"assistant","content":[{"type":"text","text":"Now I need to find the change detection code to write tests for. Let me search for change detection-related files."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2531,"cache_read_input_tokens":22412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2531},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"4e31d79e-76fd-4dc0-820e-8c66732725d7"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01AMgFFXFT1nrQfZdwCxobdF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TD2pwc9kCSrdeqNEBYHNdi","name":"Grep","input":{"pattern":"change.?detect|detect.?change","-i":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2531,"cache_read_input_tokens":22412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2531},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"3590394b-4c21-43ec-9c25-d2ee5e63f9a2"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01AMgFFXFT1nrQfZdwCxobdF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XiM6sbksFr2rLHLJcY6JrK","name":"Glob","input":{"pattern":"backend/**/*change*"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2531,"cache_read_input_tokens":22412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2531},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"b1698217-7dbf-4e2e-9ea8-c1ee1ccea8d0"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TD2pwc9kCSrdeqNEBYHNdi","type":"tool_result","content":"Found 51 files\n.ralph-tui/session.json\nprd.json\n.ralph-tui/iterations/53b946cf_2026-02-01_14-54-25_client-onboarding-v2-c3y.72.log\n.beads/issues.jsonl\n.ralph-tui/iterations/53b946cf_2026-02-01_14-51-28_client-onboarding-v2-c3y.63.log\n.beads/beads.jsonl\n.ralph-tui/iterations/81f43047_2026-02-01_14-49-10_client-onboarding-v2-c3y.104.log\n.ralph-tui/iterations/81f43047_2026-02-01_14-41-13_client-onboarding-v2-c3y.62.log\n.ralph-tui/iterations/2ea7278c_2026-02-01_14-24-49_client-onboarding-v2-c3y.61.log\n.ralph-tui/iterations/dcf043b7_2026-02-01_14-08-32_client-onboarding-v2-c3y.102.log\nbackend/app/schemas/crawl.py\n.ralph-tui/iterations/dcf043b7_2026-02-01_14-01-29_client-onboarding-v2-c3y.69.log\n.ralph-tui/iterations/5d96f1b5_2026-02-01_13-43-53_client-onboarding-v2-c3y.101.log\n.ralph-tui/iterations/5d96f1b5_2026-02-01_13-35-00_client-onboarding-v2-c3y.68.log\n.ralph-tui/iterations/6c8f3898_2026-02-01_13-27-44_client-onboarding-v2-c3y.109.log\n.ralph-tui/iterations/6c8f3898_2026-02-01_13-22-21_client-onboarding-v2-c3y.100.log\nbackend/tests/utils/test_change_detection.py\nbackend/app/utils/__init__.py\nbackend/app/utils/change_detection.py\n.ralph-tui/iterations/6c8f3898_2026-02-01_13-15-32_client-onboarding-v2-c3y.67.log\n.ralph-tui/iterations/6c8f3898_2026-02-01_13-13-03_client-onboarding-v2-c3y.58.log\n.ralph-tui/iterations/506ff53d_2026-02-01_11-46-21_client-onboarding-v2-c3y.99.log\n.ralph-tui/iterations/506ff53d_2026-02-01_11-39-04_client-onboarding-v2-c3y.65.log\n.ralph-tui/iterations/bb3029ca_2026-02-01_11-02-14_client-onboarding-v2-c3y.54.log\n.ralph-tui/iterations/bb3029ca_2026-02-01_10-41-51_client-onboarding-v2-c3y.48.log\n.ralph-tui/iterations/bb3029ca_2026-02-01_10-01-37_client-onboarding-v2-c3y.42.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-50-27_client-onboarding-v2-c3y.42.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-47-06_client-onboarding-v2-c3y.41.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-43-10_client-onboarding-v2-c3y.40.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-38-39_client-onboarding-v2-c3y.39.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-33-06_client-onboarding-v2-c3y.38.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-29-07_client-onboarding-v2-c3y.37.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-25-59_client-onboarding-v2-c3y.36.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-20-16_client-onboarding-v2-c3y.35.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-13-58_client-onboarding-v2-c3y.34.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-08-30_client-onboarding-v2-c3y.33.log\n.ralph-tui/iterations/accc7d27_2026-02-01_09-04-18_client-onboarding-v2-c3y.32.log\n.ralph-tui/iterations/accc7d27_2026-02-01_08-59-46_client-onboarding-v2-c3y.31.log\n.ralph-tui/iterations/accc7d27_2026-02-01_08-56-12_client-onboarding-v2-c3y.30.log\n.ralph-tui/iterations/accc7d27_2026-02-01_08-53-23_client-onboarding-v2-c3y.29.log\n.ralph-tui/iterations/accc7d27_2026-02-01_08-49-55_client-onboarding-v2-c3y.28.log\nbackend/app/repositories/crawl.py\n.ralph-tui/iterations/accc7d27_2026-02-01_08-45-50_client-onboarding-v2-c3y.27.log\n.ralph-tui/iterations/accc7d27_2026-02-01_08-37-53_client-onboarding-v2-c3y.25.log\n.ralph-tui/iterations/accc7d27_2026-02-01_08-07-41_client-onboarding-v2-c3y.15.log\n.ralph-tui/iterations/accc7d27_2026-02-01_08-05-52_client-onboarding-v2-c3y.14.log\n.ralph-tui/iterations/accc7d27_2026-01-31_18-32-07_client-onboarding-v2-c3y.11.log\n.ralph-tui/iterations/accc7d27_2026-01-31_18-28-53_client-onboarding-v2-c3y.10.log\nbackend/app/models/crawled_page.py\nopenspec/changes/client-fulfillment-app-v2-rebuild/tasks.md\nopenspec/changes/client-fulfillment-app-v2-rebuild/specs/scheduled-crawls.md"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"87217322-b8ed-4f72-b872-bb2e59413d81","tool_use_result":{"mode":"files_with_matches","filenames":[".ralph-tui/session.json","prd.json",".ralph-tui/iterations/53b946cf_2026-02-01_14-54-25_client-onboarding-v2-c3y.72.log",".beads/issues.jsonl",".ralph-tui/iterations/53b946cf_2026-02-01_14-51-28_client-onboarding-v2-c3y.63.log",".beads/beads.jsonl",".ralph-tui/iterations/81f43047_2026-02-01_14-49-10_client-onboarding-v2-c3y.104.log",".ralph-tui/iterations/81f43047_2026-02-01_14-41-13_client-onboarding-v2-c3y.62.log",".ralph-tui/iterations/2ea7278c_2026-02-01_14-24-49_client-onboarding-v2-c3y.61.log",".ralph-tui/iterations/dcf043b7_2026-02-01_14-08-32_client-onboarding-v2-c3y.102.log","backend/app/schemas/crawl.py",".ralph-tui/iterations/dcf043b7_2026-02-01_14-01-29_client-onboarding-v2-c3y.69.log",".ralph-tui/iterations/5d96f1b5_2026-02-01_13-43-53_client-onboarding-v2-c3y.101.log",".ralph-tui/iterations/5d96f1b5_2026-02-01_13-35-00_client-onboarding-v2-c3y.68.log",".ralph-tui/iterations/6c8f3898_2026-02-01_13-27-44_client-onboarding-v2-c3y.109.log",".ralph-tui/iterations/6c8f3898_2026-02-01_13-22-21_client-onboarding-v2-c3y.100.log","backend/tests/utils/test_change_detection.py","backend/app/utils/__init__.py","backend/app/utils/change_detection.py",".ralph-tui/iterations/6c8f3898_2026-02-01_13-15-32_client-onboarding-v2-c3y.67.log",".ralph-tui/iterations/6c8f3898_2026-02-01_13-13-03_client-onboarding-v2-c3y.58.log",".ralph-tui/iterations/506ff53d_2026-02-01_11-46-21_client-onboarding-v2-c3y.99.log",".ralph-tui/iterations/506ff53d_2026-02-01_11-39-04_client-onboarding-v2-c3y.65.log",".ralph-tui/iterations/bb3029ca_2026-02-01_11-02-14_client-onboarding-v2-c3y.54.log",".ralph-tui/iterations/bb3029ca_2026-02-01_10-41-51_client-onboarding-v2-c3y.48.log",".ralph-tui/iterations/bb3029ca_2026-02-01_10-01-37_client-onboarding-v2-c3y.42.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-50-27_client-onboarding-v2-c3y.42.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-47-06_client-onboarding-v2-c3y.41.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-43-10_client-onboarding-v2-c3y.40.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-38-39_client-onboarding-v2-c3y.39.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-33-06_client-onboarding-v2-c3y.38.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-29-07_client-onboarding-v2-c3y.37.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-25-59_client-onboarding-v2-c3y.36.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-20-16_client-onboarding-v2-c3y.35.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-13-58_client-onboarding-v2-c3y.34.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-08-30_client-onboarding-v2-c3y.33.log",".ralph-tui/iterations/accc7d27_2026-02-01_09-04-18_client-onboarding-v2-c3y.32.log",".ralph-tui/iterations/accc7d27_2026-02-01_08-59-46_client-onboarding-v2-c3y.31.log",".ralph-tui/iterations/accc7d27_2026-02-01_08-56-12_client-onboarding-v2-c3y.30.log",".ralph-tui/iterations/accc7d27_2026-02-01_08-53-23_client-onboarding-v2-c3y.29.log",".ralph-tui/iterations/accc7d27_2026-02-01_08-49-55_client-onboarding-v2-c3y.28.log","backend/app/repositories/crawl.py",".ralph-tui/iterations/accc7d27_2026-02-01_08-45-50_client-onboarding-v2-c3y.27.log",".ralph-tui/iterations/accc7d27_2026-02-01_08-37-53_client-onboarding-v2-c3y.25.log",".ralph-tui/iterations/accc7d27_2026-02-01_08-07-41_client-onboarding-v2-c3y.15.log",".ralph-tui/iterations/accc7d27_2026-02-01_08-05-52_client-onboarding-v2-c3y.14.log",".ralph-tui/iterations/accc7d27_2026-01-31_18-32-07_client-onboarding-v2-c3y.11.log",".ralph-tui/iterations/accc7d27_2026-01-31_18-28-53_client-onboarding-v2-c3y.10.log","backend/app/models/crawled_page.py","openspec/changes/client-fulfillment-app-v2-rebuild/tasks.md","openspec/changes/client-fulfillment-app-v2-rebuild/specs/scheduled-crawls.md"],"numFiles":51}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01AMgFFXFT1nrQfZdwCxobdF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012wrBkBodmgrrKf3u8nveEs","name":"Glob","input":{"pattern":"backend/**/*detect*"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2531,"cache_read_input_tokens":22412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2531},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"815c33a7-6321-4d4b-a136-aa4c321ca25e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XiM6sbksFr2rLHLJcY6JrK","type":"tool_result","content":"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py\n/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py\n/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/__pycache__/test_change_detection.cpython-312-pytest-9.0.2.pyc\n/Users/mike/Projects/client-onboarding-v2/backend/app/utils/__pycache__/change_detection.cpython-312.pyc\n/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/sqlalchemy/orm/state_changes.data.json\n/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/sqlalchemy/orm/state_changes.meta.json\n/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.data.json\n/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.meta.json"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"20863b8b-8a69-4572-8c35-514066b4a4a2","tool_use_result":{"filenames":["/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py","/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/__pycache__/test_change_detection.cpython-312-pytest-9.0.2.pyc","/Users/mike/Projects/client-onboarding-v2/backend/app/utils/__pycache__/change_detection.cpython-312.pyc","/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/sqlalchemy/orm/state_changes.data.json","/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/sqlalchemy/orm/state_changes.meta.json","/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.data.json","/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.meta.json"],"durationMs":759,"numFiles":8,"truncated":false}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012wrBkBodmgrrKf3u8nveEs","type":"tool_result","content":"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py\n/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py\n/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/__pycache__/test_change_detection.cpython-312-pytest-9.0.2.pyc\n/Users/mike/Projects/client-onboarding-v2/backend/app/utils/__pycache__/change_detection.cpython-312.pyc\n/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.data.json\n/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.meta.json"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"e81d9566-074b-41cd-8778-afb863d81825","tool_use_result":{"filenames":["/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py","/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/__pycache__/test_change_detection.cpython-312-pytest-9.0.2.pyc","/Users/mike/Projects/client-onboarding-v2/backend/app/utils/__pycache__/change_detection.cpython-312.pyc","/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.data.json","/Users/mike/Projects/client-onboarding-v2/backend/.mypy_cache/3.11/app/utils/change_detection.meta.json"],"durationMs":695,"numFiles":6,"truncated":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01RxdYz6Yi875iBSvgTpJj4d","type":"message","role":"assistant","content":[{"type":"text","text":"There's already a test file for change detection. Let me read both the source and existing test file to understand what's already tested and what might need additional coverage."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2664,"cache_read_input_tokens":24943,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2664},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"4998d4dd-1392-437f-8b60-e0d2ea5e2cc3"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01RxdYz6Yi875iBSvgTpJj4d","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GAKWYmVEUwUANdjntsfRFQ","name":"Read","input":{"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2664,"cache_read_input_tokens":24943,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2664},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"8dce8dca-aa14-4984-b189-d9aac291f025"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GAKWYmVEUwUANdjntsfRFQ","type":"tool_result","content":"     1→\"\"\"Change detection algorithm for comparing crawl results.\n     2→\n     3→Implements content hash comparison to detect changes between crawls:\n     4→- Content hash: Computed from semantic content (title, h1, meta_description, body text)\n     5→- Page comparison: Match by normalized URL, classify as new/removed/changed/unchanged\n     6→- Change summary: Aggregate statistics with significance detection\n     7→\n     8→Significance threshold: 5+ new pages OR 10%+ content changes.\n     9→\n    10→ERROR LOGGING REQUIREMENTS:\n    11→- Log method entry/exit at DEBUG level with parameters (sanitized)\n    12→- Log all exceptions with full stack trace and context\n    13→- Include entity IDs (project_id, page_id) in all service logs\n    14→- Log validation failures with field names and rejected values\n    15→- Log state transitions (phase changes) at INFO level\n    16→- Add timing logs for operations >1 second\n    17→\"\"\"\n    18→\n    19→import hashlib\n    20→import time\n    21→import traceback\n    22→from dataclasses import dataclass, field\n    23→from datetime import datetime\n    24→from enum import Enum\n    25→from typing import Any\n    26→\n    27→from app.core.logging import get_logger\n    28→\n    29→logger = get_logger(\"change_detection\")\n    30→\n    31→# Threshold for logging slow operations (in milliseconds)\n    32→SLOW_OPERATION_THRESHOLD_MS = 1000\n    33→\n    34→# Default significance thresholds\n    35→DEFAULT_NEW_PAGE_THRESHOLD = 5\n    36→DEFAULT_CHANGE_PERCENTAGE_THRESHOLD = 0.10  # 10%\n    37→\n    38→# Maximum content length for hashing (to prevent memory issues)\n    39→MAX_CONTENT_LENGTH_FOR_HASH = 5000\n    40→\n    41→\n    42→class ChangeType(Enum):\n    43→    \"\"\"Types of changes detected between crawls.\"\"\"\n    44→\n    45→    NEW = \"new\"\n    46→    REMOVED = \"removed\"\n    47→    CHANGED = \"changed\"\n    48→    UNCHANGED = \"unchanged\"\n    49→\n    50→\n    51→@dataclass\n    52→class PageSnapshot:\n    53→    \"\"\"Snapshot of a page at a point in time for comparison.\n    54→\n    55→    Attributes:\n    56→        url: Normalized URL of the page\n    57→        content_hash: Hash of semantic content\n    58→        title: Page title (optional, for context in reports)\n    59→        page_id: Database page ID (optional)\n    60→    \"\"\"\n    61→\n    62→    url: str\n    63→    content_hash: str | None\n    64→    title: str | None = None\n    65→    page_id: str | None = None\n    66→\n    67→    def to_dict(self) -> dict[str, Any]:\n    68→        \"\"\"Convert to dictionary for serialization.\"\"\"\n    69→        return {\n    70→            \"url\": self.url,\n    71→            \"content_hash\": self.content_hash,\n    72→            \"title\": self.title[:100] if self.title else None,\n    73→            \"page_id\": self.page_id,\n    74→        }\n    75→\n    76→\n    77→@dataclass\n    78→class PageChange:\n    79→    \"\"\"Represents a detected change for a single page.\n    80→\n    81→    Attributes:\n    82→        url: Normalized URL of the page\n    83→        change_type: Type of change (new, removed, changed, unchanged)\n    84→        old_hash: Previous content hash (for changed/removed)\n    85→        new_hash: New content hash (for new/changed)\n    86→        title: Page title (optional)\n    87→        page_id: Database page ID (optional)\n    88→    \"\"\"\n    89→\n    90→    url: str\n    91→    change_type: ChangeType\n    92→    old_hash: str | None = None\n    93→    new_hash: str | None = None\n    94→    title: str | None = None\n    95→    page_id: str | None = None\n    96→\n    97→    def to_dict(self) -> dict[str, Any]:\n    98→        \"\"\"Convert to dictionary for serialization.\"\"\"\n    99→        return {\n   100→            \"url\": self.url,\n   101→            \"change_type\": self.change_type.value,\n   102→            \"old_hash\": self.old_hash,\n   103→            \"new_hash\": self.new_hash,\n   104→            \"title\": self.title[:100] if self.title else None,\n   105→            \"page_id\": self.page_id,\n   106→        }\n   107→\n   108→\n   109→@dataclass\n   110→class ChangeSummary:\n   111→    \"\"\"Summary of changes detected between two crawls.\n   112→\n   113→    Attributes:\n   114→        crawl_id: ID of the new crawl\n   115→        compared_to: ID of the previous crawl being compared to\n   116→        new_pages: Count of new pages found\n   117→        removed_pages: Count of pages no longer present\n   118→        changed_pages: Count of pages with content changes\n   119→        unchanged_pages: Count of pages without changes\n   120→        new_page_urls: List of URLs for new pages\n   121→        removed_page_urls: List of URLs for removed pages\n   122→        changed_page_urls: List of URLs for changed pages\n   123→        is_significant: Whether changes meet significance threshold\n   124→        changes: Detailed list of all page changes\n   125→        computed_at: When the comparison was performed\n   126→    \"\"\"\n   127→\n   128→    crawl_id: str\n   129→    compared_to: str\n   130→    new_pages: int = 0\n   131→    removed_pages: int = 0\n   132→    changed_pages: int = 0\n   133→    unchanged_pages: int = 0\n   134→    new_page_urls: list[str] = field(default_factory=list)\n   135→    removed_page_urls: list[str] = field(default_factory=list)\n   136→    changed_page_urls: list[str] = field(default_factory=list)\n   137→    is_significant: bool = False\n   138→    changes: list[PageChange] = field(default_factory=list)\n   139→    computed_at: datetime | None = None\n   140→\n   141→    def __post_init__(self) -> None:\n   142→        \"\"\"Set computed_at if not provided.\"\"\"\n   143→        if self.computed_at is None:\n   144→            from datetime import UTC\n   145→\n   146→            self.computed_at = datetime.now(UTC)\n   147→\n   148→    @property\n   149→    def total_pages(self) -> int:\n   150→        \"\"\"Total pages in the new crawl.\"\"\"\n   151→        return self.new_pages + self.changed_pages + self.unchanged_pages\n   152→\n   153→    @property\n   154→    def total_changes(self) -> int:\n   155→        \"\"\"Total number of changes (new + removed + changed).\"\"\"\n   156→        return self.new_pages + self.removed_pages + self.changed_pages\n   157→\n   158→    @property\n   159→    def change_percentage(self) -> float:\n   160→        \"\"\"Percentage of pages that changed (for pages that existed in both crawls).\"\"\"\n   161→        comparable_pages = self.changed_pages + self.unchanged_pages\n   162→        if comparable_pages == 0:\n   163→            return 0.0\n   164→        return self.changed_pages / comparable_pages\n   165→\n   166→    def to_dict(self) -> dict[str, Any]:\n   167→        \"\"\"Convert to dictionary for serialization.\n   168→\n   169→        Matches the JSON schema from the spec.\n   170→        \"\"\"\n   171→        return {\n   172→            \"crawl_id\": self.crawl_id,\n   173→            \"compared_to\": self.compared_to,\n   174→            \"summary\": {\n   175→                \"new_pages\": self.new_pages,\n   176→                \"removed_pages\": self.removed_pages,\n   177→                \"changed_pages\": self.changed_pages,\n   178→                \"unchanged_pages\": self.unchanged_pages,\n   179→            },\n   180→            \"new_page_urls\": self.new_page_urls[:100],  # Limit for large changes\n   181→            \"removed_page_urls\": self.removed_page_urls[:100],\n   182→            \"changed_page_urls\": self.changed_page_urls[:100],\n   183→            \"is_significant\": self.is_significant,\n   184→            \"computed_at\": self.computed_at.isoformat() if self.computed_at else None,\n   185→            \"total_pages\": self.total_pages,\n   186→            \"total_changes\": self.total_changes,\n   187→            \"change_percentage\": round(self.change_percentage, 4),\n   188→        }\n   189→\n   190→\n   191→class ContentHasher:\n   192→    \"\"\"Computes content hashes for change detection.\n   193→\n   194→    Creates semantic content hashes from page content fields:\n   195→    - title: Page title\n   196→    - h1: Primary heading\n   197→    - meta_description: Meta description\n   198→    - body_text: First N characters of body content\n   199→\n   200→    Uses MD5 for fast comparison (not cryptographic security).\n   201→\n   202→    Example:\n   203→        hasher = ContentHasher()\n   204→        hash1 = hasher.compute_hash(\n   205→            title=\"Product Name\",\n   206→            h1=\"Product Name\",\n   207→            meta_description=\"Buy our product\",\n   208→            body_text=\"Product details...\",\n   209→        )\n   210→    \"\"\"\n   211→\n   212→    def __init__(\n   213→        self,\n   214→        max_content_length: int = MAX_CONTENT_LENGTH_FOR_HASH,\n   215→        include_whitespace: bool = False,\n   216→    ) -> None:\n   217→        \"\"\"Initialize the content hasher.\n   218→\n   219→        Args:\n   220→            max_content_length: Maximum body text length to include in hash\n   221→            include_whitespace: If False, normalize whitespace before hashing\n   222→        \"\"\"\n   223→        logger.debug(\n   224→            \"ContentHasher.__init__ called\",\n   225→            extra={\n   226→                \"max_content_length\": max_content_length,\n   227→                \"include_whitespace\": include_whitespace,\n   228→            },\n   229→        )\n   230→        self._max_content_length = max_content_length\n   231→        self._include_whitespace = include_whitespace\n   232→\n   233→    @property\n   234→    def max_content_length(self) -> int:\n   235→        \"\"\"Get the maximum content length for hashing.\"\"\"\n   236→        return self._max_content_length\n   237→\n   238→    def _normalize_text(self, text: str | None) -> str:\n   239→        \"\"\"Normalize text for consistent hashing.\n   240→\n   241→        Args:\n   242→            text: Text to normalize\n   243→\n   244→        Returns:\n   245→            Normalized text (lowercased, whitespace normalized)\n   246→        \"\"\"\n   247→        if not text:\n   248→            return \"\"\n   249→\n   250→        normalized = text.strip().lower()\n   251→\n   252→        if not self._include_whitespace:\n   253→            # Normalize all whitespace to single spaces\n   254→            import re\n   255→\n   256→            normalized = re.sub(r\"\\s+\", \" \", normalized)\n   257→\n   258→        return normalized\n   259→\n   260→    def compute_hash(\n   261→        self,\n   262→        title: str | None = None,\n   263→        h1: str | None = None,\n   264→        meta_description: str | None = None,\n   265→        body_text: str | None = None,\n   266→        project_id: str | None = None,\n   267→        page_id: str | None = None,\n   268→    ) -> str:\n   269→        \"\"\"Compute a content hash from page content fields.\n   270→\n   271→        The hash is computed from a normalized concatenation of:\n   272→        title|h1|meta_description|body_text[:max_content_length]\n   273→\n   274→        Args:\n   275→            title: Page title\n   276→            h1: Primary heading (H1)\n   277→            meta_description: Meta description content\n   278→            body_text: Body text content\n   279→            project_id: Optional project ID for logging\n   280→            page_id: Optional page ID for logging\n   281→\n   282→        Returns:\n   283→            MD5 hex digest of the content\n   284→        \"\"\"\n   285→        start_time = time.monotonic()\n   286→        logger.debug(\n   287→            \"compute_hash() called\",\n   288→            extra={\n   289→                \"project_id\": project_id,\n   290→                \"page_id\": page_id,\n   291→                \"has_title\": title is not None,\n   292→                \"has_h1\": h1 is not None,\n   293→                \"has_meta\": meta_description is not None,\n   294→                \"body_length\": len(body_text) if body_text else 0,\n   295→            },\n   296→        )\n   297→\n   298→        try:\n   299→            # Normalize each component\n   300→            norm_title = self._normalize_text(title)\n   301→            norm_h1 = self._normalize_text(h1)\n   302→            norm_meta = self._normalize_text(meta_description)\n   303→\n   304→            # Truncate and normalize body text\n   305→            truncated_body = \"\"\n   306→            if body_text:\n   307→                truncated_body = self._normalize_text(\n   308→                    body_text[: self._max_content_length]\n   309→                )\n   310→\n   311→            # Create content string with delimiter\n   312→            content = f\"{norm_title}|{norm_h1}|{norm_meta}|{truncated_body}\"\n   313→\n   314→            # Compute MD5 hash\n   315→            content_hash = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n   316→\n   317→            duration_ms = (time.monotonic() - start_time) * 1000\n   318→            logger.debug(\n   319→                \"Content hash computed\",\n   320→                extra={\n   321→                    \"project_id\": project_id,\n   322→                    \"page_id\": page_id,\n   323→                    \"content_hash\": content_hash,\n   324→                    \"content_length\": len(content),\n   325→                    \"duration_ms\": round(duration_ms, 2),\n   326→                },\n   327→            )\n   328→\n   329→            if duration_ms > SLOW_OPERATION_THRESHOLD_MS:\n   330→                logger.warning(\n   331→                    \"Slow content hash computation\",\n   332→                    extra={\n   333→                        \"project_id\": project_id,\n   334→                        \"page_id\": page_id,\n   335→                        \"content_length\": len(content),\n   336→                        \"duration_ms\": round(duration_ms, 2),\n   337→                    },\n   338→                )\n   339→\n   340→            return content_hash\n   341→\n   342→        except Exception as e:\n   343→            logger.error(\n   344→                \"Content hash computation failed\",\n   345→                extra={\n   346→                    \"project_id\": project_id,\n   347→                    \"page_id\": page_id,\n   348→                    \"error_type\": type(e).__name__,\n   349→                    \"error_message\": str(e),\n   350→                    \"stack_trace\": traceback.format_exc(),\n   351→                },\n   352→                exc_info=True,\n   353→            )\n   354→            raise\n   355→\n   356→    def compute_hash_from_dict(\n   357→        self,\n   358→        data: dict[str, Any],\n   359→        project_id: str | None = None,\n   360→        page_id: str | None = None,\n   361→    ) -> str:\n   362→        \"\"\"Compute content hash from a dictionary with content fields.\n   363→\n   364→        Extracts title, h1 (or first heading), meta_description, and body_text\n   365→        from the provided dictionary.\n   366→\n   367→        Args:\n   368→            data: Dictionary with content fields\n   369→            project_id: Optional project ID for logging\n   370→            page_id: Optional page ID for logging\n   371→\n   372→        Returns:\n   373→            MD5 hex digest of the content\n   374→        \"\"\"\n   375→        logger.debug(\n   376→            \"compute_hash_from_dict() called\",\n   377→            extra={\n   378→                \"project_id\": project_id,\n   379→                \"page_id\": page_id,\n   380→                \"keys\": list(data.keys())[:10],\n   381→            },\n   382→        )\n   383→\n   384→        # Extract fields with fallbacks for common naming variations\n   385→        title = data.get(\"title\") or data.get(\"page_title\")\n   386→        h1 = data.get(\"h1\") or data.get(\"heading\") or data.get(\"main_heading\")\n   387→\n   388→        # Check for headings list\n   389→        if not h1 and \"headings\" in data:\n   390→            headings = data.get(\"headings\", [])\n   391→            if headings and len(headings) > 0:\n   392→                h1 = headings[0] if isinstance(headings[0], str) else str(headings[0])\n   393→\n   394→        meta_description = (\n   395→            data.get(\"meta_description\")\n   396→            or data.get(\"description\")\n   397→            or data.get(\"meta_desc\")\n   398→        )\n   399→\n   400→        body_text = (\n   401→            data.get(\"body_text\")\n   402→            or data.get(\"content\")\n   403→            or data.get(\"text\")\n   404→            or data.get(\"markdown\")\n   405→        )\n   406→\n   407→        return self.compute_hash(\n   408→            title=title,\n   409→            h1=h1,\n   410→            meta_description=meta_description,\n   411→            body_text=body_text,\n   412→            project_id=project_id,\n   413→            page_id=page_id,\n   414→        )\n   415→\n   416→\n   417→class ChangeDetector:\n   418→    \"\"\"Detects changes between two sets of crawled pages.\n   419→\n   420→    Compares page snapshots by normalized URL and content hash to identify:\n   421→    - New pages: Present in new crawl but not in previous\n   422→    - Removed pages: Present in previous crawl but not in new\n   423→    - Changed pages: Present in both but with different content hash\n   424→    - Unchanged pages: Present in both with same content hash\n   425→\n   426→    Example:\n   427→        detector = ChangeDetector()\n   428→\n   429→        # Previous crawl results\n   430→        previous = [\n   431→            PageSnapshot(url=\"https://example.com/page1\", content_hash=\"abc123\"),\n   432→            PageSnapshot(url=\"https://example.com/page2\", content_hash=\"def456\"),\n   433→        ]\n   434→\n   435→        # New crawl results\n   436→        current = [\n   437→            PageSnapshot(url=\"https://example.com/page1\", content_hash=\"abc123\"),  # unchanged\n   438→            PageSnapshot(url=\"https://example.com/page3\", content_hash=\"ghi789\"),  # new\n   439→        ]\n   440→\n   441→        summary = detector.compare(\n   442→            crawl_id=\"new-crawl-id\",\n   443→            previous_crawl_id=\"prev-crawl-id\",\n   444→            previous_pages=previous,\n   445→            current_pages=current,\n   446→        )\n   447→\n   448→        print(summary.to_dict())\n   449→    \"\"\"\n   450→\n   451→    def __init__(\n   452→        self,\n   453→        new_page_threshold: int = DEFAULT_NEW_PAGE_THRESHOLD,\n   454→        change_percentage_threshold: float = DEFAULT_CHANGE_PERCENTAGE_THRESHOLD,\n   455→    ) -> None:\n   456→        \"\"\"Initialize the change detector.\n   457→\n   458→        Args:\n   459→            new_page_threshold: Number of new pages to trigger significance\n   460→            change_percentage_threshold: Percentage of changed pages to trigger significance\n   461→        \"\"\"\n   462→        logger.debug(\n   463→            \"ChangeDetector.__init__ called\",\n   464→            extra={\n   465→                \"new_page_threshold\": new_page_threshold,\n   466→                \"change_percentage_threshold\": change_percentage_threshold,\n   467→            },\n   468→        )\n   469→\n   470→        if new_page_threshold < 0:\n   471→            logger.warning(\n   472→                \"Validation failed: negative new_page_threshold\",\n   473→                extra={\n   474→                    \"field\": \"new_page_threshold\",\n   475→                    \"value\": new_page_threshold,\n   476→                },\n   477→            )\n   478→            raise ValueError(\"new_page_threshold must be non-negative\")\n   479→\n   480→        if not 0.0 <= change_percentage_threshold <= 1.0:\n   481→            logger.warning(\n   482→                \"Validation failed: invalid change_percentage_threshold\",\n   483→                extra={\n   484→                    \"field\": \"change_percentage_threshold\",\n   485→                    \"value\": change_percentage_threshold,\n   486→                },\n   487→            )\n   488→            raise ValueError(\"change_percentage_threshold must be between 0.0 and 1.0\")\n   489→\n   490→        self._new_page_threshold = new_page_threshold\n   491→        self._change_percentage_threshold = change_percentage_threshold\n   492→\n   493→    @property\n   494→    def new_page_threshold(self) -> int:\n   495→        \"\"\"Get the new page threshold for significance.\"\"\"\n   496→        return self._new_page_threshold\n   497→\n   498→    @property\n   499→    def change_percentage_threshold(self) -> float:\n   500→        \"\"\"Get the change percentage threshold for significance.\"\"\"\n   501→        return self._change_percentage_threshold\n   502→\n   503→    def compare(\n   504→        self,\n   505→        crawl_id: str,\n   506→        previous_crawl_id: str,\n   507→        previous_pages: list[PageSnapshot],\n   508→        current_pages: list[PageSnapshot],\n   509→        project_id: str | None = None,\n   510→    ) -> ChangeSummary:\n   511→        \"\"\"Compare two sets of crawled pages to detect changes.\n   512→\n   513→        Args:\n   514→            crawl_id: ID of the current/new crawl\n   515→            previous_crawl_id: ID of the previous crawl\n   516→            previous_pages: Page snapshots from previous crawl\n   517→            current_pages: Page snapshots from current crawl\n   518→            project_id: Optional project ID for logging\n   519→\n   520→        Returns:\n   521→            ChangeSummary with detected changes\n   522→        \"\"\"\n   523→        start_time = time.monotonic()\n   524→        logger.debug(\n   525→            \"compare() called\",\n   526→            extra={\n   527→                \"project_id\": project_id,\n   528→                \"crawl_id\": crawl_id,\n   529→                \"previous_crawl_id\": previous_crawl_id,\n   530→                \"previous_page_count\": len(previous_pages),\n   531→                \"current_page_count\": len(current_pages),\n   532→            },\n   533→        )\n   534→\n   535→        try:\n   536→            # Index previous pages by URL\n   537→            prev_by_url: dict[str, PageSnapshot] = {p.url: p for p in previous_pages}\n   538→            curr_by_url: dict[str, PageSnapshot] = {p.url: p for p in current_pages}\n   539→\n   540→            prev_urls = set(prev_by_url.keys())\n   541→            curr_urls = set(curr_by_url.keys())\n   542→\n   543→            # Calculate change sets\n   544→            new_urls = curr_urls - prev_urls\n   545→            removed_urls = prev_urls - curr_urls\n   546→            common_urls = prev_urls & curr_urls\n   547→\n   548→            # Classify common URLs as changed or unchanged\n   549→            changed_urls: set[str] = set()\n   550→            unchanged_urls: set[str] = set()\n   551→\n   552→            for url in common_urls:\n   553→                prev_hash = prev_by_url[url].content_hash\n   554→                curr_hash = curr_by_url[url].content_hash\n   555→\n   556→                if prev_hash != curr_hash:\n   557→                    changed_urls.add(url)\n   558→                else:\n   559→                    unchanged_urls.add(url)\n   560→\n   561→            # Build changes list\n   562→            changes: list[PageChange] = []\n   563→\n   564→            # New pages\n   565→            for url in sorted(new_urls):\n   566→                page = curr_by_url[url]\n   567→                changes.append(\n   568→                    PageChange(\n   569→                        url=url,\n   570→                        change_type=ChangeType.NEW,\n   571→                        new_hash=page.content_hash,\n   572→                        title=page.title,\n   573→                        page_id=page.page_id,\n   574→                    )\n   575→                )\n   576→\n   577→            # Removed pages\n   578→            for url in sorted(removed_urls):\n   579→                page = prev_by_url[url]\n   580→                changes.append(\n   581→                    PageChange(\n   582→                        url=url,\n   583→                        change_type=ChangeType.REMOVED,\n   584→                        old_hash=page.content_hash,\n   585→                        title=page.title,\n   586→                        page_id=page.page_id,\n   587→                    )\n   588→                )\n   589→\n   590→            # Changed pages\n   591→            for url in sorted(changed_urls):\n   592→                prev_page = prev_by_url[url]\n   593→                curr_page = curr_by_url[url]\n   594→                changes.append(\n   595→                    PageChange(\n   596→                        url=url,\n   597→                        change_type=ChangeType.CHANGED,\n   598→                        old_hash=prev_page.content_hash,\n   599→                        new_hash=curr_page.content_hash,\n   600→                        title=curr_page.title,\n   601→                        page_id=curr_page.page_id,\n   602→                    )\n   603→                )\n   604→\n   605→            # Unchanged pages (not typically needed in detail but track for completeness)\n   606→            for url in sorted(unchanged_urls):\n   607→                curr_page = curr_by_url[url]\n   608→                changes.append(\n   609→                    PageChange(\n   610→                        url=url,\n   611→                        change_type=ChangeType.UNCHANGED,\n   612→                        old_hash=curr_page.content_hash,\n   613→                        new_hash=curr_page.content_hash,\n   614→                        title=curr_page.title,\n   615→                        page_id=curr_page.page_id,\n   616→                    )\n   617→                )\n   618→\n   619→            # Create summary\n   620→            summary = ChangeSummary(\n   621→                crawl_id=crawl_id,\n   622→                compared_to=previous_crawl_id,\n   623→                new_pages=len(new_urls),\n   624→                removed_pages=len(removed_urls),\n   625→                changed_pages=len(changed_urls),\n   626→                unchanged_pages=len(unchanged_urls),\n   627→                new_page_urls=sorted(new_urls),\n   628→                removed_page_urls=sorted(removed_urls),\n   629→                changed_page_urls=sorted(changed_urls),\n   630→                changes=changes,\n   631→            )\n   632→\n   633→            # Determine significance\n   634→            summary.is_significant = self._is_significant(summary)\n   635→\n   636→            duration_ms = (time.monotonic() - start_time) * 1000\n   637→\n   638→            # Log state transition (comparison completed)\n   639→            logger.info(\n   640→                \"Change detection completed\",\n   641→                extra={\n   642→                    \"project_id\": project_id,\n   643→                    \"crawl_id\": crawl_id,\n   644→                    \"previous_crawl_id\": previous_crawl_id,\n   645→                    \"new_pages\": summary.new_pages,\n   646→                    \"removed_pages\": summary.removed_pages,\n   647→                    \"changed_pages\": summary.changed_pages,\n   648→                    \"unchanged_pages\": summary.unchanged_pages,\n   649→                    \"is_significant\": summary.is_significant,\n   650→                    \"change_percentage\": round(summary.change_percentage, 4),\n   651→                    \"duration_ms\": round(duration_ms, 2),\n   652→                },\n   653→            )\n   654→\n   655→            if duration_ms > SLOW_OPERATION_THRESHOLD_MS:\n   656→                logger.warning(\n   657→                    \"Slow change detection\",\n   658→                    extra={\n   659→                        \"project_id\": project_id,\n   660→                        \"crawl_id\": crawl_id,\n   661→                        \"page_count\": len(previous_pages) + len(current_pages),\n   662→                        \"duration_ms\": round(duration_ms, 2),\n   663→                    },\n   664→                )\n   665→\n   666→            return summary\n   667→\n   668→        except Exception as e:\n   669→            logger.error(\n   670→                \"Change detection failed\",\n   671→                extra={\n   672→                    \"project_id\": project_id,\n   673→                    \"crawl_id\": crawl_id,\n   674→                    \"previous_crawl_id\": previous_crawl_id,\n   675→                    \"error_type\": type(e).__name__,\n   676→                    \"error_message\": str(e),\n   677→                    \"stack_trace\": traceback.format_exc(),\n   678→                },\n   679→                exc_info=True,\n   680→            )\n   681→            raise\n   682→\n   683→    def _is_significant(self, summary: ChangeSummary) -> bool:\n   684→        \"\"\"Determine if changes are significant.\n   685→\n   686→        Significant means:\n   687→        - 5+ new pages, OR\n   688→        - 10%+ of existing pages changed content\n   689→\n   690→        Args:\n   691→            summary: The change summary to evaluate\n   692→\n   693→        Returns:\n   694→            True if changes are significant\n   695→        \"\"\"\n   696→        # Check new page threshold\n   697→        if summary.new_pages >= self._new_page_threshold:\n   698→            logger.debug(\n   699→                \"Changes significant: new page threshold met\",\n   700→                extra={\n   701→                    \"new_pages\": summary.new_pages,\n   702→                    \"threshold\": self._new_page_threshold,\n   703→                },\n   704→            )\n   705→            return True\n   706→\n   707→        # Check change percentage threshold\n   708→        if summary.change_percentage >= self._change_percentage_threshold:\n   709→            logger.debug(\n   710→                \"Changes significant: change percentage threshold met\",\n   711→                extra={\n   712→                    \"change_percentage\": round(summary.change_percentage, 4),\n   713→                    \"threshold\": self._change_percentage_threshold,\n   714→                },\n   715→            )\n   716→            return True\n   717→\n   718→        return False\n   719→\n   720→    def compare_from_dicts(\n   721→        self,\n   722→        crawl_id: str,\n   723→        previous_crawl_id: str,\n   724→        previous_pages: list[dict[str, Any]],\n   725→        current_pages: list[dict[str, Any]],\n   726→        url_key: str = \"normalized_url\",\n   727→        hash_key: str = \"content_hash\",\n   728→        title_key: str = \"title\",\n   729→        id_key: str = \"id\",\n   730→        project_id: str | None = None,\n   731→    ) -> ChangeSummary:\n   732→        \"\"\"Compare pages from dictionary representations.\n   733→\n   734→        Convenience method for comparing pages from database query results\n   735→        or API responses.\n   736→\n   737→        Args:\n   738→            crawl_id: ID of the current crawl\n   739→            previous_crawl_id: ID of the previous crawl\n   740→            previous_pages: Previous page data as dicts\n   741→            current_pages: Current page data as dicts\n   742→            url_key: Key for URL in dicts\n   743→            hash_key: Key for content hash in dicts\n   744→            title_key: Key for title in dicts\n   745→            id_key: Key for page ID in dicts\n   746→            project_id: Optional project ID for logging\n   747→\n   748→        Returns:\n   749→            ChangeSummary with detected changes\n   750→        \"\"\"\n   751→        logger.debug(\n   752→            \"compare_from_dicts() called\",\n   753→            extra={\n   754→                \"project_id\": project_id,\n   755→                \"crawl_id\": crawl_id,\n   756→                \"previous_page_count\": len(previous_pages),\n   757→                \"current_page_count\": len(current_pages),\n   758→            },\n   759→        )\n   760→\n   761→        def to_snapshot(d: dict[str, Any]) -> PageSnapshot:\n   762→            return PageSnapshot(\n   763→                url=d.get(url_key, \"\"),\n   764→                content_hash=d.get(hash_key),\n   765→                title=d.get(title_key),\n   766→                page_id=d.get(id_key),\n   767→            )\n   768→\n   769→        prev_snapshots = [to_snapshot(p) for p in previous_pages if p.get(url_key)]\n   770→        curr_snapshots = [to_snapshot(p) for p in current_pages if p.get(url_key)]\n   771→\n   772→        return self.compare(\n   773→            crawl_id=crawl_id,\n   774→            previous_crawl_id=previous_crawl_id,\n   775→            previous_pages=prev_snapshots,\n   776→            current_pages=curr_snapshots,\n   777→            project_id=project_id,\n   778→        )\n   779→\n   780→\n   781→# Default instances for convenience\n   782→_default_hasher: ContentHasher | None = None\n   783→_default_detector: ChangeDetector | None = None\n   784→\n   785→\n   786→def get_content_hasher() -> ContentHasher:\n   787→    \"\"\"Get the default ContentHasher instance (singleton).\n   788→\n   789→    Returns:\n   790→        Default ContentHasher instance.\n   791→    \"\"\"\n   792→    global _default_hasher\n   793→    if _default_hasher is None:\n   794→        _default_hasher = ContentHasher()\n   795→    return _default_hasher\n   796→\n   797→\n   798→def get_change_detector() -> ChangeDetector:\n   799→    \"\"\"Get the default ChangeDetector instance (singleton).\n   800→\n   801→    Returns:\n   802→        Default ChangeDetector instance.\n   803→    \"\"\"\n   804→    global _default_detector\n   805→    if _default_detector is None:\n   806→        _default_detector = ChangeDetector()\n   807→    return _default_detector\n   808→\n   809→\n   810→def compute_content_hash(\n   811→    title: str | None = None,\n   812→    h1: str | None = None,\n   813→    meta_description: str | None = None,\n   814→    body_text: str | None = None,\n   815→    project_id: str | None = None,\n   816→    page_id: str | None = None,\n   817→) -> str:\n   818→    \"\"\"Convenience function to compute content hash.\n   819→\n   820→    Args:\n   821→        title: Page title\n   822→        h1: Primary heading (H1)\n   823→        meta_description: Meta description content\n   824→        body_text: Body text content\n   825→        project_id: Optional project ID for logging\n   826→        page_id: Optional page ID for logging\n   827→\n   828→    Returns:\n   829→        MD5 hex digest of the content\n   830→\n   831→    Example:\n   832→        >>> hash = compute_content_hash(\n   833→        ...     title=\"My Product\",\n   834→        ...     h1=\"My Product\",\n   835→        ...     meta_description=\"Buy my product today\",\n   836→        ...     body_text=\"Product details and description...\",\n   837→        ... )\n   838→        >>> print(hash)\n   839→        'a1b2c3d4e5f6...'\n   840→    \"\"\"\n   841→    hasher = get_content_hasher()\n   842→    return hasher.compute_hash(\n   843→        title=title,\n   844→        h1=h1,\n   845→        meta_description=meta_description,\n   846→        body_text=body_text,\n   847→        project_id=project_id,\n   848→        page_id=page_id,\n   849→    )\n   850→\n   851→\n   852→def detect_changes(\n   853→    crawl_id: str,\n   854→    previous_crawl_id: str,\n   855→    previous_pages: list[PageSnapshot],\n   856→    current_pages: list[PageSnapshot],\n   857→    project_id: str | None = None,\n   858→) -> ChangeSummary:\n   859→    \"\"\"Convenience function to detect changes between crawls.\n   860→\n   861→    Args:\n   862→        crawl_id: ID of the current crawl\n   863→        previous_crawl_id: ID of the previous crawl\n   864→        previous_pages: Page snapshots from previous crawl\n   865→        current_pages: Page snapshots from current crawl\n   866→        project_id: Optional project ID for logging\n   867→\n   868→    Returns:\n   869→        ChangeSummary with detected changes\n   870→\n   871→    Example:\n   872→        >>> previous = [\n   873→        ...     PageSnapshot(url=\"https://example.com/a\", content_hash=\"hash1\"),\n   874→        ...     PageSnapshot(url=\"https://example.com/b\", content_hash=\"hash2\"),\n   875→        ... ]\n   876→        >>> current = [\n   877→        ...     PageSnapshot(url=\"https://example.com/a\", content_hash=\"hash1\"),  # unchanged\n   878→        ...     PageSnapshot(url=\"https://example.com/c\", content_hash=\"hash3\"),  # new\n   879→        ... ]\n   880→        >>> summary = detect_changes(\"crawl2\", \"crawl1\", previous, current)\n   881→        >>> print(summary.new_pages)\n   882→        1\n   883→        >>> print(summary.removed_pages)\n   884→        1\n   885→    \"\"\"\n   886→    detector = get_change_detector()\n   887→    return detector.compare(\n   888→        crawl_id=crawl_id,\n   889→        previous_crawl_id=previous_crawl_id,\n   890→        previous_pages=previous_pages,\n   891→        current_pages=current_pages,\n   892→        project_id=project_id,\n   893→    )\n   894→\n   895→\n   896→# Valid change types for external validation\n   897→VALID_CHANGE_TYPES = frozenset(c.value for c in ChangeType)\n   898→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"4b988a81-7221-4202-a90d-dfe321fcee1f","tool_use_result":{"type":"text","file":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py","content":"\"\"\"Change detection algorithm for comparing crawl results.\n\nImplements content hash comparison to detect changes between crawls:\n- Content hash: Computed from semantic content (title, h1, meta_description, body text)\n- Page comparison: Match by normalized URL, classify as new/removed/changed/unchanged\n- Change summary: Aggregate statistics with significance detection\n\nSignificance threshold: 5+ new pages OR 10%+ content changes.\n\nERROR LOGGING REQUIREMENTS:\n- Log method entry/exit at DEBUG level with parameters (sanitized)\n- Log all exceptions with full stack trace and context\n- Include entity IDs (project_id, page_id) in all service logs\n- Log validation failures with field names and rejected values\n- Log state transitions (phase changes) at INFO level\n- Add timing logs for operations >1 second\n\"\"\"\n\nimport hashlib\nimport time\nimport traceback\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\n\nfrom app.core.logging import get_logger\n\nlogger = get_logger(\"change_detection\")\n\n# Threshold for logging slow operations (in milliseconds)\nSLOW_OPERATION_THRESHOLD_MS = 1000\n\n# Default significance thresholds\nDEFAULT_NEW_PAGE_THRESHOLD = 5\nDEFAULT_CHANGE_PERCENTAGE_THRESHOLD = 0.10  # 10%\n\n# Maximum content length for hashing (to prevent memory issues)\nMAX_CONTENT_LENGTH_FOR_HASH = 5000\n\n\nclass ChangeType(Enum):\n    \"\"\"Types of changes detected between crawls.\"\"\"\n\n    NEW = \"new\"\n    REMOVED = \"removed\"\n    CHANGED = \"changed\"\n    UNCHANGED = \"unchanged\"\n\n\n@dataclass\nclass PageSnapshot:\n    \"\"\"Snapshot of a page at a point in time for comparison.\n\n    Attributes:\n        url: Normalized URL of the page\n        content_hash: Hash of semantic content\n        title: Page title (optional, for context in reports)\n        page_id: Database page ID (optional)\n    \"\"\"\n\n    url: str\n    content_hash: str | None\n    title: str | None = None\n    page_id: str | None = None\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"url\": self.url,\n            \"content_hash\": self.content_hash,\n            \"title\": self.title[:100] if self.title else None,\n            \"page_id\": self.page_id,\n        }\n\n\n@dataclass\nclass PageChange:\n    \"\"\"Represents a detected change for a single page.\n\n    Attributes:\n        url: Normalized URL of the page\n        change_type: Type of change (new, removed, changed, unchanged)\n        old_hash: Previous content hash (for changed/removed)\n        new_hash: New content hash (for new/changed)\n        title: Page title (optional)\n        page_id: Database page ID (optional)\n    \"\"\"\n\n    url: str\n    change_type: ChangeType\n    old_hash: str | None = None\n    new_hash: str | None = None\n    title: str | None = None\n    page_id: str | None = None\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"url\": self.url,\n            \"change_type\": self.change_type.value,\n            \"old_hash\": self.old_hash,\n            \"new_hash\": self.new_hash,\n            \"title\": self.title[:100] if self.title else None,\n            \"page_id\": self.page_id,\n        }\n\n\n@dataclass\nclass ChangeSummary:\n    \"\"\"Summary of changes detected between two crawls.\n\n    Attributes:\n        crawl_id: ID of the new crawl\n        compared_to: ID of the previous crawl being compared to\n        new_pages: Count of new pages found\n        removed_pages: Count of pages no longer present\n        changed_pages: Count of pages with content changes\n        unchanged_pages: Count of pages without changes\n        new_page_urls: List of URLs for new pages\n        removed_page_urls: List of URLs for removed pages\n        changed_page_urls: List of URLs for changed pages\n        is_significant: Whether changes meet significance threshold\n        changes: Detailed list of all page changes\n        computed_at: When the comparison was performed\n    \"\"\"\n\n    crawl_id: str\n    compared_to: str\n    new_pages: int = 0\n    removed_pages: int = 0\n    changed_pages: int = 0\n    unchanged_pages: int = 0\n    new_page_urls: list[str] = field(default_factory=list)\n    removed_page_urls: list[str] = field(default_factory=list)\n    changed_page_urls: list[str] = field(default_factory=list)\n    is_significant: bool = False\n    changes: list[PageChange] = field(default_factory=list)\n    computed_at: datetime | None = None\n\n    def __post_init__(self) -> None:\n        \"\"\"Set computed_at if not provided.\"\"\"\n        if self.computed_at is None:\n            from datetime import UTC\n\n            self.computed_at = datetime.now(UTC)\n\n    @property\n    def total_pages(self) -> int:\n        \"\"\"Total pages in the new crawl.\"\"\"\n        return self.new_pages + self.changed_pages + self.unchanged_pages\n\n    @property\n    def total_changes(self) -> int:\n        \"\"\"Total number of changes (new + removed + changed).\"\"\"\n        return self.new_pages + self.removed_pages + self.changed_pages\n\n    @property\n    def change_percentage(self) -> float:\n        \"\"\"Percentage of pages that changed (for pages that existed in both crawls).\"\"\"\n        comparable_pages = self.changed_pages + self.unchanged_pages\n        if comparable_pages == 0:\n            return 0.0\n        return self.changed_pages / comparable_pages\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\n\n        Matches the JSON schema from the spec.\n        \"\"\"\n        return {\n            \"crawl_id\": self.crawl_id,\n            \"compared_to\": self.compared_to,\n            \"summary\": {\n                \"new_pages\": self.new_pages,\n                \"removed_pages\": self.removed_pages,\n                \"changed_pages\": self.changed_pages,\n                \"unchanged_pages\": self.unchanged_pages,\n            },\n            \"new_page_urls\": self.new_page_urls[:100],  # Limit for large changes\n            \"removed_page_urls\": self.removed_page_urls[:100],\n            \"changed_page_urls\": self.changed_page_urls[:100],\n            \"is_significant\": self.is_significant,\n            \"computed_at\": self.computed_at.isoformat() if self.computed_at else None,\n            \"total_pages\": self.total_pages,\n            \"total_changes\": self.total_changes,\n            \"change_percentage\": round(self.change_percentage, 4),\n        }\n\n\nclass ContentHasher:\n    \"\"\"Computes content hashes for change detection.\n\n    Creates semantic content hashes from page content fields:\n    - title: Page title\n    - h1: Primary heading\n    - meta_description: Meta description\n    - body_text: First N characters of body content\n\n    Uses MD5 for fast comparison (not cryptographic security).\n\n    Example:\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Product Name\",\n            h1=\"Product Name\",\n            meta_description=\"Buy our product\",\n            body_text=\"Product details...\",\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        max_content_length: int = MAX_CONTENT_LENGTH_FOR_HASH,\n        include_whitespace: bool = False,\n    ) -> None:\n        \"\"\"Initialize the content hasher.\n\n        Args:\n            max_content_length: Maximum body text length to include in hash\n            include_whitespace: If False, normalize whitespace before hashing\n        \"\"\"\n        logger.debug(\n            \"ContentHasher.__init__ called\",\n            extra={\n                \"max_content_length\": max_content_length,\n                \"include_whitespace\": include_whitespace,\n            },\n        )\n        self._max_content_length = max_content_length\n        self._include_whitespace = include_whitespace\n\n    @property\n    def max_content_length(self) -> int:\n        \"\"\"Get the maximum content length for hashing.\"\"\"\n        return self._max_content_length\n\n    def _normalize_text(self, text: str | None) -> str:\n        \"\"\"Normalize text for consistent hashing.\n\n        Args:\n            text: Text to normalize\n\n        Returns:\n            Normalized text (lowercased, whitespace normalized)\n        \"\"\"\n        if not text:\n            return \"\"\n\n        normalized = text.strip().lower()\n\n        if not self._include_whitespace:\n            # Normalize all whitespace to single spaces\n            import re\n\n            normalized = re.sub(r\"\\s+\", \" \", normalized)\n\n        return normalized\n\n    def compute_hash(\n        self,\n        title: str | None = None,\n        h1: str | None = None,\n        meta_description: str | None = None,\n        body_text: str | None = None,\n        project_id: str | None = None,\n        page_id: str | None = None,\n    ) -> str:\n        \"\"\"Compute a content hash from page content fields.\n\n        The hash is computed from a normalized concatenation of:\n        title|h1|meta_description|body_text[:max_content_length]\n\n        Args:\n            title: Page title\n            h1: Primary heading (H1)\n            meta_description: Meta description content\n            body_text: Body text content\n            project_id: Optional project ID for logging\n            page_id: Optional page ID for logging\n\n        Returns:\n            MD5 hex digest of the content\n        \"\"\"\n        start_time = time.monotonic()\n        logger.debug(\n            \"compute_hash() called\",\n            extra={\n                \"project_id\": project_id,\n                \"page_id\": page_id,\n                \"has_title\": title is not None,\n                \"has_h1\": h1 is not None,\n                \"has_meta\": meta_description is not None,\n                \"body_length\": len(body_text) if body_text else 0,\n            },\n        )\n\n        try:\n            # Normalize each component\n            norm_title = self._normalize_text(title)\n            norm_h1 = self._normalize_text(h1)\n            norm_meta = self._normalize_text(meta_description)\n\n            # Truncate and normalize body text\n            truncated_body = \"\"\n            if body_text:\n                truncated_body = self._normalize_text(\n                    body_text[: self._max_content_length]\n                )\n\n            # Create content string with delimiter\n            content = f\"{norm_title}|{norm_h1}|{norm_meta}|{truncated_body}\"\n\n            # Compute MD5 hash\n            content_hash = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n\n            duration_ms = (time.monotonic() - start_time) * 1000\n            logger.debug(\n                \"Content hash computed\",\n                extra={\n                    \"project_id\": project_id,\n                    \"page_id\": page_id,\n                    \"content_hash\": content_hash,\n                    \"content_length\": len(content),\n                    \"duration_ms\": round(duration_ms, 2),\n                },\n            )\n\n            if duration_ms > SLOW_OPERATION_THRESHOLD_MS:\n                logger.warning(\n                    \"Slow content hash computation\",\n                    extra={\n                        \"project_id\": project_id,\n                        \"page_id\": page_id,\n                        \"content_length\": len(content),\n                        \"duration_ms\": round(duration_ms, 2),\n                    },\n                )\n\n            return content_hash\n\n        except Exception as e:\n            logger.error(\n                \"Content hash computation failed\",\n                extra={\n                    \"project_id\": project_id,\n                    \"page_id\": page_id,\n                    \"error_type\": type(e).__name__,\n                    \"error_message\": str(e),\n                    \"stack_trace\": traceback.format_exc(),\n                },\n                exc_info=True,\n            )\n            raise\n\n    def compute_hash_from_dict(\n        self,\n        data: dict[str, Any],\n        project_id: str | None = None,\n        page_id: str | None = None,\n    ) -> str:\n        \"\"\"Compute content hash from a dictionary with content fields.\n\n        Extracts title, h1 (or first heading), meta_description, and body_text\n        from the provided dictionary.\n\n        Args:\n            data: Dictionary with content fields\n            project_id: Optional project ID for logging\n            page_id: Optional page ID for logging\n\n        Returns:\n            MD5 hex digest of the content\n        \"\"\"\n        logger.debug(\n            \"compute_hash_from_dict() called\",\n            extra={\n                \"project_id\": project_id,\n                \"page_id\": page_id,\n                \"keys\": list(data.keys())[:10],\n            },\n        )\n\n        # Extract fields with fallbacks for common naming variations\n        title = data.get(\"title\") or data.get(\"page_title\")\n        h1 = data.get(\"h1\") or data.get(\"heading\") or data.get(\"main_heading\")\n\n        # Check for headings list\n        if not h1 and \"headings\" in data:\n            headings = data.get(\"headings\", [])\n            if headings and len(headings) > 0:\n                h1 = headings[0] if isinstance(headings[0], str) else str(headings[0])\n\n        meta_description = (\n            data.get(\"meta_description\")\n            or data.get(\"description\")\n            or data.get(\"meta_desc\")\n        )\n\n        body_text = (\n            data.get(\"body_text\")\n            or data.get(\"content\")\n            or data.get(\"text\")\n            or data.get(\"markdown\")\n        )\n\n        return self.compute_hash(\n            title=title,\n            h1=h1,\n            meta_description=meta_description,\n            body_text=body_text,\n            project_id=project_id,\n            page_id=page_id,\n        )\n\n\nclass ChangeDetector:\n    \"\"\"Detects changes between two sets of crawled pages.\n\n    Compares page snapshots by normalized URL and content hash to identify:\n    - New pages: Present in new crawl but not in previous\n    - Removed pages: Present in previous crawl but not in new\n    - Changed pages: Present in both but with different content hash\n    - Unchanged pages: Present in both with same content hash\n\n    Example:\n        detector = ChangeDetector()\n\n        # Previous crawl results\n        previous = [\n            PageSnapshot(url=\"https://example.com/page1\", content_hash=\"abc123\"),\n            PageSnapshot(url=\"https://example.com/page2\", content_hash=\"def456\"),\n        ]\n\n        # New crawl results\n        current = [\n            PageSnapshot(url=\"https://example.com/page1\", content_hash=\"abc123\"),  # unchanged\n            PageSnapshot(url=\"https://example.com/page3\", content_hash=\"ghi789\"),  # new\n        ]\n\n        summary = detector.compare(\n            crawl_id=\"new-crawl-id\",\n            previous_crawl_id=\"prev-crawl-id\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        print(summary.to_dict())\n    \"\"\"\n\n    def __init__(\n        self,\n        new_page_threshold: int = DEFAULT_NEW_PAGE_THRESHOLD,\n        change_percentage_threshold: float = DEFAULT_CHANGE_PERCENTAGE_THRESHOLD,\n    ) -> None:\n        \"\"\"Initialize the change detector.\n\n        Args:\n            new_page_threshold: Number of new pages to trigger significance\n            change_percentage_threshold: Percentage of changed pages to trigger significance\n        \"\"\"\n        logger.debug(\n            \"ChangeDetector.__init__ called\",\n            extra={\n                \"new_page_threshold\": new_page_threshold,\n                \"change_percentage_threshold\": change_percentage_threshold,\n            },\n        )\n\n        if new_page_threshold < 0:\n            logger.warning(\n                \"Validation failed: negative new_page_threshold\",\n                extra={\n                    \"field\": \"new_page_threshold\",\n                    \"value\": new_page_threshold,\n                },\n            )\n            raise ValueError(\"new_page_threshold must be non-negative\")\n\n        if not 0.0 <= change_percentage_threshold <= 1.0:\n            logger.warning(\n                \"Validation failed: invalid change_percentage_threshold\",\n                extra={\n                    \"field\": \"change_percentage_threshold\",\n                    \"value\": change_percentage_threshold,\n                },\n            )\n            raise ValueError(\"change_percentage_threshold must be between 0.0 and 1.0\")\n\n        self._new_page_threshold = new_page_threshold\n        self._change_percentage_threshold = change_percentage_threshold\n\n    @property\n    def new_page_threshold(self) -> int:\n        \"\"\"Get the new page threshold for significance.\"\"\"\n        return self._new_page_threshold\n\n    @property\n    def change_percentage_threshold(self) -> float:\n        \"\"\"Get the change percentage threshold for significance.\"\"\"\n        return self._change_percentage_threshold\n\n    def compare(\n        self,\n        crawl_id: str,\n        previous_crawl_id: str,\n        previous_pages: list[PageSnapshot],\n        current_pages: list[PageSnapshot],\n        project_id: str | None = None,\n    ) -> ChangeSummary:\n        \"\"\"Compare two sets of crawled pages to detect changes.\n\n        Args:\n            crawl_id: ID of the current/new crawl\n            previous_crawl_id: ID of the previous crawl\n            previous_pages: Page snapshots from previous crawl\n            current_pages: Page snapshots from current crawl\n            project_id: Optional project ID for logging\n\n        Returns:\n            ChangeSummary with detected changes\n        \"\"\"\n        start_time = time.monotonic()\n        logger.debug(\n            \"compare() called\",\n            extra={\n                \"project_id\": project_id,\n                \"crawl_id\": crawl_id,\n                \"previous_crawl_id\": previous_crawl_id,\n                \"previous_page_count\": len(previous_pages),\n                \"current_page_count\": len(current_pages),\n            },\n        )\n\n        try:\n            # Index previous pages by URL\n            prev_by_url: dict[str, PageSnapshot] = {p.url: p for p in previous_pages}\n            curr_by_url: dict[str, PageSnapshot] = {p.url: p for p in current_pages}\n\n            prev_urls = set(prev_by_url.keys())\n            curr_urls = set(curr_by_url.keys())\n\n            # Calculate change sets\n            new_urls = curr_urls - prev_urls\n            removed_urls = prev_urls - curr_urls\n            common_urls = prev_urls & curr_urls\n\n            # Classify common URLs as changed or unchanged\n            changed_urls: set[str] = set()\n            unchanged_urls: set[str] = set()\n\n            for url in common_urls:\n                prev_hash = prev_by_url[url].content_hash\n                curr_hash = curr_by_url[url].content_hash\n\n                if prev_hash != curr_hash:\n                    changed_urls.add(url)\n                else:\n                    unchanged_urls.add(url)\n\n            # Build changes list\n            changes: list[PageChange] = []\n\n            # New pages\n            for url in sorted(new_urls):\n                page = curr_by_url[url]\n                changes.append(\n                    PageChange(\n                        url=url,\n                        change_type=ChangeType.NEW,\n                        new_hash=page.content_hash,\n                        title=page.title,\n                        page_id=page.page_id,\n                    )\n                )\n\n            # Removed pages\n            for url in sorted(removed_urls):\n                page = prev_by_url[url]\n                changes.append(\n                    PageChange(\n                        url=url,\n                        change_type=ChangeType.REMOVED,\n                        old_hash=page.content_hash,\n                        title=page.title,\n                        page_id=page.page_id,\n                    )\n                )\n\n            # Changed pages\n            for url in sorted(changed_urls):\n                prev_page = prev_by_url[url]\n                curr_page = curr_by_url[url]\n                changes.append(\n                    PageChange(\n                        url=url,\n                        change_type=ChangeType.CHANGED,\n                        old_hash=prev_page.content_hash,\n                        new_hash=curr_page.content_hash,\n                        title=curr_page.title,\n                        page_id=curr_page.page_id,\n                    )\n                )\n\n            # Unchanged pages (not typically needed in detail but track for completeness)\n            for url in sorted(unchanged_urls):\n                curr_page = curr_by_url[url]\n                changes.append(\n                    PageChange(\n                        url=url,\n                        change_type=ChangeType.UNCHANGED,\n                        old_hash=curr_page.content_hash,\n                        new_hash=curr_page.content_hash,\n                        title=curr_page.title,\n                        page_id=curr_page.page_id,\n                    )\n                )\n\n            # Create summary\n            summary = ChangeSummary(\n                crawl_id=crawl_id,\n                compared_to=previous_crawl_id,\n                new_pages=len(new_urls),\n                removed_pages=len(removed_urls),\n                changed_pages=len(changed_urls),\n                unchanged_pages=len(unchanged_urls),\n                new_page_urls=sorted(new_urls),\n                removed_page_urls=sorted(removed_urls),\n                changed_page_urls=sorted(changed_urls),\n                changes=changes,\n            )\n\n            # Determine significance\n            summary.is_significant = self._is_significant(summary)\n\n            duration_ms = (time.monotonic() - start_time) * 1000\n\n            # Log state transition (comparison completed)\n            logger.info(\n                \"Change detection completed\",\n                extra={\n                    \"project_id\": project_id,\n                    \"crawl_id\": crawl_id,\n                    \"previous_crawl_id\": previous_crawl_id,\n                    \"new_pages\": summary.new_pages,\n                    \"removed_pages\": summary.removed_pages,\n                    \"changed_pages\": summary.changed_pages,\n                    \"unchanged_pages\": summary.unchanged_pages,\n                    \"is_significant\": summary.is_significant,\n                    \"change_percentage\": round(summary.change_percentage, 4),\n                    \"duration_ms\": round(duration_ms, 2),\n                },\n            )\n\n            if duration_ms > SLOW_OPERATION_THRESHOLD_MS:\n                logger.warning(\n                    \"Slow change detection\",\n                    extra={\n                        \"project_id\": project_id,\n                        \"crawl_id\": crawl_id,\n                        \"page_count\": len(previous_pages) + len(current_pages),\n                        \"duration_ms\": round(duration_ms, 2),\n                    },\n                )\n\n            return summary\n\n        except Exception as e:\n            logger.error(\n                \"Change detection failed\",\n                extra={\n                    \"project_id\": project_id,\n                    \"crawl_id\": crawl_id,\n                    \"previous_crawl_id\": previous_crawl_id,\n                    \"error_type\": type(e).__name__,\n                    \"error_message\": str(e),\n                    \"stack_trace\": traceback.format_exc(),\n                },\n                exc_info=True,\n            )\n            raise\n\n    def _is_significant(self, summary: ChangeSummary) -> bool:\n        \"\"\"Determine if changes are significant.\n\n        Significant means:\n        - 5+ new pages, OR\n        - 10%+ of existing pages changed content\n\n        Args:\n            summary: The change summary to evaluate\n\n        Returns:\n            True if changes are significant\n        \"\"\"\n        # Check new page threshold\n        if summary.new_pages >= self._new_page_threshold:\n            logger.debug(\n                \"Changes significant: new page threshold met\",\n                extra={\n                    \"new_pages\": summary.new_pages,\n                    \"threshold\": self._new_page_threshold,\n                },\n            )\n            return True\n\n        # Check change percentage threshold\n        if summary.change_percentage >= self._change_percentage_threshold:\n            logger.debug(\n                \"Changes significant: change percentage threshold met\",\n                extra={\n                    \"change_percentage\": round(summary.change_percentage, 4),\n                    \"threshold\": self._change_percentage_threshold,\n                },\n            )\n            return True\n\n        return False\n\n    def compare_from_dicts(\n        self,\n        crawl_id: str,\n        previous_crawl_id: str,\n        previous_pages: list[dict[str, Any]],\n        current_pages: list[dict[str, Any]],\n        url_key: str = \"normalized_url\",\n        hash_key: str = \"content_hash\",\n        title_key: str = \"title\",\n        id_key: str = \"id\",\n        project_id: str | None = None,\n    ) -> ChangeSummary:\n        \"\"\"Compare pages from dictionary representations.\n\n        Convenience method for comparing pages from database query results\n        or API responses.\n\n        Args:\n            crawl_id: ID of the current crawl\n            previous_crawl_id: ID of the previous crawl\n            previous_pages: Previous page data as dicts\n            current_pages: Current page data as dicts\n            url_key: Key for URL in dicts\n            hash_key: Key for content hash in dicts\n            title_key: Key for title in dicts\n            id_key: Key for page ID in dicts\n            project_id: Optional project ID for logging\n\n        Returns:\n            ChangeSummary with detected changes\n        \"\"\"\n        logger.debug(\n            \"compare_from_dicts() called\",\n            extra={\n                \"project_id\": project_id,\n                \"crawl_id\": crawl_id,\n                \"previous_page_count\": len(previous_pages),\n                \"current_page_count\": len(current_pages),\n            },\n        )\n\n        def to_snapshot(d: dict[str, Any]) -> PageSnapshot:\n            return PageSnapshot(\n                url=d.get(url_key, \"\"),\n                content_hash=d.get(hash_key),\n                title=d.get(title_key),\n                page_id=d.get(id_key),\n            )\n\n        prev_snapshots = [to_snapshot(p) for p in previous_pages if p.get(url_key)]\n        curr_snapshots = [to_snapshot(p) for p in current_pages if p.get(url_key)]\n\n        return self.compare(\n            crawl_id=crawl_id,\n            previous_crawl_id=previous_crawl_id,\n            previous_pages=prev_snapshots,\n            current_pages=curr_snapshots,\n            project_id=project_id,\n        )\n\n\n# Default instances for convenience\n_default_hasher: ContentHasher | None = None\n_default_detector: ChangeDetector | None = None\n\n\ndef get_content_hasher() -> ContentHasher:\n    \"\"\"Get the default ContentHasher instance (singleton).\n\n    Returns:\n        Default ContentHasher instance.\n    \"\"\"\n    global _default_hasher\n    if _default_hasher is None:\n        _default_hasher = ContentHasher()\n    return _default_hasher\n\n\ndef get_change_detector() -> ChangeDetector:\n    \"\"\"Get the default ChangeDetector instance (singleton).\n\n    Returns:\n        Default ChangeDetector instance.\n    \"\"\"\n    global _default_detector\n    if _default_detector is None:\n        _default_detector = ChangeDetector()\n    return _default_detector\n\n\ndef compute_content_hash(\n    title: str | None = None,\n    h1: str | None = None,\n    meta_description: str | None = None,\n    body_text: str | None = None,\n    project_id: str | None = None,\n    page_id: str | None = None,\n) -> str:\n    \"\"\"Convenience function to compute content hash.\n\n    Args:\n        title: Page title\n        h1: Primary heading (H1)\n        meta_description: Meta description content\n        body_text: Body text content\n        project_id: Optional project ID for logging\n        page_id: Optional page ID for logging\n\n    Returns:\n        MD5 hex digest of the content\n\n    Example:\n        >>> hash = compute_content_hash(\n        ...     title=\"My Product\",\n        ...     h1=\"My Product\",\n        ...     meta_description=\"Buy my product today\",\n        ...     body_text=\"Product details and description...\",\n        ... )\n        >>> print(hash)\n        'a1b2c3d4e5f6...'\n    \"\"\"\n    hasher = get_content_hasher()\n    return hasher.compute_hash(\n        title=title,\n        h1=h1,\n        meta_description=meta_description,\n        body_text=body_text,\n        project_id=project_id,\n        page_id=page_id,\n    )\n\n\ndef detect_changes(\n    crawl_id: str,\n    previous_crawl_id: str,\n    previous_pages: list[PageSnapshot],\n    current_pages: list[PageSnapshot],\n    project_id: str | None = None,\n) -> ChangeSummary:\n    \"\"\"Convenience function to detect changes between crawls.\n\n    Args:\n        crawl_id: ID of the current crawl\n        previous_crawl_id: ID of the previous crawl\n        previous_pages: Page snapshots from previous crawl\n        current_pages: Page snapshots from current crawl\n        project_id: Optional project ID for logging\n\n    Returns:\n        ChangeSummary with detected changes\n\n    Example:\n        >>> previous = [\n        ...     PageSnapshot(url=\"https://example.com/a\", content_hash=\"hash1\"),\n        ...     PageSnapshot(url=\"https://example.com/b\", content_hash=\"hash2\"),\n        ... ]\n        >>> current = [\n        ...     PageSnapshot(url=\"https://example.com/a\", content_hash=\"hash1\"),  # unchanged\n        ...     PageSnapshot(url=\"https://example.com/c\", content_hash=\"hash3\"),  # new\n        ... ]\n        >>> summary = detect_changes(\"crawl2\", \"crawl1\", previous, current)\n        >>> print(summary.new_pages)\n        1\n        >>> print(summary.removed_pages)\n        1\n    \"\"\"\n    detector = get_change_detector()\n    return detector.compare(\n        crawl_id=crawl_id,\n        previous_crawl_id=previous_crawl_id,\n        previous_pages=previous_pages,\n        current_pages=current_pages,\n        project_id=project_id,\n    )\n\n\n# Valid change types for external validation\nVALID_CHANGE_TYPES = frozenset(c.value for c in ChangeType)\n","numLines":898,"startLine":1,"totalLines":898}}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01RxdYz6Yi875iBSvgTpJj4d","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01ETsbp4japz19xUSKRp55Ri","name":"Read","input":{"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2664,"cache_read_input_tokens":24943,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2664},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"d9952622-80ea-483f-b86f-0222a8ebd9ba"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01ETsbp4japz19xUSKRp55Ri","type":"tool_result","content":"     1→\"\"\"Unit tests for change detection algorithm.\n     2→\n     3→Tests cover:\n     4→- ContentHasher: Content hash computation with various inputs\n     5→- ChangeDetector: Page comparison and change detection\n     6→- ChangeSummary: Summary generation and significance detection\n     7→- Edge cases: Empty inputs, None values, large datasets\n     8→\"\"\"\n     9→\n    10→import pytest\n    11→\n    12→from app.utils.change_detection import (\n    13→    VALID_CHANGE_TYPES,\n    14→    ChangeDetector,\n    15→    ChangeSummary,\n    16→    ChangeType,\n    17→    ContentHasher,\n    18→    PageChange,\n    19→    PageSnapshot,\n    20→    compute_content_hash,\n    21→    detect_changes,\n    22→    get_change_detector,\n    23→    get_content_hasher,\n    24→)\n    25→\n    26→\n    27→class TestContentHasher:\n    28→    \"\"\"Tests for ContentHasher class.\"\"\"\n    29→\n    30→    def test_init_default(self) -> None:\n    31→        \"\"\"Test default initialization.\"\"\"\n    32→        hasher = ContentHasher()\n    33→        assert hasher.max_content_length == 5000\n    34→\n    35→    def test_init_custom_length(self) -> None:\n    36→        \"\"\"Test initialization with custom content length.\"\"\"\n    37→        hasher = ContentHasher(max_content_length=1000)\n    38→        assert hasher.max_content_length == 1000\n    39→\n    40→    def test_compute_hash_all_fields(self) -> None:\n    41→        \"\"\"Test hash computation with all fields.\"\"\"\n    42→        hasher = ContentHasher()\n    43→        result = hasher.compute_hash(\n    44→            title=\"Test Title\",\n    45→            h1=\"Test Heading\",\n    46→            meta_description=\"Test description\",\n    47→            body_text=\"Test body content\",\n    48→        )\n    49→\n    50→        assert result is not None\n    51→        assert len(result) == 32  # MD5 hex digest length\n    52→        assert result.isalnum()\n    53→\n    54→    def test_compute_hash_empty_fields(self) -> None:\n    55→        \"\"\"Test hash computation with empty fields.\"\"\"\n    56→        hasher = ContentHasher()\n    57→        result = hasher.compute_hash()\n    58→\n    59→        assert result is not None\n    60→        assert len(result) == 32\n    61→\n    62→    def test_compute_hash_none_fields(self) -> None:\n    63→        \"\"\"Test hash computation with None fields.\"\"\"\n    64→        hasher = ContentHasher()\n    65→        result = hasher.compute_hash(\n    66→            title=None,\n    67→            h1=None,\n    68→            meta_description=None,\n    69→            body_text=None,\n    70→        )\n    71→\n    72→        assert result is not None\n    73→        assert len(result) == 32\n    74→\n    75→    def test_compute_hash_partial_fields(self) -> None:\n    76→        \"\"\"Test hash computation with some fields.\"\"\"\n    77→        hasher = ContentHasher()\n    78→        result = hasher.compute_hash(\n    79→            title=\"Test Title\",\n    80→            body_text=\"Test body\",\n    81→        )\n    82→\n    83→        assert result is not None\n    84→        assert len(result) == 32\n    85→\n    86→    def test_compute_hash_consistency(self) -> None:\n    87→        \"\"\"Test that same input produces same hash.\"\"\"\n    88→        hasher = ContentHasher()\n    89→        hash1 = hasher.compute_hash(\n    90→            title=\"Title\",\n    91→            h1=\"Heading\",\n    92→            meta_description=\"Description\",\n    93→            body_text=\"Body\",\n    94→        )\n    95→        hash2 = hasher.compute_hash(\n    96→            title=\"Title\",\n    97→            h1=\"Heading\",\n    98→            meta_description=\"Description\",\n    99→            body_text=\"Body\",\n   100→        )\n   101→\n   102→        assert hash1 == hash2\n   103→\n   104→    def test_compute_hash_different_inputs(self) -> None:\n   105→        \"\"\"Test that different inputs produce different hashes.\"\"\"\n   106→        hasher = ContentHasher()\n   107→        hash1 = hasher.compute_hash(title=\"Title A\")\n   108→        hash2 = hasher.compute_hash(title=\"Title B\")\n   109→\n   110→        assert hash1 != hash2\n   111→\n   112→    def test_compute_hash_whitespace_normalization(self) -> None:\n   113→        \"\"\"Test whitespace normalization in hashing.\"\"\"\n   114→        hasher = ContentHasher(include_whitespace=False)\n   115→        hash1 = hasher.compute_hash(title=\"Test  Title\")\n   116→        hash2 = hasher.compute_hash(title=\"Test Title\")\n   117→\n   118→        assert hash1 == hash2\n   119→\n   120→    def test_compute_hash_case_normalization(self) -> None:\n   121→        \"\"\"Test case normalization in hashing.\"\"\"\n   122→        hasher = ContentHasher()\n   123→        hash1 = hasher.compute_hash(title=\"Test Title\")\n   124→        hash2 = hasher.compute_hash(title=\"test title\")\n   125→\n   126→        assert hash1 == hash2\n   127→\n   128→    def test_compute_hash_body_truncation(self) -> None:\n   129→        \"\"\"Test body text truncation.\"\"\"\n   130→        hasher = ContentHasher(max_content_length=100)\n   131→        long_body = \"x\" * 1000\n   132→\n   133→        # Hash with truncated body\n   134→        hash1 = hasher.compute_hash(body_text=long_body)\n   135→\n   136→        # Hash with exactly max length body\n   137→        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n   138→\n   139→        assert hash1 == hash2\n   140→\n   141→    def test_compute_hash_with_ids(self) -> None:\n   142→        \"\"\"Test hash computation with logging IDs.\"\"\"\n   143→        hasher = ContentHasher()\n   144→        result = hasher.compute_hash(\n   145→            title=\"Test\",\n   146→            project_id=\"proj-123\",\n   147→            page_id=\"page-456\",\n   148→        )\n   149→\n   150→        assert result is not None\n   151→        assert len(result) == 32\n   152→\n   153→    def test_compute_hash_from_dict(self) -> None:\n   154→        \"\"\"Test hash computation from dictionary.\"\"\"\n   155→        hasher = ContentHasher()\n   156→\n   157→        data = {\n   158→            \"title\": \"Test Title\",\n   159→            \"h1\": \"Test Heading\",\n   160→            \"meta_description\": \"Test description\",\n   161→            \"body_text\": \"Test body\",\n   162→        }\n   163→\n   164→        result = hasher.compute_hash_from_dict(data)\n   165→        expected = hasher.compute_hash(\n   166→            title=\"Test Title\",\n   167→            h1=\"Test Heading\",\n   168→            meta_description=\"Test description\",\n   169→            body_text=\"Test body\",\n   170→        )\n   171→\n   172→        assert result == expected\n   173→\n   174→    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n   175→        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n   176→        hasher = ContentHasher()\n   177→\n   178→        data = {\n   179→            \"page_title\": \"Test Title\",\n   180→            \"heading\": \"Test Heading\",\n   181→            \"description\": \"Test description\",\n   182→            \"content\": \"Test body\",\n   183→        }\n   184→\n   185→        result = hasher.compute_hash_from_dict(data)\n   186→\n   187→        assert result is not None\n   188→        assert len(result) == 32\n   189→\n   190→    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n   191→        \"\"\"Test hash from dict with headings list.\"\"\"\n   192→        hasher = ContentHasher()\n   193→\n   194→        data = {\n   195→            \"title\": \"Test Title\",\n   196→            \"headings\": [\"First Heading\", \"Second Heading\"],\n   197→            \"body_text\": \"Test body\",\n   198→        }\n   199→\n   200→        result = hasher.compute_hash_from_dict(data)\n   201→\n   202→        assert result is not None\n   203→        assert len(result) == 32\n   204→\n   205→    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n   206→        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n   207→        hasher = ContentHasher(include_whitespace=True)\n   208→        hash1 = hasher.compute_hash(title=\"Test  Title\")\n   209→        hash2 = hasher.compute_hash(title=\"Test Title\")\n   210→\n   211→        assert hash1 != hash2\n   212→\n   213→\n   214→class TestPageSnapshot:\n   215→    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n   216→\n   217→    def test_basic_creation(self) -> None:\n   218→        \"\"\"Test basic PageSnapshot creation.\"\"\"\n   219→        snapshot = PageSnapshot(\n   220→            url=\"https://example.com/page\",\n   221→            content_hash=\"abc123\",\n   222→        )\n   223→\n   224→        assert snapshot.url == \"https://example.com/page\"\n   225→        assert snapshot.content_hash == \"abc123\"\n   226→        assert snapshot.title is None\n   227→        assert snapshot.page_id is None\n   228→\n   229→    def test_full_creation(self) -> None:\n   230→        \"\"\"Test PageSnapshot with all fields.\"\"\"\n   231→        snapshot = PageSnapshot(\n   232→            url=\"https://example.com/page\",\n   233→            content_hash=\"abc123\",\n   234→            title=\"Test Page\",\n   235→            page_id=\"page-456\",\n   236→        )\n   237→\n   238→        assert snapshot.title == \"Test Page\"\n   239→        assert snapshot.page_id == \"page-456\"\n   240→\n   241→    def test_to_dict(self) -> None:\n   242→        \"\"\"Test conversion to dictionary.\"\"\"\n   243→        snapshot = PageSnapshot(\n   244→            url=\"https://example.com/page\",\n   245→            content_hash=\"abc123\",\n   246→            title=\"Test Page\",\n   247→            page_id=\"page-456\",\n   248→        )\n   249→\n   250→        result = snapshot.to_dict()\n   251→\n   252→        assert result[\"url\"] == \"https://example.com/page\"\n   253→        assert result[\"content_hash\"] == \"abc123\"\n   254→        assert result[\"title\"] == \"Test Page\"\n   255→        assert result[\"page_id\"] == \"page-456\"\n   256→\n   257→    def test_to_dict_truncates_long_title(self) -> None:\n   258→        \"\"\"Test that long titles are truncated in dict.\"\"\"\n   259→        long_title = \"x\" * 200\n   260→        snapshot = PageSnapshot(\n   261→            url=\"https://example.com/page\",\n   262→            content_hash=\"abc123\",\n   263→            title=long_title,\n   264→        )\n   265→\n   266→        result = snapshot.to_dict()\n   267→\n   268→        assert len(result[\"title\"]) == 100\n   269→\n   270→\n   271→class TestPageChange:\n   272→    \"\"\"Tests for PageChange dataclass.\"\"\"\n   273→\n   274→    def test_new_page_change(self) -> None:\n   275→        \"\"\"Test PageChange for new page.\"\"\"\n   276→        change = PageChange(\n   277→            url=\"https://example.com/new\",\n   278→            change_type=ChangeType.NEW,\n   279→            new_hash=\"abc123\",\n   280→        )\n   281→\n   282→        assert change.change_type == ChangeType.NEW\n   283→        assert change.old_hash is None\n   284→        assert change.new_hash == \"abc123\"\n   285→\n   286→    def test_removed_page_change(self) -> None:\n   287→        \"\"\"Test PageChange for removed page.\"\"\"\n   288→        change = PageChange(\n   289→            url=\"https://example.com/removed\",\n   290→            change_type=ChangeType.REMOVED,\n   291→            old_hash=\"abc123\",\n   292→        )\n   293→\n   294→        assert change.change_type == ChangeType.REMOVED\n   295→        assert change.old_hash == \"abc123\"\n   296→        assert change.new_hash is None\n   297→\n   298→    def test_changed_page_change(self) -> None:\n   299→        \"\"\"Test PageChange for changed page.\"\"\"\n   300→        change = PageChange(\n   301→            url=\"https://example.com/changed\",\n   302→            change_type=ChangeType.CHANGED,\n   303→            old_hash=\"old123\",\n   304→            new_hash=\"new456\",\n   305→        )\n   306→\n   307→        assert change.change_type == ChangeType.CHANGED\n   308→        assert change.old_hash == \"old123\"\n   309→        assert change.new_hash == \"new456\"\n   310→\n   311→    def test_to_dict(self) -> None:\n   312→        \"\"\"Test conversion to dictionary.\"\"\"\n   313→        change = PageChange(\n   314→            url=\"https://example.com/page\",\n   315→            change_type=ChangeType.CHANGED,\n   316→            old_hash=\"old\",\n   317→            new_hash=\"new\",\n   318→            title=\"Test\",\n   319→            page_id=\"page-123\",\n   320→        )\n   321→\n   322→        result = change.to_dict()\n   323→\n   324→        assert result[\"change_type\"] == \"changed\"\n   325→        assert result[\"url\"] == \"https://example.com/page\"\n   326→\n   327→\n   328→class TestChangeSummary:\n   329→    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n   330→\n   331→    def test_basic_creation(self) -> None:\n   332→        \"\"\"Test basic ChangeSummary creation.\"\"\"\n   333→        summary = ChangeSummary(\n   334→            crawl_id=\"crawl-new\",\n   335→            compared_to=\"crawl-old\",\n   336→        )\n   337→\n   338→        assert summary.crawl_id == \"crawl-new\"\n   339→        assert summary.compared_to == \"crawl-old\"\n   340→        assert summary.computed_at is not None\n   341→\n   342→    def test_total_pages(self) -> None:\n   343→        \"\"\"Test total_pages property.\"\"\"\n   344→        summary = ChangeSummary(\n   345→            crawl_id=\"new\",\n   346→            compared_to=\"old\",\n   347→            new_pages=5,\n   348→            changed_pages=3,\n   349→            unchanged_pages=10,\n   350→        )\n   351→\n   352→        assert summary.total_pages == 18  # 5 + 3 + 10\n   353→\n   354→    def test_total_changes(self) -> None:\n   355→        \"\"\"Test total_changes property.\"\"\"\n   356→        summary = ChangeSummary(\n   357→            crawl_id=\"new\",\n   358→            compared_to=\"old\",\n   359→            new_pages=5,\n   360→            removed_pages=2,\n   361→            changed_pages=3,\n   362→        )\n   363→\n   364→        assert summary.total_changes == 10  # 5 + 2 + 3\n   365→\n   366→    def test_change_percentage(self) -> None:\n   367→        \"\"\"Test change_percentage calculation.\"\"\"\n   368→        summary = ChangeSummary(\n   369→            crawl_id=\"new\",\n   370→            compared_to=\"old\",\n   371→            changed_pages=10,\n   372→            unchanged_pages=90,\n   373→        )\n   374→\n   375→        assert summary.change_percentage == 0.1  # 10%\n   376→\n   377→    def test_change_percentage_zero_pages(self) -> None:\n   378→        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n   379→        summary = ChangeSummary(\n   380→            crawl_id=\"new\",\n   381→            compared_to=\"old\",\n   382→            changed_pages=0,\n   383→            unchanged_pages=0,\n   384→        )\n   385→\n   386→        assert summary.change_percentage == 0.0\n   387→\n   388→    def test_to_dict(self) -> None:\n   389→        \"\"\"Test conversion to dictionary.\"\"\"\n   390→        summary = ChangeSummary(\n   391→            crawl_id=\"new\",\n   392→            compared_to=\"old\",\n   393→            new_pages=5,\n   394→            removed_pages=2,\n   395→            changed_pages=10,\n   396→            unchanged_pages=83,\n   397→            new_page_urls=[\"url1\", \"url2\"],\n   398→            removed_page_urls=[\"url3\"],\n   399→            changed_page_urls=[\"url4\", \"url5\"],\n   400→            is_significant=True,\n   401→        )\n   402→\n   403→        result = summary.to_dict()\n   404→\n   405→        assert result[\"crawl_id\"] == \"new\"\n   406→        assert result[\"compared_to\"] == \"old\"\n   407→        assert result[\"summary\"][\"new_pages\"] == 5\n   408→        assert result[\"summary\"][\"removed_pages\"] == 2\n   409→        assert result[\"summary\"][\"changed_pages\"] == 10\n   410→        assert result[\"summary\"][\"unchanged_pages\"] == 83\n   411→        assert result[\"is_significant\"] is True\n   412→        assert result[\"total_pages\"] == 98\n   413→        assert result[\"total_changes\"] == 17\n   414→\n   415→    def test_to_dict_truncates_urls(self) -> None:\n   416→        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n   417→        urls = [f\"url{i}\" for i in range(200)]\n   418→        summary = ChangeSummary(\n   419→            crawl_id=\"new\",\n   420→            compared_to=\"old\",\n   421→            new_page_urls=urls,\n   422→        )\n   423→\n   424→        result = summary.to_dict()\n   425→\n   426→        assert len(result[\"new_page_urls\"]) == 100\n   427→\n   428→\n   429→class TestChangeDetector:\n   430→    \"\"\"Tests for ChangeDetector class.\"\"\"\n   431→\n   432→    def test_init_default(self) -> None:\n   433→        \"\"\"Test default initialization.\"\"\"\n   434→        detector = ChangeDetector()\n   435→        assert detector.new_page_threshold == 5\n   436→        assert detector.change_percentage_threshold == 0.10\n   437→\n   438→    def test_init_custom_thresholds(self) -> None:\n   439→        \"\"\"Test initialization with custom thresholds.\"\"\"\n   440→        detector = ChangeDetector(\n   441→            new_page_threshold=10,\n   442→            change_percentage_threshold=0.20,\n   443→        )\n   444→\n   445→        assert detector.new_page_threshold == 10\n   446→        assert detector.change_percentage_threshold == 0.20\n   447→\n   448→    def test_init_invalid_new_page_threshold(self) -> None:\n   449→        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n   450→        with pytest.raises(ValueError, match=\"non-negative\"):\n   451→            ChangeDetector(new_page_threshold=-1)\n   452→\n   453→    def test_init_invalid_change_percentage_threshold(self) -> None:\n   454→        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n   455→        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n   456→            ChangeDetector(change_percentage_threshold=1.5)\n   457→\n   458→    def test_compare_empty_crawls(self) -> None:\n   459→        \"\"\"Test comparison of empty crawls.\"\"\"\n   460→        detector = ChangeDetector()\n   461→\n   462→        result = detector.compare(\n   463→            crawl_id=\"new\",\n   464→            previous_crawl_id=\"old\",\n   465→            previous_pages=[],\n   466→            current_pages=[],\n   467→        )\n   468→\n   469→        assert result.new_pages == 0\n   470→        assert result.removed_pages == 0\n   471→        assert result.changed_pages == 0\n   472→        assert result.unchanged_pages == 0\n   473→\n   474→    def test_compare_all_new_pages(self) -> None:\n   475→        \"\"\"Test comparison where all pages are new.\"\"\"\n   476→        detector = ChangeDetector()\n   477→\n   478→        current = [\n   479→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   480→            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n   481→        ]\n   482→\n   483→        result = detector.compare(\n   484→            crawl_id=\"new\",\n   485→            previous_crawl_id=\"old\",\n   486→            previous_pages=[],\n   487→            current_pages=current,\n   488→        )\n   489→\n   490→        assert result.new_pages == 2\n   491→        assert result.removed_pages == 0\n   492→        assert result.changed_pages == 0\n   493→        assert \"url1\" in result.new_page_urls\n   494→        assert \"url2\" in result.new_page_urls\n   495→\n   496→    def test_compare_all_removed_pages(self) -> None:\n   497→        \"\"\"Test comparison where all pages are removed.\"\"\"\n   498→        detector = ChangeDetector()\n   499→\n   500→        previous = [\n   501→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   502→            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n   503→        ]\n   504→\n   505→        result = detector.compare(\n   506→            crawl_id=\"new\",\n   507→            previous_crawl_id=\"old\",\n   508→            previous_pages=previous,\n   509→            current_pages=[],\n   510→        )\n   511→\n   512→        assert result.new_pages == 0\n   513→        assert result.removed_pages == 2\n   514→        assert \"url1\" in result.removed_page_urls\n   515→        assert \"url2\" in result.removed_page_urls\n   516→\n   517→    def test_compare_unchanged_pages(self) -> None:\n   518→        \"\"\"Test comparison with unchanged pages.\"\"\"\n   519→        detector = ChangeDetector()\n   520→\n   521→        pages = [\n   522→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   523→            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n   524→        ]\n   525→\n   526→        result = detector.compare(\n   527→            crawl_id=\"new\",\n   528→            previous_crawl_id=\"old\",\n   529→            previous_pages=pages,\n   530→            current_pages=pages,\n   531→        )\n   532→\n   533→        assert result.new_pages == 0\n   534→        assert result.removed_pages == 0\n   535→        assert result.changed_pages == 0\n   536→        assert result.unchanged_pages == 2\n   537→\n   538→    def test_compare_changed_pages(self) -> None:\n   539→        \"\"\"Test comparison with changed pages.\"\"\"\n   540→        detector = ChangeDetector()\n   541→\n   542→        previous = [\n   543→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   544→            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n   545→        ]\n   546→\n   547→        current = [\n   548→            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n   549→            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n   550→        ]\n   551→\n   552→        result = detector.compare(\n   553→            crawl_id=\"new\",\n   554→            previous_crawl_id=\"old\",\n   555→            previous_pages=previous,\n   556→            current_pages=current,\n   557→        )\n   558→\n   559→        assert result.changed_pages == 2\n   560→        assert result.unchanged_pages == 0\n   561→        assert \"url1\" in result.changed_page_urls\n   562→        assert \"url2\" in result.changed_page_urls\n   563→\n   564→    def test_compare_mixed_changes(self) -> None:\n   565→        \"\"\"Test comparison with mixed changes.\"\"\"\n   566→        detector = ChangeDetector()\n   567→\n   568→        previous = [\n   569→            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n   570→            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n   571→            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n   572→        ]\n   573→\n   574→        current = [\n   575→            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n   576→            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n   577→            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n   578→        ]\n   579→\n   580→        result = detector.compare(\n   581→            crawl_id=\"new\",\n   582→            previous_crawl_id=\"old\",\n   583→            previous_pages=previous,\n   584→            current_pages=current,\n   585→        )\n   586→\n   587→        assert result.new_pages == 1\n   588→        assert result.removed_pages == 1\n   589→        assert result.changed_pages == 1\n   590→        assert result.unchanged_pages == 1\n   591→        assert \"new\" in result.new_page_urls\n   592→        assert \"removed\" in result.removed_page_urls\n   593→        assert \"changed\" in result.changed_page_urls\n   594→\n   595→    def test_significance_by_new_pages(self) -> None:\n   596→        \"\"\"Test significance detection by new page count.\"\"\"\n   597→        detector = ChangeDetector(new_page_threshold=5)\n   598→\n   599→        current = [\n   600→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n   601→            for i in range(5)\n   602→        ]\n   603→\n   604→        result = detector.compare(\n   605→            crawl_id=\"new\",\n   606→            previous_crawl_id=\"old\",\n   607→            previous_pages=[],\n   608→            current_pages=current,\n   609→        )\n   610→\n   611→        assert result.is_significant is True\n   612→\n   613→    def test_significance_below_new_page_threshold(self) -> None:\n   614→        \"\"\"Test non-significance when below new page threshold.\"\"\"\n   615→        detector = ChangeDetector(new_page_threshold=5)\n   616→\n   617→        current = [\n   618→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n   619→            for i in range(4)\n   620→        ]\n   621→\n   622→        result = detector.compare(\n   623→            crawl_id=\"new\",\n   624→            previous_crawl_id=\"old\",\n   625→            previous_pages=[],\n   626→            current_pages=current,\n   627→        )\n   628→\n   629→        assert result.is_significant is False\n   630→\n   631→    def test_significance_by_change_percentage(self) -> None:\n   632→        \"\"\"Test significance detection by change percentage.\"\"\"\n   633→        detector = ChangeDetector(change_percentage_threshold=0.10)\n   634→\n   635→        previous = [\n   636→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n   637→            for i in range(10)\n   638→        ]\n   639→\n   640→        current = [\n   641→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n   642→            for i in range(10)\n   643→        ]\n   644→\n   645→        result = detector.compare(\n   646→            crawl_id=\"new\",\n   647→            previous_crawl_id=\"old\",\n   648→            previous_pages=previous,\n   649→            current_pages=current,\n   650→        )\n   651→\n   652→        # 1 changed out of 10 = 10% = threshold\n   653→        assert result.is_significant is True\n   654→\n   655→    def test_significance_below_change_percentage(self) -> None:\n   656→        \"\"\"Test non-significance when below change percentage.\"\"\"\n   657→        detector = ChangeDetector(change_percentage_threshold=0.20)\n   658→\n   659→        previous = [\n   660→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n   661→            for i in range(10)\n   662→        ]\n   663→\n   664→        current = [\n   665→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n   666→            for i in range(10)\n   667→        ]\n   668→\n   669→        result = detector.compare(\n   670→            crawl_id=\"new\",\n   671→            previous_crawl_id=\"old\",\n   672→            previous_pages=previous,\n   673→            current_pages=current,\n   674→        )\n   675→\n   676→        # 1 changed out of 10 = 10% < 20% threshold\n   677→        assert result.is_significant is False\n   678→\n   679→    def test_compare_with_project_id(self) -> None:\n   680→        \"\"\"Test comparison with project_id for logging.\"\"\"\n   681→        detector = ChangeDetector()\n   682→\n   683→        result = detector.compare(\n   684→            crawl_id=\"new\",\n   685→            previous_crawl_id=\"old\",\n   686→            previous_pages=[],\n   687→            current_pages=[],\n   688→            project_id=\"proj-123\",\n   689→        )\n   690→\n   691→        assert result.crawl_id == \"new\"\n   692→\n   693→    def test_compare_from_dicts(self) -> None:\n   694→        \"\"\"Test comparison from dictionary representations.\"\"\"\n   695→        detector = ChangeDetector()\n   696→\n   697→        previous = [\n   698→            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n   699→            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n   700→        ]\n   701→\n   702→        current = [\n   703→            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n   704→            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n   705→        ]\n   706→\n   707→        result = detector.compare_from_dicts(\n   708→            crawl_id=\"new\",\n   709→            previous_crawl_id=\"old\",\n   710→            previous_pages=previous,\n   711→            current_pages=current,\n   712→        )\n   713→\n   714→        assert result.new_pages == 1\n   715→        assert result.removed_pages == 1\n   716→        assert result.changed_pages == 1\n   717→        assert result.unchanged_pages == 0\n   718→\n   719→    def test_compare_from_dicts_custom_keys(self) -> None:\n   720→        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n   721→        detector = ChangeDetector()\n   722→\n   723→        previous = [\n   724→            {\"url\": \"url1\", \"hash\": \"hash1\"},\n   725→        ]\n   726→\n   727→        current = [\n   728→            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n   729→        ]\n   730→\n   731→        result = detector.compare_from_dicts(\n   732→            crawl_id=\"new\",\n   733→            previous_crawl_id=\"old\",\n   734→            previous_pages=previous,\n   735→            current_pages=current,\n   736→            url_key=\"url\",\n   737→            hash_key=\"hash\",\n   738→        )\n   739→\n   740→        assert result.changed_pages == 1\n   741→\n   742→    def test_compare_changes_list(self) -> None:\n   743→        \"\"\"Test that changes list contains all changes.\"\"\"\n   744→        detector = ChangeDetector()\n   745→\n   746→        previous = [\n   747→            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n   748→            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n   749→            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n   750→        ]\n   751→\n   752→        current = [\n   753→            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n   754→            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n   755→            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n   756→        ]\n   757→\n   758→        result = detector.compare(\n   759→            crawl_id=\"new\",\n   760→            previous_crawl_id=\"old\",\n   761→            previous_pages=previous,\n   762→            current_pages=current,\n   763→        )\n   764→\n   765→        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n   766→        assert len(result.changes) == 4\n   767→\n   768→        # Check change types\n   769→        change_types = {c.change_type for c in result.changes}\n   770→        assert ChangeType.NEW in change_types\n   771→        assert ChangeType.REMOVED in change_types\n   772→        assert ChangeType.CHANGED in change_types\n   773→        assert ChangeType.UNCHANGED in change_types\n   774→\n   775→    def test_compare_none_hash_treated_as_change(self) -> None:\n   776→        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n   777→        detector = ChangeDetector()\n   778→\n   779→        previous = [\n   780→            PageSnapshot(url=\"url1\", content_hash=None),\n   781→        ]\n   782→\n   783→        current = [\n   784→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   785→        ]\n   786→\n   787→        result = detector.compare(\n   788→            crawl_id=\"new\",\n   789→            previous_crawl_id=\"old\",\n   790→            previous_pages=previous,\n   791→            current_pages=current,\n   792→        )\n   793→\n   794→        assert result.changed_pages == 1\n   795→\n   796→    def test_compare_both_none_hash_is_unchanged(self) -> None:\n   797→        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n   798→        detector = ChangeDetector()\n   799→\n   800→        previous = [\n   801→            PageSnapshot(url=\"url1\", content_hash=None),\n   802→        ]\n   803→\n   804→        current = [\n   805→            PageSnapshot(url=\"url1\", content_hash=None),\n   806→        ]\n   807→\n   808→        result = detector.compare(\n   809→            crawl_id=\"new\",\n   810→            previous_crawl_id=\"old\",\n   811→            previous_pages=previous,\n   812→            current_pages=current,\n   813→        )\n   814→\n   815→        assert result.unchanged_pages == 1\n   816→\n   817→\n   818→class TestConvenienceFunctions:\n   819→    \"\"\"Tests for convenience functions.\"\"\"\n   820→\n   821→    def test_get_content_hasher_singleton(self) -> None:\n   822→        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n   823→        hasher1 = get_content_hasher()\n   824→        hasher2 = get_content_hasher()\n   825→\n   826→        assert hasher1 is hasher2\n   827→\n   828→    def test_get_change_detector_singleton(self) -> None:\n   829→        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n   830→        detector1 = get_change_detector()\n   831→        detector2 = get_change_detector()\n   832→\n   833→        assert detector1 is detector2\n   834→\n   835→    def test_compute_content_hash(self) -> None:\n   836→        \"\"\"Test compute_content_hash function.\"\"\"\n   837→        result = compute_content_hash(\n   838→            title=\"Test Title\",\n   839→            h1=\"Test Heading\",\n   840→            meta_description=\"Test description\",\n   841→            body_text=\"Test body\",\n   842→        )\n   843→\n   844→        assert result is not None\n   845→        assert len(result) == 32\n   846→\n   847→    def test_detect_changes(self) -> None:\n   848→        \"\"\"Test detect_changes function.\"\"\"\n   849→        previous = [\n   850→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   851→        ]\n   852→\n   853→        current = [\n   854→            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n   855→            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n   856→        ]\n   857→\n   858→        result = detect_changes(\n   859→            crawl_id=\"new\",\n   860→            previous_crawl_id=\"old\",\n   861→            previous_pages=previous,\n   862→            current_pages=current,\n   863→        )\n   864→\n   865→        assert result.new_pages == 1\n   866→        assert result.changed_pages == 1\n   867→\n   868→\n   869→class TestChangeType:\n   870→    \"\"\"Tests for ChangeType enum.\"\"\"\n   871→\n   872→    def test_valid_change_types(self) -> None:\n   873→        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n   874→        assert \"new\" in VALID_CHANGE_TYPES\n   875→        assert \"removed\" in VALID_CHANGE_TYPES\n   876→        assert \"changed\" in VALID_CHANGE_TYPES\n   877→        assert \"unchanged\" in VALID_CHANGE_TYPES\n   878→\n   879→    def test_change_type_values(self) -> None:\n   880→        \"\"\"Test ChangeType enum values.\"\"\"\n   881→        assert ChangeType.NEW.value == \"new\"\n   882→        assert ChangeType.REMOVED.value == \"removed\"\n   883→        assert ChangeType.CHANGED.value == \"changed\"\n   884→        assert ChangeType.UNCHANGED.value == \"unchanged\"\n   885→\n   886→\n   887→class TestEdgeCases:\n   888→    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n   889→\n   890→    def test_large_number_of_pages(self) -> None:\n   891→        \"\"\"Test comparison with large number of pages.\"\"\"\n   892→        detector = ChangeDetector()\n   893→\n   894→        previous = [\n   895→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n   896→            for i in range(1000)\n   897→        ]\n   898→\n   899→        current = [\n   900→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n   901→            for i in range(1000)\n   902→        ]\n   903→\n   904→        result = detector.compare(\n   905→            crawl_id=\"new\",\n   906→            previous_crawl_id=\"old\",\n   907→            previous_pages=previous,\n   908→            current_pages=current,\n   909→        )\n   910→\n   911→        assert result.changed_pages == 100\n   912→        assert result.unchanged_pages == 900\n   913→\n   914→    def test_unicode_content_hash(self) -> None:\n   915→        \"\"\"Test hash computation with unicode content.\"\"\"\n   916→        hasher = ContentHasher()\n   917→\n   918→        result = hasher.compute_hash(\n   919→            title=\"日本語タイトル\",\n   920→            h1=\"中文标题\",\n   921→            meta_description=\"Descripción en español\",\n   922→            body_text=\"Контент на русском языке\",\n   923→        )\n   924→\n   925→        assert result is not None\n   926→        assert len(result) == 32\n   927→\n   928→    def test_special_characters_in_content(self) -> None:\n   929→        \"\"\"Test hash computation with special characters.\"\"\"\n   930→        hasher = ContentHasher()\n   931→\n   932→        result = hasher.compute_hash(\n   933→            title=\"Title with <html> & \\\"quotes\\\"\",\n   934→            body_text=\"Body with\\n\\ttabs and newlines\",\n   935→        )\n   936→\n   937→        assert result is not None\n   938→        assert len(result) == 32\n   939→\n   940→    def test_duplicate_urls_in_pages(self) -> None:\n   941→        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n   942→        detector = ChangeDetector()\n   943→\n   944→        previous = [\n   945→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   946→            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n   947→        ]\n   948→\n   949→        current = [\n   950→            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n   951→        ]\n   952→\n   953→        result = detector.compare(\n   954→            crawl_id=\"new\",\n   955→            previous_crawl_id=\"old\",\n   956→            previous_pages=previous,\n   957→            current_pages=current,\n   958→        )\n   959→\n   960→        # Last duplicate wins, so should be unchanged\n   961→        assert result.unchanged_pages == 1\n   962→\n   963→    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n   964→        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n   965→        detector = ChangeDetector()\n   966→\n   967→        previous = [\n   968→            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n   969→            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n   970→        ]\n   971→\n   972→        current = [\n   973→            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n   974→        ]\n   975→\n   976→        result = detector.compare_from_dicts(\n   977→            crawl_id=\"new\",\n   978→            previous_crawl_id=\"old\",\n   979→            previous_pages=previous,\n   980→            current_pages=current,\n   981→        )\n   982→\n   983→        # Empty URL page should be filtered out\n   984→        assert result.unchanged_pages == 1\n   985→        assert result.removed_pages == 0\n   986→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"a8e41333-21ca-4090-bee2-f1dcfcef0721","tool_use_result":{"type":"text","file":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","content":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n","numLines":986,"startLine":1,"totalLines":986}}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_014o2ma5mJzSpiaXwVdLJpqC","type":"message","role":"assistant","content":[{"type":"text","text":"The test file already exists with comprehensive tests. Let me check the current test coverage to see if there are any gaps that need to be addressed."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":22705,"cache_read_input_tokens":27607,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":22705},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"d9fab246-c15c-4a72-a2d2-e3af8178996a"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_014o2ma5mJzSpiaXwVdLJpqC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UkEwpqi1d61r9fN7HKPEBv","name":"Bash","input":{"command":"cd /Users/mike/Projects/client-onboarding-v2/backend && python3 -m pytest tests/utils/test_change_detection.py -v --tb=short 2>&1 | head -100","description":"Run existing change detection tests"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":22705,"cache_read_input_tokens":27607,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":22705},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"a760c8b0-9636-4998-9858-189f60da2a0e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UkEwpqi1d61r9fN7HKPEBv","type":"tool_result","content":"============================= test session starts ==============================\nplatform darwin -- Python 3.12.8, pytest-9.0.2, pluggy-1.6.0 -- /Library/Frameworks/Python.framework/Versions/3.12/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/mike/Projects/client-onboarding-v2/backend\nconfigfile: pyproject.toml\nplugins: asyncio-1.3.0, anyio-3.7.1, cov-7.0.0\nasyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 62 items\n\ntests/utils/test_change_detection.py::TestContentHasher::test_init_default PASSED [  1%]\ntests/utils/test_change_detection.py::TestContentHasher::test_init_custom_length PASSED [  3%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_all_fields PASSED [  4%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_empty_fields PASSED [  6%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_none_fields PASSED [  8%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_partial_fields PASSED [  9%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_consistency PASSED [ 11%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_different_inputs PASSED [ 12%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_whitespace_normalization PASSED [ 14%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_case_normalization PASSED [ 16%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_body_truncation PASSED [ 17%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_with_ids PASSED [ 19%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict PASSED [ 20%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_alternative_keys PASSED [ 22%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_with_headings_list PASSED [ 24%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_preserves_whitespace_when_enabled PASSED [ 25%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_basic_creation PASSED [ 27%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_full_creation PASSED [ 29%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict PASSED [ 30%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict_truncates_long_title PASSED [ 32%]\ntests/utils/test_change_detection.py::TestPageChange::test_new_page_change PASSED [ 33%]\ntests/utils/test_change_detection.py::TestPageChange::test_removed_page_change PASSED [ 35%]\ntests/utils/test_change_detection.py::TestPageChange::test_changed_page_change PASSED [ 37%]\ntests/utils/test_change_detection.py::TestPageChange::test_to_dict PASSED [ 38%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_basic_creation PASSED [ 40%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_pages PASSED [ 41%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_changes PASSED [ 43%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage_zero_pages PASSED [ 46%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict_truncates_urls PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_default PASSED [ 51%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_custom_thresholds PASSED [ 53%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_new_page_threshold PASSED [ 54%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_change_percentage_threshold PASSED [ 56%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_empty_crawls PASSED [ 58%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_new_pages PASSED [ 59%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_removed_pages PASSED [ 61%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_unchanged_pages PASSED [ 62%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 64%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 66%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 67%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 69%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 70%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_change_percentage PASSED [ 72%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_with_project_id PASSED [ 74%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts PASSED [ 75%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts_custom_keys PASSED [ 77%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changes_list PASSED [ 79%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_none_hash_treated_as_change PASSED [ 80%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_both_none_hash_is_unchanged PASSED [ 82%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_content_hasher_singleton PASSED [ 83%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_change_detector_singleton PASSED [ 85%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_compute_content_hash PASSED [ 87%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_detect_changes PASSED [ 88%]\ntests/utils/test_change_detection.py::TestChangeType::test_valid_change_types PASSED [ 90%]\ntests/utils/test_change_detection.py::TestChangeType::test_change_type_values PASSED [ 91%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_large_number_of_pages PASSED [ 93%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_unicode_content_hash PASSED [ 95%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 96%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 98%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [100%]\n\n============================== 62 passed in 0.03s ==============================","is_error":false}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"ea3fbc3f-c1b3-407a-9a10-60b2210edf58","tool_use_result":{"stdout":"============================= test session starts ==============================\nplatform darwin -- Python 3.12.8, pytest-9.0.2, pluggy-1.6.0 -- /Library/Frameworks/Python.framework/Versions/3.12/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/mike/Projects/client-onboarding-v2/backend\nconfigfile: pyproject.toml\nplugins: asyncio-1.3.0, anyio-3.7.1, cov-7.0.0\nasyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 62 items\n\ntests/utils/test_change_detection.py::TestContentHasher::test_init_default PASSED [  1%]\ntests/utils/test_change_detection.py::TestContentHasher::test_init_custom_length PASSED [  3%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_all_fields PASSED [  4%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_empty_fields PASSED [  6%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_none_fields PASSED [  8%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_partial_fields PASSED [  9%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_consistency PASSED [ 11%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_different_inputs PASSED [ 12%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_whitespace_normalization PASSED [ 14%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_case_normalization PASSED [ 16%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_body_truncation PASSED [ 17%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_with_ids PASSED [ 19%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict PASSED [ 20%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_alternative_keys PASSED [ 22%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_with_headings_list PASSED [ 24%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_preserves_whitespace_when_enabled PASSED [ 25%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_basic_creation PASSED [ 27%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_full_creation PASSED [ 29%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict PASSED [ 30%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict_truncates_long_title PASSED [ 32%]\ntests/utils/test_change_detection.py::TestPageChange::test_new_page_change PASSED [ 33%]\ntests/utils/test_change_detection.py::TestPageChange::test_removed_page_change PASSED [ 35%]\ntests/utils/test_change_detection.py::TestPageChange::test_changed_page_change PASSED [ 37%]\ntests/utils/test_change_detection.py::TestPageChange::test_to_dict PASSED [ 38%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_basic_creation PASSED [ 40%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_pages PASSED [ 41%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_changes PASSED [ 43%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage_zero_pages PASSED [ 46%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict_truncates_urls PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_default PASSED [ 51%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_custom_thresholds PASSED [ 53%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_new_page_threshold PASSED [ 54%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_change_percentage_threshold PASSED [ 56%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_empty_crawls PASSED [ 58%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_new_pages PASSED [ 59%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_removed_pages PASSED [ 61%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_unchanged_pages PASSED [ 62%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 64%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 66%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 67%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 69%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 70%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_change_percentage PASSED [ 72%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_with_project_id PASSED [ 74%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts PASSED [ 75%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts_custom_keys PASSED [ 77%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changes_list PASSED [ 79%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_none_hash_treated_as_change PASSED [ 80%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_both_none_hash_is_unchanged PASSED [ 82%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_content_hasher_singleton PASSED [ 83%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_change_detector_singleton PASSED [ 85%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_compute_content_hash PASSED [ 87%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_detect_changes PASSED [ 88%]\ntests/utils/test_change_detection.py::TestChangeType::test_valid_change_types PASSED [ 90%]\ntests/utils/test_change_detection.py::TestChangeType::test_change_type_values PASSED [ 91%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_large_number_of_pages PASSED [ 93%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_unicode_content_hash PASSED [ 95%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 96%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 98%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [100%]\n\n============================== 62 passed in 0.03s ==============================","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Sm7DjrYZv9yJQB3LGjgNxB","type":"message","role":"assistant","content":[{"type":"text","text":"All 62 tests pass. Let me check the coverage to see if there are any gaps."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2451,"cache_read_input_tokens":50312,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2451},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"7472b749-4c84-4201-8160-685bb6062bab"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Sm7DjrYZv9yJQB3LGjgNxB","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01QYCq1WeMw6XS9BUycbKvMp","name":"Bash","input":{"command":"python3 -m pytest tests/utils/test_change_detection.py -v --cov=app.utils.change_detection --cov-report=term-missing 2>&1","description":"Run tests with coverage report"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2451,"cache_read_input_tokens":50312,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2451},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"3cce7169-430b-4f26-b20a-de133a0fa877"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QYCq1WeMw6XS9BUycbKvMp","type":"tool_result","content":"============================= test session starts ==============================\nplatform darwin -- Python 3.12.8, pytest-9.0.2, pluggy-1.6.0 -- /Library/Frameworks/Python.framework/Versions/3.12/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/mike/Projects/client-onboarding-v2/backend\nconfigfile: pyproject.toml\nplugins: asyncio-1.3.0, anyio-3.7.1, cov-7.0.0\nasyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 62 items\n\ntests/utils/test_change_detection.py::TestContentHasher::test_init_default PASSED [  1%]\ntests/utils/test_change_detection.py::TestContentHasher::test_init_custom_length PASSED [  3%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_all_fields PASSED [  4%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_empty_fields PASSED [  6%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_none_fields PASSED [  8%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_partial_fields PASSED [  9%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_consistency PASSED [ 11%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_different_inputs PASSED [ 12%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_whitespace_normalization PASSED [ 14%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_case_normalization PASSED [ 16%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_body_truncation PASSED [ 17%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_with_ids PASSED [ 19%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict PASSED [ 20%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_alternative_keys PASSED [ 22%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_with_headings_list PASSED [ 24%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_preserves_whitespace_when_enabled PASSED [ 25%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_basic_creation PASSED [ 27%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_full_creation PASSED [ 29%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict PASSED [ 30%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict_truncates_long_title PASSED [ 32%]\ntests/utils/test_change_detection.py::TestPageChange::test_new_page_change PASSED [ 33%]\ntests/utils/test_change_detection.py::TestPageChange::test_removed_page_change PASSED [ 35%]\ntests/utils/test_change_detection.py::TestPageChange::test_changed_page_change PASSED [ 37%]\ntests/utils/test_change_detection.py::TestPageChange::test_to_dict PASSED [ 38%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_basic_creation PASSED [ 40%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_pages PASSED [ 41%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_changes PASSED [ 43%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage_zero_pages PASSED [ 46%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict_truncates_urls PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_default PASSED [ 51%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_custom_thresholds PASSED [ 53%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_new_page_threshold PASSED [ 54%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_change_percentage_threshold PASSED [ 56%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_empty_crawls PASSED [ 58%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_new_pages PASSED [ 59%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_removed_pages PASSED [ 61%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_unchanged_pages PASSED [ 62%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 64%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 66%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 67%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 69%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 70%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_change_percentage PASSED [ 72%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_with_project_id PASSED [ 74%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts PASSED [ 75%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts_custom_keys PASSED [ 77%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changes_list PASSED [ 79%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_none_hash_treated_as_change PASSED [ 80%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_both_none_hash_is_unchanged PASSED [ 82%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_content_hasher_singleton PASSED [ 83%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_change_detector_singleton PASSED [ 85%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_compute_content_hash PASSED [ 87%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_detect_changes PASSED [ 88%]\ntests/utils/test_change_detection.py::TestChangeType::test_valid_change_types PASSED [ 90%]\ntests/utils/test_change_detection.py::TestChangeType::test_change_type_values PASSED [ 91%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_large_number_of_pages PASSED [ 93%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_unicode_content_hash PASSED [ 95%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 96%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 98%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [100%]\n\n================================ tests coverage ================================\n_______________ coverage: platform darwin, python 3.12.8-final-0 _______________\n\nName                            Stmts   Miss  Cover   Missing\n-------------------------------------------------------------\napp/utils/change_detection.py     207      8    96%   330, 342-354, 656, 668-681\n-------------------------------------------------------------\nTOTAL                             207      8    96%\n============================== 62 passed in 0.07s ==============================","is_error":false}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"daf4fb26-36ca-4ae3-8af3-e961a99d2292","tool_use_result":{"stdout":"============================= test session starts ==============================\nplatform darwin -- Python 3.12.8, pytest-9.0.2, pluggy-1.6.0 -- /Library/Frameworks/Python.framework/Versions/3.12/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/mike/Projects/client-onboarding-v2/backend\nconfigfile: pyproject.toml\nplugins: asyncio-1.3.0, anyio-3.7.1, cov-7.0.0\nasyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 62 items\n\ntests/utils/test_change_detection.py::TestContentHasher::test_init_default PASSED [  1%]\ntests/utils/test_change_detection.py::TestContentHasher::test_init_custom_length PASSED [  3%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_all_fields PASSED [  4%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_empty_fields PASSED [  6%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_none_fields PASSED [  8%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_partial_fields PASSED [  9%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_consistency PASSED [ 11%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_different_inputs PASSED [ 12%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_whitespace_normalization PASSED [ 14%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_case_normalization PASSED [ 16%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_body_truncation PASSED [ 17%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_with_ids PASSED [ 19%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict PASSED [ 20%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_alternative_keys PASSED [ 22%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_with_headings_list PASSED [ 24%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_preserves_whitespace_when_enabled PASSED [ 25%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_basic_creation PASSED [ 27%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_full_creation PASSED [ 29%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict PASSED [ 30%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict_truncates_long_title PASSED [ 32%]\ntests/utils/test_change_detection.py::TestPageChange::test_new_page_change PASSED [ 33%]\ntests/utils/test_change_detection.py::TestPageChange::test_removed_page_change PASSED [ 35%]\ntests/utils/test_change_detection.py::TestPageChange::test_changed_page_change PASSED [ 37%]\ntests/utils/test_change_detection.py::TestPageChange::test_to_dict PASSED [ 38%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_basic_creation PASSED [ 40%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_pages PASSED [ 41%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_changes PASSED [ 43%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage_zero_pages PASSED [ 46%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict_truncates_urls PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_default PASSED [ 51%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_custom_thresholds PASSED [ 53%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_new_page_threshold PASSED [ 54%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_change_percentage_threshold PASSED [ 56%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_empty_crawls PASSED [ 58%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_new_pages PASSED [ 59%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_removed_pages PASSED [ 61%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_unchanged_pages PASSED [ 62%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 64%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 66%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 67%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 69%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 70%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_change_percentage PASSED [ 72%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_with_project_id PASSED [ 74%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts PASSED [ 75%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts_custom_keys PASSED [ 77%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changes_list PASSED [ 79%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_none_hash_treated_as_change PASSED [ 80%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_both_none_hash_is_unchanged PASSED [ 82%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_content_hasher_singleton PASSED [ 83%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_change_detector_singleton PASSED [ 85%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_compute_content_hash PASSED [ 87%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_detect_changes PASSED [ 88%]\ntests/utils/test_change_detection.py::TestChangeType::test_valid_change_types PASSED [ 90%]\ntests/utils/test_change_detection.py::TestChangeType::test_change_type_values PASSED [ 91%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_large_number_of_pages PASSED [ 93%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_unicode_content_hash PASSED [ 95%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 96%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 98%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [100%]\n\n================================ tests coverage ================================\n_______________ coverage: platform darwin, python 3.12.8-final-0 _______________\n\nName                            Stmts   Miss  Cover   Missing\n-------------------------------------------------------------\napp/utils/change_detection.py     207      8    96%   330, 342-354, 656, 668-681\n-------------------------------------------------------------\nTOTAL                             207      8    96%\n============================== 62 passed in 0.07s ==============================","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01MUK1qWRMMv64hBVVm5NcQQ","type":"message","role":"assistant","content":[{"type":"text","text":"Coverage is at 96% which exceeds the 80% target. The uncovered lines are:\n- Lines 330, 342-354: Slow operation warning logging (triggered when operation takes >1s)\n- Lines 656, 668-681: Error handling paths for exceptions in `compare()` method\n\nThese are mostly logging paths for rare conditions (slow operations, exceptions). The existing test suite is comprehensive. However, the task description mentions ERROR LOGGING REQUIREMENTS that should be verified. Let me check if there are additional tests needed for:\n1. Error logging verification with `caplog`\n2. Slow operation timing logs\n3. Exception handling paths\n\nLet me add tests for these missing coverage areas and also add tests that verify the ERROR LOGGING REQUIREMENTS."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2528,"cache_read_input_tokens":52763,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2528},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"da181467-0ab6-4091-af0f-c55917f5f085"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01MUK1qWRMMv64hBVVm5NcQQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XWV9yiYfCxmDuS4qsC5x41","name":"TodoWrite","input":{"todos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"in_progress","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"pending","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"pending","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run quality checks (typecheck, lint)","status":"pending","activeForm":"Running quality checks"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2528,"cache_read_input_tokens":52763,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2528},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"3b3123c7-4b3f-4cca-b68c-5416055d5629"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XWV9yiYfCxmDuS4qsC5x41","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"9396bf3b-1bb9-4e51-b23e-e1e2e201321b","tool_use_result":{"oldTodos":[],"newTodos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"in_progress","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"pending","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"pending","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run quality checks (typecheck, lint)","status":"pending","activeForm":"Running quality checks"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01DkRTh2RXVx1ZJNBUZC2srt","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Rjx2nFzeZZuEVBBk2mmLSL","name":"Read","input":{"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","offset":900}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":421,"cache_read_input_tokens":55291,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":421},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"08083dae-87da-4f01-9aff-971acf25a6dd"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Rjx2nFzeZZuEVBBk2mmLSL","type":"tool_result","content":"   900→            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n   901→            for i in range(1000)\n   902→        ]\n   903→\n   904→        result = detector.compare(\n   905→            crawl_id=\"new\",\n   906→            previous_crawl_id=\"old\",\n   907→            previous_pages=previous,\n   908→            current_pages=current,\n   909→        )\n   910→\n   911→        assert result.changed_pages == 100\n   912→        assert result.unchanged_pages == 900\n   913→\n   914→    def test_unicode_content_hash(self) -> None:\n   915→        \"\"\"Test hash computation with unicode content.\"\"\"\n   916→        hasher = ContentHasher()\n   917→\n   918→        result = hasher.compute_hash(\n   919→            title=\"日本語タイトル\",\n   920→            h1=\"中文标题\",\n   921→            meta_description=\"Descripción en español\",\n   922→            body_text=\"Контент на русском языке\",\n   923→        )\n   924→\n   925→        assert result is not None\n   926→        assert len(result) == 32\n   927→\n   928→    def test_special_characters_in_content(self) -> None:\n   929→        \"\"\"Test hash computation with special characters.\"\"\"\n   930→        hasher = ContentHasher()\n   931→\n   932→        result = hasher.compute_hash(\n   933→            title=\"Title with <html> & \\\"quotes\\\"\",\n   934→            body_text=\"Body with\\n\\ttabs and newlines\",\n   935→        )\n   936→\n   937→        assert result is not None\n   938→        assert len(result) == 32\n   939→\n   940→    def test_duplicate_urls_in_pages(self) -> None:\n   941→        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n   942→        detector = ChangeDetector()\n   943→\n   944→        previous = [\n   945→            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n   946→            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n   947→        ]\n   948→\n   949→        current = [\n   950→            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n   951→        ]\n   952→\n   953→        result = detector.compare(\n   954→            crawl_id=\"new\",\n   955→            previous_crawl_id=\"old\",\n   956→            previous_pages=previous,\n   957→            current_pages=current,\n   958→        )\n   959→\n   960→        # Last duplicate wins, so should be unchanged\n   961→        assert result.unchanged_pages == 1\n   962→\n   963→    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n   964→        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n   965→        detector = ChangeDetector()\n   966→\n   967→        previous = [\n   968→            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n   969→            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n   970→        ]\n   971→\n   972→        current = [\n   973→            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n   974→        ]\n   975→\n   976→        result = detector.compare_from_dicts(\n   977→            crawl_id=\"new\",\n   978→            previous_crawl_id=\"old\",\n   979→            previous_pages=previous,\n   980→            current_pages=current,\n   981→        )\n   982→\n   983→        # Empty URL page should be filtered out\n   984→        assert result.unchanged_pages == 1\n   985→        assert result.removed_pages == 0\n   986→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"ade11588-4e8b-4359-a53e-318f26d10823","tool_use_result":{"type":"text","file":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","content":"            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n","numLines":87,"startLine":900,"totalLines":986}}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_012eBFqKpqxiLvGNAJnYQgrV","type":"message","role":"assistant","content":[{"type":"text","text":"Now I'll add comprehensive tests for error logging verification, slow operation warnings, and exception handling. I'll append these to the existing test file."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1207,"cache_read_input_tokens":55712,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1207},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"58ee59b5-a38d-46fa-ae2a-a1b05449fb41"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_012eBFqKpqxiLvGNAJnYQgrV","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TZ53VSnrfBka7r9kGdpNmq","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n","new_string":"        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-123\" in log_text, \"project_id should be logged\"\n        assert \"page-456\" in log_text, \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        log_text = caplog.text\n        assert \"Content hash computed\" in log_text or \"compute_hash\" in log_text\n        assert \"duration_ms\" in log_text, \"Should log timing information\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 1, \"Should have debug log entries\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-789\" in log_text, \"project_id should be logged\"\n        assert \"crawl-new\" in log_text, \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        log_text = caplog.text\n        assert \"duration_ms\" in log_text, \"Should include timing information\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"negative\" in log_text.lower()\n        assert \"new_page_threshold\" in log_text\n        assert \"-5\" in log_text or \"-1\" in log_text  # The rejected value\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n        original_monotonic = time.monotonic\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n        assert \"slow-proj\" in log_text\n        assert \"slow-page\" in log_text\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n        assert \"slow-project\" in log_text\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n        assert \"err-proj\" in log_text\n        assert \"err-page\" in log_text\n        assert \"ValueError\" in log_text\n        assert \"Mocked hash error\" in log_text\n        # Should include stack trace\n        assert \"stack_trace\" in log_text or \"Traceback\" in log_text\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n        assert \"err-project\" in log_text\n        assert \"err-crawl\" in log_text\n        assert \"RuntimeError\" in log_text\n        assert \"Bad page access\" in log_text\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1207,"cache_read_input_tokens":55712,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1207},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"002077ab-b4cc-4c83-8678-07d814cec355"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TZ53VSnrfBka7r9kGdpNmq","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"b5b894dd-b5df-45cc-9747-e6fef3e7707e","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n","newString":"        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-123\" in log_text, \"project_id should be logged\"\n        assert \"page-456\" in log_text, \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        log_text = caplog.text\n        assert \"Content hash computed\" in log_text or \"compute_hash\" in log_text\n        assert \"duration_ms\" in log_text, \"Should log timing information\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 1, \"Should have debug log entries\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-789\" in log_text, \"project_id should be logged\"\n        assert \"crawl-new\" in log_text, \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        log_text = caplog.text\n        assert \"duration_ms\" in log_text, \"Should include timing information\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"negative\" in log_text.lower()\n        assert \"new_page_threshold\" in log_text\n        assert \"-5\" in log_text or \"-1\" in log_text  # The rejected value\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n        original_monotonic = time.monotonic\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n        assert \"slow-proj\" in log_text\n        assert \"slow-page\" in log_text\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n        assert \"slow-project\" in log_text\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n        assert \"err-proj\" in log_text\n        assert \"err-page\" in log_text\n        assert \"ValueError\" in log_text\n        assert \"Mocked hash error\" in log_text\n        # Should include stack trace\n        assert \"stack_trace\" in log_text or \"Traceback\" in log_text\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n        assert \"err-project\" in log_text\n        assert \"err-crawl\" in log_text\n        assert \"RuntimeError\" in log_text\n        assert \"Bad page access\" in log_text\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text\n","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n","structuredPatch":[{"oldStart":983,"oldLines":3,"newStart":983,"newLines":525,"lines":["         # Empty URL page should be filtered out","         assert result.unchanged_pages == 1","         assert result.removed_pages == 0","+","+","+class TestErrorLogging:","+    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.","+","+    ERROR LOGGING REQUIREMENTS:","+    - Ensure test failures include full assertion context","+    - Log test setup/teardown at DEBUG level","+    - Capture and display logs from failed tests","+    - Include timing information in test reports","+    - Log mock/stub invocations for debugging","+    \"\"\"","+","+    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"","+        import logging","+","+        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):","+            hasher = ContentHasher()","+            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")","+","+        # Check for method entry log","+        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]","+        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"","+","+        # Check for entity IDs in logs","+        log_text = caplog.text","+        assert \"proj-123\" in log_text, \"project_id should be logged\"","+        assert \"page-456\" in log_text, \"page_id should be logged\"","+","+    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test that compute_hash logs completion with timing.\"\"\"","+        import logging","+","+        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):","+            hasher = ContentHasher()","+            hasher.compute_hash(title=\"Test\", body_text=\"Content\")","+","+        # Check for hash computed log","+        log_text = caplog.text","+        assert \"Content hash computed\" in log_text or \"compute_hash\" in log_text","+        assert \"duration_ms\" in log_text, \"Should log timing information\"","+","+    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"","+        import logging","+","+        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):","+            detector = ChangeDetector()","+            detector.compare(","+                crawl_id=\"crawl-new\",","+                previous_crawl_id=\"crawl-old\",","+                previous_pages=[],","+                current_pages=[],","+                project_id=\"proj-789\",","+            )","+","+        # Check for method entry log","+        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]","+        assert len(debug_logs) >= 1, \"Should have debug log entries\"","+","+        # Check for entity IDs in logs","+        log_text = caplog.text","+        assert \"proj-789\" in log_text, \"project_id should be logged\"","+        assert \"crawl-new\" in log_text, \"crawl_id should be logged\"","+","+    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"","+        import logging","+","+        with caplog.at_level(logging.INFO, logger=\"change_detection\"):","+            detector = ChangeDetector()","+            detector.compare(","+                crawl_id=\"crawl-new\",","+                previous_crawl_id=\"crawl-old\",","+                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],","+                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],","+            )","+","+        # Check for INFO level completion log","+        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]","+        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"","+","+        log_text = caplog.text","+        assert \"Change detection completed\" in log_text","+","+    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test that change detection logs include timing information.\"\"\"","+        import logging","+","+        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):","+            detector = ChangeDetector()","+            detector.compare(","+                crawl_id=\"new\",","+                previous_crawl_id=\"old\",","+                previous_pages=[","+                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")","+                    for i in range(100)","+                ],","+                current_pages=[","+                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")","+                    for i in range(100)","+                ],","+            )","+","+        log_text = caplog.text","+        assert \"duration_ms\" in log_text, \"Should include timing information\"","+","+    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"","+        import logging","+","+        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):","+            with pytest.raises(ValueError):","+                ChangeDetector(new_page_threshold=-5)","+","+        log_text = caplog.text","+        assert \"Validation failed\" in log_text or \"negative\" in log_text.lower()","+        assert \"new_page_threshold\" in log_text","+        assert \"-5\" in log_text or \"-1\" in log_text  # The rejected value","+","+    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test that invalid percentage threshold is logged.\"\"\"","+        import logging","+","+        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):","+            with pytest.raises(ValueError):","+                ChangeDetector(change_percentage_threshold=2.0)","+","+        log_text = caplog.text","+        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text","+","+","+class TestSlowOperationLogging:","+    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"","+","+    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:","+        \"\"\"Test that slow hash computation logs a warning.","+","+        Uses monkeypatch to simulate a slow operation by making time.monotonic","+        return values that indicate >1 second elapsed.","+        \"\"\"","+        import logging","+        import time","+","+        # Mock time.monotonic to return values indicating slow operation","+        call_count = 0","+        original_monotonic = time.monotonic","+","+        def mock_monotonic() -> float:","+            nonlocal call_count","+            call_count += 1","+            if call_count == 1:","+                return 0.0  # Start time","+            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)","+","+        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)","+","+        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):","+            hasher = ContentHasher()","+            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")","+","+        log_text = caplog.text","+        assert \"Slow content hash computation\" in log_text","+        assert \"slow-proj\" in log_text","+        assert \"slow-page\" in log_text","+","+    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:","+        \"\"\"Test that slow change detection logs a warning.","+","+        Uses monkeypatch to simulate a slow operation by making time.monotonic","+        return values that indicate >1 second elapsed.","+        \"\"\"","+        import logging","+        import time","+","+        # Mock time.monotonic to return values indicating slow operation","+        call_count = 0","+","+        def mock_monotonic() -> float:","+            nonlocal call_count","+            call_count += 1","+            if call_count == 1:","+                return 0.0  # Start time","+            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)","+","+        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)","+","+        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):","+            detector = ChangeDetector()","+            detector.compare(","+                crawl_id=\"slow-crawl\",","+                previous_crawl_id=\"prev-crawl\",","+                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],","+                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],","+                project_id=\"slow-project\",","+            )","+","+        log_text = caplog.text","+        assert \"Slow change detection\" in log_text","+        assert \"slow-project\" in log_text","+","+","+class TestExceptionHandling:","+    \"\"\"Tests for exception handling paths.\"\"\"","+","+    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:","+        \"\"\"Test that exceptions in compute_hash are logged with full context.","+","+        This tests the error path in ContentHasher.compute_hash() for exception handling.","+        \"\"\"","+        import hashlib","+        import logging","+","+        # Mock hashlib.md5 to raise an exception","+        def mock_md5(*args: object, **kwargs: object) -> None:","+            raise ValueError(\"Mocked hash error\")","+","+        monkeypatch.setattr(hashlib, \"md5\", mock_md5)","+","+        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):","+            hasher = ContentHasher()","+            with pytest.raises(ValueError, match=\"Mocked hash error\"):","+                hasher.compute_hash(","+                    title=\"Test\",","+                    project_id=\"err-proj\",","+                    page_id=\"err-page\",","+                )","+","+        log_text = caplog.text","+        assert \"Content hash computation failed\" in log_text","+        assert \"err-proj\" in log_text","+        assert \"err-page\" in log_text","+        assert \"ValueError\" in log_text","+        assert \"Mocked hash error\" in log_text","+        # Should include stack trace","+        assert \"stack_trace\" in log_text or \"Traceback\" in log_text","+","+    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:","+        \"\"\"Test that exceptions in compare() are logged with full context.","+","+        This tests the error path in ChangeDetector.compare() for exception handling.","+        \"\"\"","+        import logging","+","+        # Create a mock that raises when trying to access url attribute","+        class BadPage:","+            @property","+            def url(self) -> str:","+                raise RuntimeError(\"Bad page access\")","+","+            @property","+            def content_hash(self) -> str:","+                return \"hash\"","+","+        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):","+            detector = ChangeDetector()","+            with pytest.raises(RuntimeError, match=\"Bad page access\"):","+                detector.compare(","+                    crawl_id=\"err-crawl\",","+                    previous_crawl_id=\"prev\",","+                    previous_pages=[BadPage()],  # type: ignore[list-item]","+                    current_pages=[],","+                    project_id=\"err-project\",","+                )","+","+        log_text = caplog.text","+        assert \"Change detection failed\" in log_text","+        assert \"err-project\" in log_text","+        assert \"err-crawl\" in log_text","+        assert \"RuntimeError\" in log_text","+        assert \"Bad page access\" in log_text","+","+","+class TestSingletonReset:","+    \"\"\"Tests for singleton behavior with proper reset.","+","+    Following codebase patterns: reset module-level singleton before and after tests.","+    \"\"\"","+","+    def test_singleton_reset_hasher(self) -> None:","+        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"","+        import app.utils.change_detection as change_detection_module","+","+        # Reset singleton","+        change_detection_module._default_hasher = None","+","+        # Get fresh instance","+        hasher1 = get_content_hasher()","+        assert hasher1 is not None","+","+        # Reset again","+        change_detection_module._default_hasher = None","+","+        # Get another fresh instance","+        hasher2 = get_content_hasher()","+        assert hasher2 is not None","+","+        # These should be different instances because we reset","+        # (But in normal operation they would be the same)","+        assert hasher1 is not hasher2","+","+        # Cleanup","+        change_detection_module._default_hasher = None","+","+    def test_singleton_reset_detector(self) -> None:","+        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"","+        import app.utils.change_detection as change_detection_module","+","+        # Reset singleton","+        change_detection_module._default_detector = None","+","+        # Get fresh instance","+        detector1 = get_change_detector()","+        assert detector1 is not None","+","+        # Reset again","+        change_detection_module._default_detector = None","+","+        # Get another fresh instance","+        detector2 = get_change_detector()","+        assert detector2 is not None","+","+        # These should be different instances because we reset","+        assert detector1 is not detector2","+","+        # Cleanup","+        change_detection_module._default_detector = None","+","+","+class TestChangeSummaryAdditional:","+    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"","+","+    def test_computed_at_default_is_utc(self) -> None:","+        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"","+        from datetime import UTC","+","+        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")","+","+        assert summary.computed_at is not None","+        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC","+","+    def test_to_dict_with_none_computed_at(self) -> None:","+        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"","+        from datetime import datetime, UTC","+","+        # Create with explicit computed_at","+        computed = datetime.now(UTC)","+        summary = ChangeSummary(","+            crawl_id=\"test\",","+            compared_to=\"old\",","+            computed_at=computed,","+        )","+","+        result = summary.to_dict()","+        assert result[\"computed_at\"] == computed.isoformat()","+","+","+class TestPageChangeAdditional:","+    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"","+","+    def test_unchanged_page_change(self) -> None:","+        \"\"\"Test PageChange for unchanged page.\"\"\"","+        change = PageChange(","+            url=\"https://example.com/unchanged\",","+            change_type=ChangeType.UNCHANGED,","+            old_hash=\"same123\",","+            new_hash=\"same123\",","+            title=\"Unchanged Page\",","+            page_id=\"page-unchanged\",","+        )","+","+        assert change.change_type == ChangeType.UNCHANGED","+        assert change.old_hash == change.new_hash","+","+    def test_to_dict_truncates_long_title(self) -> None:","+        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"","+        long_title = \"x\" * 200","+        change = PageChange(","+            url=\"https://example.com/page\",","+            change_type=ChangeType.NEW,","+            new_hash=\"hash123\",","+            title=long_title,","+        )","+","+        result = change.to_dict()","+        assert len(result[\"title\"]) == 100","+","+    def test_to_dict_none_title(self) -> None:","+        \"\"\"Test to_dict with None title.\"\"\"","+        change = PageChange(","+            url=\"https://example.com/page\",","+            change_type=ChangeType.NEW,","+            new_hash=\"hash123\",","+            title=None,","+        )","+","+        result = change.to_dict()","+        assert result[\"title\"] is None","+","+","+class TestPageSnapshotAdditional:","+    \"\"\"Additional tests for PageSnapshot.\"\"\"","+","+    def test_to_dict_none_title(self) -> None:","+        \"\"\"Test to_dict with None title.\"\"\"","+        snapshot = PageSnapshot(","+            url=\"https://example.com/page\",","+            content_hash=\"hash123\",","+            title=None,","+        )","+","+        result = snapshot.to_dict()","+        assert result[\"title\"] is None","+","+    def test_none_content_hash(self) -> None:","+        \"\"\"Test PageSnapshot with None content_hash.\"\"\"","+        snapshot = PageSnapshot(","+            url=\"https://example.com/page\",","+            content_hash=None,","+        )","+","+        result = snapshot.to_dict()","+        assert result[\"content_hash\"] is None","+","+","+class TestHashFromDictEdgeCases:","+    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"","+","+    def test_empty_headings_list(self) -> None:","+        \"\"\"Test hash from dict with empty headings list.\"\"\"","+        hasher = ContentHasher()","+","+        data = {","+            \"title\": \"Test Title\",","+            \"headings\": [],  # Empty list","+            \"body_text\": \"Test body\",","+        }","+","+        result = hasher.compute_hash_from_dict(data)","+","+        assert result is not None","+        assert len(result) == 32","+","+    def test_headings_list_with_non_string(self) -> None:","+        \"\"\"Test hash from dict with non-string heading.\"\"\"","+        hasher = ContentHasher()","+","+        data = {","+            \"title\": \"Test Title\",","+            \"headings\": [123, \"Second Heading\"],  # First is int","+            \"body_text\": \"Test body\",","+        }","+","+        result = hasher.compute_hash_from_dict(data)","+","+        assert result is not None","+        assert len(result) == 32","+","+    def test_fallback_to_markdown_for_body(self) -> None:","+        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"","+        hasher = ContentHasher()","+","+        data = {","+            \"title\": \"Test Title\",","+            \"markdown\": \"# Markdown content\",","+        }","+","+        result = hasher.compute_hash_from_dict(data)","+","+        # Should match hash with body_text as markdown content","+        expected = hasher.compute_hash(","+            title=\"Test Title\",","+            body_text=\"# Markdown content\",","+        )","+","+        assert result == expected","+","+","+class TestCompareFromDictsEdgeCases:","+    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"","+","+    def test_missing_url_key_filtered(self) -> None:","+        \"\"\"Test that pages without url_key are filtered.\"\"\"","+        detector = ChangeDetector()","+","+        previous = [","+            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},","+            {\"content_hash\": \"hash2\"},  # Missing url key","+        ]","+","+        current = [","+            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},","+        ]","+","+        result = detector.compare_from_dicts(","+            crawl_id=\"new\",","+            previous_crawl_id=\"old\",","+            previous_pages=previous,","+            current_pages=current,","+        )","+","+        # Page without url should be filtered out","+        assert result.unchanged_pages == 1","+        assert result.removed_pages == 0","+","+    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:","+        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"","+        import logging","+","+        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):","+            detector = ChangeDetector()","+            detector.compare_from_dicts(","+                crawl_id=\"new\",","+                previous_crawl_id=\"old\",","+                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],","+                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],","+                project_id=\"dict-project\",","+            )","+","+        log_text = caplog.text","+        assert \"dict-project\" in log_text"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_017q5HhXw9oxWWwitTNrrRMd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016JZcvV7XBUSTAt3Wg8y49o","name":"TodoWrite","input":{"todos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"in_progress","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"pending","activeForm":"Running quality checks"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5255,"cache_read_input_tokens":56919,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5255},"output_tokens":26,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"e92ed5f2-7e9d-4ee1-866c-46e5738817a8"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016JZcvV7XBUSTAt3Wg8y49o","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"ebdee45c-ce04-4527-8253-4e9ac9deaa89","tool_use_result":{"oldTodos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"in_progress","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"pending","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"pending","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run quality checks (typecheck, lint)","status":"pending","activeForm":"Running quality checks"}],"newTodos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"in_progress","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"pending","activeForm":"Running quality checks"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01NvoQLqwbmdiFf6Hvjpnaiw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01QVwatqHmGYoiynkMa1sPKp","name":"Bash","input":{"command":"python3 -m pytest tests/utils/test_change_detection.py -v --tb=short 2>&1","description":"Run all change detection tests"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":285,"cache_read_input_tokens":62174,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":285},"output_tokens":26,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"71b974c9-0865-4807-9cdc-4724606f3556"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 1\n============================= test session starts ==============================\nplatform darwin -- Python 3.12.8, pytest-9.0.2, pluggy-1.6.0 -- /Library/Frameworks/Python.framework/Versions/3.12/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/mike/Projects/client-onboarding-v2/backend\nconfigfile: pyproject.toml\nplugins: asyncio-1.3.0, anyio-3.7.1, cov-7.0.0\nasyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 87 items\n\ntests/utils/test_change_detection.py::TestContentHasher::test_init_default PASSED [  1%]\ntests/utils/test_change_detection.py::TestContentHasher::test_init_custom_length PASSED [  2%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_all_fields PASSED [  3%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_empty_fields PASSED [  4%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_none_fields PASSED [  5%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_partial_fields PASSED [  6%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_consistency PASSED [  8%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_different_inputs PASSED [  9%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_whitespace_normalization PASSED [ 10%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_case_normalization PASSED [ 11%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_body_truncation PASSED [ 12%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_with_ids PASSED [ 13%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict PASSED [ 14%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_alternative_keys PASSED [ 16%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_with_headings_list PASSED [ 17%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_preserves_whitespace_when_enabled PASSED [ 18%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_basic_creation PASSED [ 19%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_full_creation PASSED [ 20%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict PASSED [ 21%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict_truncates_long_title PASSED [ 22%]\ntests/utils/test_change_detection.py::TestPageChange::test_new_page_change PASSED [ 24%]\ntests/utils/test_change_detection.py::TestPageChange::test_removed_page_change PASSED [ 25%]\ntests/utils/test_change_detection.py::TestPageChange::test_changed_page_change PASSED [ 26%]\ntests/utils/test_change_detection.py::TestPageChange::test_to_dict PASSED [ 27%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_basic_creation PASSED [ 28%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_pages PASSED [ 29%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_changes PASSED [ 31%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage PASSED [ 32%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage_zero_pages PASSED [ 33%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict PASSED [ 34%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict_truncates_urls PASSED [ 35%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_default PASSED [ 36%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_custom_thresholds PASSED [ 37%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_new_page_threshold PASSED [ 39%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_change_percentage_threshold PASSED [ 40%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_empty_crawls PASSED [ 41%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_new_pages PASSED [ 42%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_removed_pages PASSED [ 43%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_unchanged_pages PASSED [ 44%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 47%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 49%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_signific\n\n... [12934 characters truncated] ...\n\npare_from_dicts() called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\n------------------------------ Captured log call -------------------------------\nDEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\nDEBUG    change_detection:change_detection.py:751 compare_from_dicts() called\nDEBUG    change_detection:change_detection.py:524 compare() called\nINFO     change_detection:change_detection.py:639 Change detection completed\n=========================== short test summary info ============================\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_method_entry_debug - AssertionError: project_id should be logged\nassert 'proj-123' in 'DEBUG    change_detection:change_detection.py:223 ContentHasher.__init__ called\\nDEBUG    change_detection:change_detection.py:286 compute_hash() called\\nDEBUG    change_detection:change_detection.py:318 Content hash computed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_hash_computed_debug - AssertionError: Should log timing information\nassert 'duration_ms' in 'DEBUG    change_detection:change_detection.py:223 ContentHasher.__init__ called\\nDEBUG    change_detection:change_detection.py:286 compute_hash() called\\nDEBUG    change_detection:change_detection.py:318 Content hash computed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_method_entry_debug - AssertionError: project_id should be logged\nassert 'proj-789' in 'DEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_with_timing - AssertionError: Should include timing information\nassert 'duration_ms' in 'DEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_validation_failure - AssertionError: assert ('-5' in 'WARNING  change_detection:change_detection.py:471 Validation failed: negative new_page_threshold\\n' or '-1' in 'WARNING  change_detection:change_detection.py:471 Validation failed: negative new_page_threshold\\n')\nFAILED tests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_hash_computation_logs_warning - AssertionError: assert 'slow-proj' in 'WARNING  change_detection:change_detection.py:330 Slow content hash computation\\n'\nFAILED tests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_change_detection_logs_warning - AssertionError: assert 'slow-project' in 'WARNING  change_detection:change_detection.py:656 Slow change detection\\n'\nFAILED tests/utils/test_change_detection.py::TestExceptionHandling::test_content_hasher_exception_logs_error - assert 'err-proj' in 'ERROR    change_detection:change_detection.py:343 Content hash computation failed\\nTraceback (most recent call last):\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py\", line 315, in compute_hash\\n    content_hash = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py\", line 1202, in mock_md5\\n    raise ValueError(\"Mocked hash error\")\\nValueError: Mocked hash error\\n'\nFAILED tests/utils/test_change_detection.py::TestExceptionHandling::test_change_detector_exception_logs_error - assert 'err-project' in 'ERROR    change_detection:change_detection.py:669 Change detection failed\\nTraceback (most recent call last):\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py\", line 537, in compare\\n    prev_by_url: dict[str, PageSnapshot] = {p.url: p for p in previous_pages}\\n                                            ^^^^^\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py\", line 1235, in url\\n    raise RuntimeError(\"Bad page access\")\\nRuntimeError: Bad page access\\n'\nFAILED tests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging - AssertionError: assert 'dict-project' in 'DEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\\nDEBUG    change_detection:change_detection.py:751 compare_from_dicts() called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\n======================== 10 failed, 77 passed in 0.22s =========================","is_error":true,"tool_use_id":"toolu_01QVwatqHmGYoiynkMa1sPKp"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"952960a0-9929-4f94-b09f-8654d45e5b3a","tool_use_result":"Error: Exit code 1\n============================= test session starts ==============================\nplatform darwin -- Python 3.12.8, pytest-9.0.2, pluggy-1.6.0 -- /Library/Frameworks/Python.framework/Versions/3.12/bin/python3\ncachedir: .pytest_cache\nrootdir: /Users/mike/Projects/client-onboarding-v2/backend\nconfigfile: pyproject.toml\nplugins: asyncio-1.3.0, anyio-3.7.1, cov-7.0.0\nasyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 87 items\n\ntests/utils/test_change_detection.py::TestContentHasher::test_init_default PASSED [  1%]\ntests/utils/test_change_detection.py::TestContentHasher::test_init_custom_length PASSED [  2%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_all_fields PASSED [  3%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_empty_fields PASSED [  4%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_none_fields PASSED [  5%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_partial_fields PASSED [  6%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_consistency PASSED [  8%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_different_inputs PASSED [  9%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_whitespace_normalization PASSED [ 10%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_case_normalization PASSED [ 11%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_body_truncation PASSED [ 12%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_with_ids PASSED [ 13%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict PASSED [ 14%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_alternative_keys PASSED [ 16%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_from_dict_with_headings_list PASSED [ 17%]\ntests/utils/test_change_detection.py::TestContentHasher::test_compute_hash_preserves_whitespace_when_enabled PASSED [ 18%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_basic_creation PASSED [ 19%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_full_creation PASSED [ 20%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict PASSED [ 21%]\ntests/utils/test_change_detection.py::TestPageSnapshot::test_to_dict_truncates_long_title PASSED [ 22%]\ntests/utils/test_change_detection.py::TestPageChange::test_new_page_change PASSED [ 24%]\ntests/utils/test_change_detection.py::TestPageChange::test_removed_page_change PASSED [ 25%]\ntests/utils/test_change_detection.py::TestPageChange::test_changed_page_change PASSED [ 26%]\ntests/utils/test_change_detection.py::TestPageChange::test_to_dict PASSED [ 27%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_basic_creation PASSED [ 28%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_pages PASSED [ 29%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_total_changes PASSED [ 31%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage PASSED [ 32%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_change_percentage_zero_pages PASSED [ 33%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict PASSED [ 34%]\ntests/utils/test_change_detection.py::TestChangeSummary::test_to_dict_truncates_urls PASSED [ 35%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_default PASSED [ 36%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_custom_thresholds PASSED [ 37%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_new_page_threshold PASSED [ 39%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_init_invalid_change_percentage_threshold PASSED [ 40%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_empty_crawls PASSED [ 41%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_new_pages PASSED [ 42%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_all_removed_pages PASSED [ 43%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_unchanged_pages PASSED [ 44%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 47%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 49%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_signific\n\n... [12934 characters truncated] ...\n\npare_from_dicts() called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\n------------------------------ Captured log call -------------------------------\nDEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\nDEBUG    change_detection:change_detection.py:751 compare_from_dicts() called\nDEBUG    change_detection:change_detection.py:524 compare() called\nINFO     change_detection:change_detection.py:639 Change detection completed\n=========================== short test summary info ============================\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_method_entry_debug - AssertionError: project_id should be logged\nassert 'proj-123' in 'DEBUG    change_detection:change_detection.py:223 ContentHasher.__init__ called\\nDEBUG    change_detection:change_detection.py:286 compute_hash() called\\nDEBUG    change_detection:change_detection.py:318 Content hash computed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_hash_computed_debug - AssertionError: Should log timing information\nassert 'duration_ms' in 'DEBUG    change_detection:change_detection.py:223 ContentHasher.__init__ called\\nDEBUG    change_detection:change_detection.py:286 compute_hash() called\\nDEBUG    change_detection:change_detection.py:318 Content hash computed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_method_entry_debug - AssertionError: project_id should be logged\nassert 'proj-789' in 'DEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_with_timing - AssertionError: Should include timing information\nassert 'duration_ms' in 'DEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\nFAILED tests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_validation_failure - AssertionError: assert ('-5' in 'WARNING  change_detection:change_detection.py:471 Validation failed: negative new_page_threshold\\n' or '-1' in 'WARNING  change_detection:change_detection.py:471 Validation failed: negative new_page_threshold\\n')\nFAILED tests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_hash_computation_logs_warning - AssertionError: assert 'slow-proj' in 'WARNING  change_detection:change_detection.py:330 Slow content hash computation\\n'\nFAILED tests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_change_detection_logs_warning - AssertionError: assert 'slow-project' in 'WARNING  change_detection:change_detection.py:656 Slow change detection\\n'\nFAILED tests/utils/test_change_detection.py::TestExceptionHandling::test_content_hasher_exception_logs_error - assert 'err-proj' in 'ERROR    change_detection:change_detection.py:343 Content hash computation failed\\nTraceback (most recent call last):\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py\", line 315, in compute_hash\\n    content_hash = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py\", line 1202, in mock_md5\\n    raise ValueError(\"Mocked hash error\")\\nValueError: Mocked hash error\\n'\nFAILED tests/utils/test_change_detection.py::TestExceptionHandling::test_change_detector_exception_logs_error - assert 'err-project' in 'ERROR    change_detection:change_detection.py:669 Change detection failed\\nTraceback (most recent call last):\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/app/utils/change_detection.py\", line 537, in compare\\n    prev_by_url: dict[str, PageSnapshot] = {p.url: p for p in previous_pages}\\n                                            ^^^^^\\n  File \"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py\", line 1235, in url\\n    raise RuntimeError(\"Bad page access\")\\nRuntimeError: Bad page access\\n'\nFAILED tests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging - AssertionError: assert 'dict-project' in 'DEBUG    change_detection:change_detection.py:462 ChangeDetector.__init__ called\\nDEBUG    change_detection:change_detection.py:751 compare_from_dicts() called\\nDEBUG    change_detection:change_detection.py:524 compare() called\\nINFO     change_detection:change_detection.py:639 Change detection completed\\n'\n======================== 10 failed, 77 passed in 0.22s ========================="}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01GU6j83E6gn8uBToX44j1wq","type":"message","role":"assistant","content":[{"type":"text","text":"The tests are failing because the logger uses `extra={}` dict for structured logging, which doesn't appear in the standard log text output format. The extra fields are passed to the log record but aren't rendered in the simple message. Let me check how logging is configured and fix the tests to check the log records' extra data instead."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3299,"cache_read_input_tokens":62459,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3299},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"bbbec58e-eea1-472e-b206-5047cf2b8600"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01GU6j83E6gn8uBToX44j1wq","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011yUrc3MzCsbW9xscQ8zF1d","name":"Read","input":{"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/app/core/logging.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3299,"cache_read_input_tokens":62459,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3299},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"5e1d6f6a-06ec-4d84-b725-45a0ab5accc5"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011yUrc3MzCsbW9xscQ8zF1d","type":"tool_result","content":"     1→\"\"\"Structured logging configuration for Railway deployment.\n     2→\n     3→All logs go to stdout/stderr for Railway to capture.\n     4→Uses JSON format for structured logging in production.\n     5→\n     6→ERROR LOGGING REQUIREMENTS:\n     7→- Database connection errors with masked connection string\n     8→- Slow queries (>100ms) at WARNING level\n     9→- Transaction failures with rollback context\n    10→- Migration start/end with version info\n    11→- Table/model name in database error logs\n    12→- Connection pool exhaustion at CRITICAL level\n    13→\"\"\"\n    14→\n    15→import logging\n    16→import sys\n    17→from datetime import UTC, datetime\n    18→from typing import Any\n    19→\n    20→from pythonjsonlogger import jsonlogger\n    21→\n    22→from app.core.config import get_settings\n    23→\n    24→\n    25→class CustomJsonFormatter(jsonlogger.JsonFormatter):\n    26→    \"\"\"Custom JSON formatter with additional fields.\"\"\"\n    27→\n    28→    def add_fields(\n    29→        self,\n    30→        log_record: dict[str, Any],\n    31→        record: logging.LogRecord,\n    32→        message_dict: dict[str, Any],\n    33→    ) -> None:\n    34→        super().add_fields(log_record, record, message_dict)\n    35→        log_record[\"timestamp\"] = datetime.now(UTC).isoformat()\n    36→        log_record[\"level\"] = record.levelname\n    37→        log_record[\"logger\"] = record.name\n    38→        if record.exc_info:\n    39→            log_record[\"exception\"] = self.formatException(record.exc_info)\n    40→\n    41→\n    42→def mask_connection_string(conn_str: str) -> str:\n    43→    \"\"\"Mask sensitive parts of database connection string.\"\"\"\n    44→    if not conn_str:\n    45→        return \"\"\n    46→    # Mask password in postgresql://user:password@host:port/db format\n    47→    import re\n    48→\n    49→    pattern = r\"(://[^:]+:)([^@]+)(@)\"\n    50→    return re.sub(pattern, r\"\\1****\\3\", conn_str)\n    51→\n    52→\n    53→def setup_logging() -> None:\n    54→    \"\"\"Configure application logging.\n    55→\n    56→    Outputs to stdout/stderr only for Railway deployment.\n    57→    Uses JSON format in production, text format in development.\n    58→    \"\"\"\n    59→    settings = get_settings()\n    60→\n    61→    # Root logger configuration\n    62→    root_logger = logging.getLogger()\n    63→    root_logger.setLevel(getattr(logging, settings.log_level.upper()))\n    64→\n    65→    # Clear existing handlers\n    66→    root_logger.handlers.clear()\n    67→\n    68→    # Create stdout handler (Railway captures stdout/stderr)\n    69→    handler = logging.StreamHandler(sys.stdout)\n    70→\n    71→    formatter: logging.Formatter\n    72→    if settings.log_format == \"json\":\n    73→        formatter = CustomJsonFormatter(\"%(timestamp)s %(level)s %(name)s %(message)s\")\n    74→    else:\n    75→        formatter = logging.Formatter(\n    76→            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    77→        )\n    78→\n    79→    handler.setFormatter(formatter)\n    80→    root_logger.addHandler(handler)\n    81→\n    82→    # Set log levels for noisy libraries\n    83→    logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n    84→    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n    85→\n    86→\n    87→def get_logger(name: str) -> logging.Logger:\n    88→    \"\"\"Get a logger with the given name.\"\"\"\n    89→    return logging.getLogger(name)\n    90→\n    91→\n    92→# Database-specific logging helpers\n    93→class DatabaseLogger:\n    94→    \"\"\"Logger for database operations with required error logging.\"\"\"\n    95→\n    96→    def __init__(self) -> None:\n    97→        self.logger = get_logger(\"database\")\n    98→\n    99→    def connection_error(self, error: Exception, connection_string: str) -> None:\n   100→        \"\"\"Log database connection error with masked connection string.\"\"\"\n   101→        self.logger.error(\n   102→            \"Database connection failed\",\n   103→            extra={\n   104→                \"connection_string\": mask_connection_string(connection_string),\n   105→                \"error_type\": type(error).__name__,\n   106→                \"error_message\": str(error),\n   107→            },\n   108→        )\n   109→\n   110→    def slow_query(\n   111→        self, query: str, duration_ms: float, table: str | None = None\n   112→    ) -> None:\n   113→        \"\"\"Log slow query at WARNING level.\"\"\"\n   114→        self.logger.warning(\n   115→            \"Slow query detected\",\n   116→            extra={\n   117→                \"duration_ms\": duration_ms,\n   118→                \"query\": query[:500],  # Truncate long queries\n   119→                \"table\": table,\n   120→            },\n   121→        )\n   122→\n   123→    def transaction_failure(\n   124→        self, error: Exception, table: str | None = None, context: str | None = None\n   125→    ) -> None:\n   126→        \"\"\"Log transaction failure with rollback context.\"\"\"\n   127→        self.logger.error(\n   128→            \"Transaction failed, rolling back\",\n   129→            extra={\n   130→                \"error_type\": type(error).__name__,\n   131→                \"error_message\": str(error),\n   132→                \"table\": table,\n   133→                \"rollback_context\": context,\n   134→            },\n   135→        )\n   136→\n   137→    def migration_start(self, version: str, description: str) -> None:\n   138→        \"\"\"Log migration start.\"\"\"\n   139→        self.logger.info(\n   140→            \"Starting database migration\",\n   141→            extra={\n   142→                \"migration_version\": version,\n   143→                \"description\": description,\n   144→            },\n   145→        )\n   146→\n   147→    def migration_end(self, version: str, success: bool) -> None:\n   148→        \"\"\"Log migration completion.\"\"\"\n   149→        level = logging.INFO if success else logging.ERROR\n   150→        self.logger.log(\n   151→            level,\n   152→            \"Database migration completed\",\n   153→            extra={\n   154→                \"migration_version\": version,\n   155→                \"success\": success,\n   156→            },\n   157→        )\n   158→\n   159→    def migration_step(\n   160→        self, step: str, revision: str, success: bool, duration_ms: float | None = None\n   161→    ) -> None:\n   162→        \"\"\"Log individual migration step.\"\"\"\n   163→        level = logging.INFO if success else logging.ERROR\n   164→        self.logger.log(\n   165→            level,\n   166→            f\"Migration step: {step}\",\n   167→            extra={\n   168→                \"step\": step,\n   169→                \"revision\": revision,\n   170→                \"success\": success,\n   171→                \"duration_ms\": duration_ms,\n   172→            },\n   173→        )\n   174→\n   175→    def rollback_triggered(\n   176→        self, reason: str, from_version: str, to_version: str | None = None\n   177→    ) -> None:\n   178→        \"\"\"Log rollback trigger.\"\"\"\n   179→        self.logger.warning(\n   180→            \"Database rollback triggered\",\n   181→            extra={\n   182→                \"reason\": reason,\n   183→                \"from_version\": from_version,\n   184→                \"to_version\": to_version or \"previous\",\n   185→            },\n   186→        )\n   187→\n   188→    def rollback_executed(self, from_version: str, to_version: str, success: bool) -> None:\n   189→        \"\"\"Log rollback execution.\"\"\"\n   190→        level = logging.INFO if success else logging.ERROR\n   191→        self.logger.log(\n   192→            level,\n   193→            \"Database rollback executed\",\n   194→            extra={\n   195→                \"from_version\": from_version,\n   196→                \"to_version\": to_version,\n   197→                \"success\": success,\n   198→            },\n   199→        )\n   200→\n   201→    def pool_exhausted(self, pool_size: int, waiting: int) -> None:\n   202→        \"\"\"Log connection pool exhaustion at CRITICAL level.\"\"\"\n   203→        self.logger.critical(\n   204→            \"Database connection pool exhausted\",\n   205→            extra={\n   206→                \"pool_size\": pool_size,\n   207→                \"waiting_connections\": waiting,\n   208→            },\n   209→        )\n   210→\n   211→\n   212→# Singleton database logger\n   213→db_logger = DatabaseLogger()\n   214→\n   215→\n   216→class RedisLogger:\n   217→    \"\"\"Logger for Redis operations with required error logging.\"\"\"\n   218→\n   219→    def __init__(self) -> None:\n   220→        self.logger = get_logger(\"redis\")\n   221→\n   222→    def connection_error(self, error: Exception, connection_string: str) -> None:\n   223→        \"\"\"Log Redis connection error with masked connection string.\"\"\"\n   224→        self.logger.error(\n   225→            \"Redis connection failed\",\n   226→            extra={\n   227→                \"connection_string\": mask_connection_string(connection_string),\n   228→                \"error_type\": type(error).__name__,\n   229→                \"error_message\": str(error),\n   230→            },\n   231→        )\n   232→\n   233→    def connection_success(self) -> None:\n   234→        \"\"\"Log successful Redis connection.\"\"\"\n   235→        self.logger.info(\"Redis connection established successfully\")\n   236→\n   237→    def operation(\n   238→        self,\n   239→        operation: str,\n   240→        key: str,\n   241→        duration_ms: float,\n   242→        success: bool,\n   243→        retry_attempt: int = 0,\n   244→    ) -> None:\n   245→        \"\"\"Log Redis operation with timing.\"\"\"\n   246→        level = logging.DEBUG if success else logging.WARNING\n   247→        self.logger.log(\n   248→            level,\n   249→            f\"Redis {operation}\",\n   250→            extra={\n   251→                \"operation\": operation,\n   252→                \"key\": key[:100] if key else None,  # Truncate long keys\n   253→                \"duration_ms\": round(duration_ms, 2),\n   254→                \"success\": success,\n   255→                \"retry_attempt\": retry_attempt,\n   256→            },\n   257→        )\n   258→\n   259→    def timeout(self, operation: str, key: str, timeout_seconds: float) -> None:\n   260→        \"\"\"Log Redis timeout.\"\"\"\n   261→        self.logger.warning(\n   262→            \"Redis operation timeout\",\n   263→            extra={\n   264→                \"operation\": operation,\n   265→                \"key\": key[:100] if key else None,\n   266→                \"timeout_seconds\": timeout_seconds,\n   267→            },\n   268→        )\n   269→\n   270→    def circuit_state_change(\n   271→        self, previous_state: str, new_state: str, failure_count: int\n   272→    ) -> None:\n   273→        \"\"\"Log circuit breaker state change.\"\"\"\n   274→        self.logger.warning(\n   275→            \"Redis circuit breaker state changed\",\n   276→            extra={\n   277→                \"previous_state\": previous_state,\n   278→                \"new_state\": new_state,\n   279→                \"failure_count\": failure_count,\n   280→            },\n   281→        )\n   282→\n   283→    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n   284→        \"\"\"Log circuit breaker opening.\"\"\"\n   285→        self.logger.error(\n   286→            \"Redis circuit breaker opened - Redis operations disabled\",\n   287→            extra={\n   288→                \"failure_count\": failure_count,\n   289→                \"recovery_timeout_seconds\": recovery_timeout,\n   290→            },\n   291→        )\n   292→\n   293→    def circuit_recovery_attempt(self) -> None:\n   294→        \"\"\"Log circuit breaker recovery attempt.\"\"\"\n   295→        self.logger.info(\"Redis circuit breaker attempting recovery\")\n   296→\n   297→    def circuit_closed(self) -> None:\n   298→        \"\"\"Log circuit breaker closing (recovery successful).\"\"\"\n   299→        self.logger.info(\"Redis circuit breaker closed - Redis operations restored\")\n   300→\n   301→    def pool_exhausted(self, pool_size: int) -> None:\n   302→        \"\"\"Log Redis connection pool exhaustion at CRITICAL level.\"\"\"\n   303→        self.logger.critical(\n   304→            \"Redis connection pool exhausted\",\n   305→            extra={\n   306→                \"pool_size\": pool_size,\n   307→            },\n   308→        )\n   309→\n   310→    def graceful_fallback(self, operation: str, reason: str) -> None:\n   311→        \"\"\"Log graceful fallback when Redis is unavailable.\"\"\"\n   312→        self.logger.info(\n   313→            \"Redis unavailable, using fallback\",\n   314→            extra={\n   315→                \"operation\": operation,\n   316→                \"reason\": reason,\n   317→            },\n   318→        )\n   319→\n   320→\n   321→# Singleton Redis logger\n   322→redis_logger = RedisLogger()\n   323→\n   324→\n   325→class Crawl4AILogger:\n   326→    \"\"\"Logger for Crawl4AI operations with required error logging.\n   327→\n   328→    Logs all outbound API calls with endpoint, method, timing.\n   329→    Logs request/response bodies at DEBUG level.\n   330→    Handles timeouts, rate limits (429), auth failures (401/403).\n   331→    \"\"\"\n   332→\n   333→    def __init__(self) -> None:\n   334→        self.logger = get_logger(\"crawl4ai\")\n   335→\n   336→    def api_call_start(\n   337→        self,\n   338→        method: str,\n   339→        endpoint: str,\n   340→        url: str | None = None,\n   341→        retry_attempt: int = 0,\n   342→    ) -> None:\n   343→        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n   344→        self.logger.debug(\n   345→            f\"Crawl4AI API call: {method} {endpoint}\",\n   346→            extra={\n   347→                \"method\": method,\n   348→                \"endpoint\": endpoint,\n   349→                \"target_url\": url,\n   350→                \"retry_attempt\": retry_attempt,\n   351→            },\n   352→        )\n   353→\n   354→    def api_call_success(\n   355→        self,\n   356→        method: str,\n   357→        endpoint: str,\n   358→        duration_ms: float,\n   359→        status_code: int,\n   360→        url: str | None = None,\n   361→    ) -> None:\n   362→        \"\"\"Log successful API call at DEBUG level.\"\"\"\n   363→        self.logger.debug(\n   364→            f\"Crawl4AI API call completed: {method} {endpoint}\",\n   365→            extra={\n   366→                \"method\": method,\n   367→                \"endpoint\": endpoint,\n   368→                \"target_url\": url,\n   369→                \"duration_ms\": round(duration_ms, 2),\n   370→                \"status_code\": status_code,\n   371→                \"success\": True,\n   372→            },\n   373→        )\n   374→\n   375→    def api_call_error(\n   376→        self,\n   377→        method: str,\n   378→        endpoint: str,\n   379→        duration_ms: float,\n   380→        status_code: int | None,\n   381→        error: str,\n   382→        error_type: str,\n   383→        url: str | None = None,\n   384→        retry_attempt: int = 0,\n   385→    ) -> None:\n   386→        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n   387→        # 4xx at WARNING, 5xx and others at ERROR\n   388→        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n   389→        self.logger.log(\n   390→            level,\n   391→            f\"Crawl4AI API call failed: {method} {endpoint}\",\n   392→            extra={\n   393→                \"method\": method,\n   394→                \"endpoint\": endpoint,\n   395→                \"target_url\": url,\n   396→                \"duration_ms\": round(duration_ms, 2),\n   397→                \"status_code\": status_code,\n   398→                \"error\": error,\n   399→                \"error_type\": error_type,\n   400→                \"retry_attempt\": retry_attempt,\n   401→                \"success\": False,\n   402→            },\n   403→        )\n   404→\n   405→    def timeout(\n   406→        self, endpoint: str, timeout_seconds: float, url: str | None = None\n   407→    ) -> None:\n   408→        \"\"\"Log request timeout at WARNING level.\"\"\"\n   409→        self.logger.warning(\n   410→            \"Crawl4AI request timeout\",\n   411→            extra={\n   412→                \"endpoint\": endpoint,\n   413→                \"target_url\": url,\n   414→                \"timeout_seconds\": timeout_seconds,\n   415→            },\n   416→        )\n   417→\n   418→    def rate_limit(\n   419→        self,\n   420→        endpoint: str,\n   421→        retry_after: int | None = None,\n   422→        url: str | None = None,\n   423→    ) -> None:\n   424→        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n   425→        self.logger.warning(\n   426→            \"Crawl4AI rate limit hit (429)\",\n   427→            extra={\n   428→                \"endpoint\": endpoint,\n   429→                \"target_url\": url,\n   430→                \"retry_after_seconds\": retry_after,\n   431→            },\n   432→        )\n   433→\n   434→    def auth_failure(\n   435→        self, endpoint: str, status_code: int, url: str | None = None\n   436→    ) -> None:\n   437→        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n   438→        self.logger.warning(\n   439→            f\"Crawl4AI authentication failed ({status_code})\",\n   440→            extra={\n   441→                \"endpoint\": endpoint,\n   442→                \"target_url\": url,\n   443→                \"status_code\": status_code,\n   444→            },\n   445→        )\n   446→\n   447→    def request_body(self, endpoint: str, body: dict[str, Any]) -> None:\n   448→        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n   449→        # Truncate large string values for logging\n   450→        truncated_body = self._truncate_body(body)\n   451→        self.logger.debug(\n   452→            \"Crawl4AI request body\",\n   453→            extra={\n   454→                \"endpoint\": endpoint,\n   455→                \"request_body\": truncated_body,\n   456→            },\n   457→        )\n   458→\n   459→    def response_body(\n   460→        self, endpoint: str, body: dict[str, Any], duration_ms: float\n   461→    ) -> None:\n   462→        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n   463→        truncated_body = self._truncate_body(body)\n   464→        self.logger.debug(\n   465→            \"Crawl4AI response body\",\n   466→            extra={\n   467→                \"endpoint\": endpoint,\n   468→                \"response_body\": truncated_body,\n   469→                \"duration_ms\": round(duration_ms, 2),\n   470→            },\n   471→        )\n   472→\n   473→    def _truncate_body(self, body: dict[str, Any], max_length: int = 500) -> dict[str, Any]:\n   474→        \"\"\"Truncate large string values in body for logging.\"\"\"\n   475→        result: dict[str, Any] = {}\n   476→        for key, value in body.items():\n   477→            if isinstance(value, str) and len(value) > max_length:\n   478→                result[key] = value[:max_length] + f\"... (truncated, {len(value)} chars)\"\n   479→            elif isinstance(value, dict):\n   480→                result[key] = self._truncate_body(value, max_length)\n   481→            elif isinstance(value, list) and len(value) > 10:\n   482→                result[key] = f\"[list with {len(value)} items]\"\n   483→            else:\n   484→                result[key] = value\n   485→        return result\n   486→\n   487→    def circuit_state_change(\n   488→        self, previous_state: str, new_state: str, failure_count: int\n   489→    ) -> None:\n   490→        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n   491→        self.logger.warning(\n   492→            \"Crawl4AI circuit breaker state changed\",\n   493→            extra={\n   494→                \"previous_state\": previous_state,\n   495→                \"new_state\": new_state,\n   496→                \"failure_count\": failure_count,\n   497→            },\n   498→        )\n   499→\n   500→    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n   501→        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n   502→        self.logger.error(\n   503→            \"Crawl4AI circuit breaker opened - API calls disabled\",\n   504→            extra={\n   505→                \"failure_count\": failure_count,\n   506→                \"recovery_timeout_seconds\": recovery_timeout,\n   507→            },\n   508→        )\n   509→\n   510→    def circuit_recovery_attempt(self) -> None:\n   511→        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n   512→        self.logger.info(\"Crawl4AI circuit breaker attempting recovery\")\n   513→\n   514→    def circuit_closed(self) -> None:\n   515→        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n   516→        self.logger.info(\"Crawl4AI circuit breaker closed - API calls restored\")\n   517→\n   518→    def graceful_fallback(self, operation: str, reason: str) -> None:\n   519→        \"\"\"Log graceful fallback when Crawl4AI is unavailable.\"\"\"\n   520→        self.logger.info(\n   521→            \"Crawl4AI unavailable, using fallback\",\n   522→            extra={\n   523→                \"operation\": operation,\n   524→                \"reason\": reason,\n   525→            },\n   526→        )\n   527→\n   528→    def crawl_start(self, url: str, options: dict[str, Any] | None = None) -> None:\n   529→        \"\"\"Log crawl operation start at INFO level.\"\"\"\n   530→        self.logger.info(\n   531→            \"Starting crawl\",\n   532→            extra={\n   533→                \"target_url\": url,\n   534→                \"options\": options or {},\n   535→            },\n   536→        )\n   537→\n   538→    def crawl_complete(\n   539→        self,\n   540→        url: str,\n   541→        duration_ms: float,\n   542→        success: bool,\n   543→        pages_crawled: int = 1,\n   544→    ) -> None:\n   545→        \"\"\"Log crawl operation completion.\"\"\"\n   546→        level = logging.INFO if success else logging.WARNING\n   547→        self.logger.log(\n   548→            level,\n   549→            \"Crawl completed\" if success else \"Crawl failed\",\n   550→            extra={\n   551→                \"target_url\": url,\n   552→                \"duration_ms\": round(duration_ms, 2),\n   553→                \"success\": success,\n   554→                \"pages_crawled\": pages_crawled,\n   555→            },\n   556→        )\n   557→\n   558→\n   559→# Singleton Crawl4AI logger\n   560→crawl4ai_logger = Crawl4AILogger()\n   561→\n   562→\n   563→class ClaudeLogger:\n   564→    \"\"\"Logger for Claude/Anthropic LLM operations with required error logging.\n   565→\n   566→    Logs all outbound API calls with endpoint, method, timing.\n   567→    Logs request/response bodies at DEBUG level (truncate large responses).\n   568→    Handles timeouts, rate limits (429), auth failures (401/403).\n   569→    Includes retry attempt number in logs.\n   570→    Masks API keys in all logs.\n   571→    Logs circuit breaker state changes.\n   572→    \"\"\"\n   573→\n   574→    def __init__(self) -> None:\n   575→        self.logger = get_logger(\"claude\")\n   576→\n   577→    def api_call_start(\n   578→        self,\n   579→        model: str,\n   580→        prompt_length: int,\n   581→        retry_attempt: int = 0,\n   582→        request_id: str | None = None,\n   583→    ) -> None:\n   584→        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n   585→        self.logger.debug(\n   586→            f\"Claude API call: {model}\",\n   587→            extra={\n   588→                \"model\": model,\n   589→                \"prompt_length\": prompt_length,\n   590→                \"retry_attempt\": retry_attempt,\n   591→                \"request_id\": request_id,\n   592→            },\n   593→        )\n   594→\n   595→    def api_call_success(\n   596→        self,\n   597→        model: str,\n   598→        duration_ms: float,\n   599→        input_tokens: int | None = None,\n   600→        output_tokens: int | None = None,\n   601→        request_id: str | None = None,\n   602→    ) -> None:\n   603→        \"\"\"Log successful API call at DEBUG level with token usage.\"\"\"\n   604→        self.logger.debug(\n   605→            f\"Claude API call completed: {model}\",\n   606→            extra={\n   607→                \"model\": model,\n   608→                \"duration_ms\": round(duration_ms, 2),\n   609→                \"input_tokens\": input_tokens,\n   610→                \"output_tokens\": output_tokens,\n   611→                \"total_tokens\": (input_tokens or 0) + (output_tokens or 0),\n   612→                \"request_id\": request_id,\n   613→                \"success\": True,\n   614→            },\n   615→        )\n   616→\n   617→    def api_call_error(\n   618→        self,\n   619→        model: str,\n   620→        duration_ms: float,\n   621→        status_code: int | None,\n   622→        error: str,\n   623→        error_type: str,\n   624→        retry_attempt: int = 0,\n   625→        request_id: str | None = None,\n   626→    ) -> None:\n   627→        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n   628→        # 4xx at WARNING, 5xx and others at ERROR\n   629→        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n   630→        self.logger.log(\n   631→            level,\n   632→            f\"Claude API call failed: {model}\",\n   633→            extra={\n   634→                \"model\": model,\n   635→                \"duration_ms\": round(duration_ms, 2),\n   636→                \"status_code\": status_code,\n   637→                \"error\": error,\n   638→                \"error_type\": error_type,\n   639→                \"retry_attempt\": retry_attempt,\n   640→                \"request_id\": request_id,\n   641→                \"success\": False,\n   642→            },\n   643→        )\n   644→\n   645→    def timeout(self, model: str, timeout_seconds: float) -> None:\n   646→        \"\"\"Log request timeout at WARNING level.\"\"\"\n   647→        self.logger.warning(\n   648→            \"Claude API request timeout\",\n   649→            extra={\n   650→                \"model\": model,\n   651→                \"timeout_seconds\": timeout_seconds,\n   652→            },\n   653→        )\n   654→\n   655→    def rate_limit(\n   656→        self,\n   657→        model: str,\n   658→        retry_after: float | None = None,\n   659→        request_id: str | None = None,\n   660→    ) -> None:\n   661→        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n   662→        self.logger.warning(\n   663→            \"Claude API rate limit hit (429)\",\n   664→            extra={\n   665→                \"model\": model,\n   666→                \"retry_after_seconds\": retry_after,\n   667→                \"request_id\": request_id,\n   668→            },\n   669→        )\n   670→\n   671→    def auth_failure(self, status_code: int) -> None:\n   672→        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n   673→        self.logger.warning(\n   674→            f\"Claude API authentication failed ({status_code})\",\n   675→            extra={\n   676→                \"status_code\": status_code,\n   677→            },\n   678→        )\n   679→\n   680→    def request_body(self, model: str, system_prompt: str, user_prompt: str) -> None:\n   681→        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n   682→        self.logger.debug(\n   683→            \"Claude API request body\",\n   684→            extra={\n   685→                \"model\": model,\n   686→                \"system_prompt\": self._truncate_text(system_prompt, 200),\n   687→                \"user_prompt\": self._truncate_text(user_prompt, 500),\n   688→            },\n   689→        )\n   690→\n   691→    def response_body(\n   692→        self,\n   693→        model: str,\n   694→        response_text: str,\n   695→        duration_ms: float,\n   696→        stop_reason: str | None = None,\n   697→    ) -> None:\n   698→        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n   699→        self.logger.debug(\n   700→            \"Claude API response body\",\n   701→            extra={\n   702→                \"model\": model,\n   703→                \"response_text\": self._truncate_text(response_text, 500),\n   704→                \"duration_ms\": round(duration_ms, 2),\n   705→                \"stop_reason\": stop_reason,\n   706→            },\n   707→        )\n   708→\n   709→    def _truncate_text(self, text: str, max_length: int = 500) -> str:\n   710→        \"\"\"Truncate text for logging.\"\"\"\n   711→        if len(text) <= max_length:\n   712→            return text\n   713→        return text[:max_length] + f\"... (truncated, {len(text)} chars)\"\n   714→\n   715→    def token_usage(\n   716→        self,\n   717→        model: str,\n   718→        input_tokens: int,\n   719→        output_tokens: int,\n   720→        cache_creation_input_tokens: int | None = None,\n   721→        cache_read_input_tokens: int | None = None,\n   722→    ) -> None:\n   723→        \"\"\"Log API quota/credit usage at INFO level.\"\"\"\n   724→        extra: dict[str, Any] = {\n   725→            \"model\": model,\n   726→            \"input_tokens\": input_tokens,\n   727→            \"output_tokens\": output_tokens,\n   728→            \"total_tokens\": input_tokens + output_tokens,\n   729→        }\n   730→        if cache_creation_input_tokens:\n   731→            extra[\"cache_creation_input_tokens\"] = cache_creation_input_tokens\n   732→        if cache_read_input_tokens:\n   733→            extra[\"cache_read_input_tokens\"] = cache_read_input_tokens\n   734→\n   735→        self.logger.info(\"Claude API token usage\", extra=extra)\n   736→\n   737→    def circuit_state_change(\n   738→        self, previous_state: str, new_state: str, failure_count: int\n   739→    ) -> None:\n   740→        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n   741→        self.logger.warning(\n   742→            \"Claude circuit breaker state changed\",\n   743→            extra={\n   744→                \"previous_state\": previous_state,\n   745→                \"new_state\": new_state,\n   746→                \"failure_count\": failure_count,\n   747→            },\n   748→        )\n   749→\n   750→    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n   751→        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n   752→        self.logger.error(\n   753→            \"Claude circuit breaker opened - API calls disabled\",\n   754→            extra={\n   755→                \"failure_count\": failure_count,\n   756→                \"recovery_timeout_seconds\": recovery_timeout,\n   757→            },\n   758→        )\n   759→\n   760→    def circuit_recovery_attempt(self) -> None:\n   761→        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n   762→        self.logger.info(\"Claude circuit breaker attempting recovery\")\n   763→\n   764→    def circuit_closed(self) -> None:\n   765→        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n   766→        self.logger.info(\"Claude circuit breaker closed - API calls restored\")\n   767→\n   768→    def graceful_fallback(self, operation: str, reason: str) -> None:\n   769→        \"\"\"Log graceful fallback when Claude is unavailable.\"\"\"\n   770→        self.logger.info(\n   771→            \"Claude unavailable, using fallback\",\n   772→            extra={\n   773→                \"operation\": operation,\n   774→                \"reason\": reason,\n   775→            },\n   776→        )\n   777→\n   778→    def categorization_start(self, url: str, category_count: int) -> None:\n   779→        \"\"\"Log categorization operation start at INFO level.\"\"\"\n   780→        self.logger.info(\n   781→            \"Starting page categorization\",\n   782→            extra={\n   783→                \"target_url\": url[:200],\n   784→                \"category_count\": category_count,\n   785→            },\n   786→        )\n   787→\n   788→    def categorization_complete(\n   789→        self,\n   790→        url: str,\n   791→        category: str,\n   792→        confidence: float,\n   793→        duration_ms: float,\n   794→        success: bool,\n   795→    ) -> None:\n   796→        \"\"\"Log categorization operation completion.\"\"\"\n   797→        level = logging.INFO if success else logging.WARNING\n   798→        self.logger.log(\n   799→            level,\n   800→            \"Categorization completed\" if success else \"Categorization failed\",\n   801→            extra={\n   802→                \"target_url\": url[:200],\n   803→                \"category\": category,\n   804→                \"confidence\": round(confidence, 3),\n   805→                \"duration_ms\": round(duration_ms, 2),\n   806→                \"success\": success,\n   807→            },\n   808→        )\n   809→\n   810→    def batch_start(\n   811→        self,\n   812→        batch_index: int,\n   813→        batch_size: int,\n   814→        total_batches: int,\n   815→        total_pages: int,\n   816→        project_id: str | None = None,\n   817→    ) -> None:\n   818→        \"\"\"Log batch processing start at DEBUG level.\"\"\"\n   819→        self.logger.debug(\n   820→            f\"Starting LLM batch {batch_index + 1}/{total_batches}\",\n   821→            extra={\n   822→                \"batch_index\": batch_index,\n   823→                \"batch_size\": batch_size,\n   824→                \"total_batches\": total_batches,\n   825→                \"total_pages\": total_pages,\n   826→                \"project_id\": project_id,\n   827→            },\n   828→        )\n   829→\n   830→    def batch_complete(\n   831→        self,\n   832→        batch_index: int,\n   833→        batch_size: int,\n   834→        total_batches: int,\n   835→        success_count: int,\n   836→        failure_count: int,\n   837→        duration_ms: float,\n   838→        total_input_tokens: int,\n   839→        total_output_tokens: int,\n   840→        project_id: str | None = None,\n   841→    ) -> None:\n   842→        \"\"\"Log batch processing completion at INFO level.\"\"\"\n   843→        self.logger.info(\n   844→            f\"LLM batch {batch_index + 1}/{total_batches} complete\",\n   845→            extra={\n   846→                \"batch_index\": batch_index,\n   847→                \"batch_size\": batch_size,\n   848→                \"total_batches\": total_batches,\n   849→                \"success_count\": success_count,\n   850→                \"failure_count\": failure_count,\n   851→                \"duration_ms\": round(duration_ms, 2),\n   852→                \"total_input_tokens\": total_input_tokens,\n   853→                \"total_output_tokens\": total_output_tokens,\n   854→                \"project_id\": project_id,\n   855→            },\n   856→        )\n   857→\n   858→    def batch_error(\n   859→        self,\n   860→        batch_index: int,\n   861→        total_batches: int,\n   862→        error: str,\n   863→        error_type: str,\n   864→        duration_ms: float,\n   865→        project_id: str | None = None,\n   866→    ) -> None:\n   867→        \"\"\"Log batch processing error at ERROR level.\"\"\"\n   868→        self.logger.error(\n   869→            f\"LLM batch {batch_index + 1}/{total_batches} failed\",\n   870→            extra={\n   871→                \"batch_index\": batch_index,\n   872→                \"total_batches\": total_batches,\n   873→                \"error\": error,\n   874→                \"error_type\": error_type,\n   875→                \"duration_ms\": round(duration_ms, 2),\n   876→                \"project_id\": project_id,\n   877→            },\n   878→        )\n   879→\n   880→    def batch_processing_start(\n   881→        self,\n   882→        total_pages: int,\n   883→        batch_size: int,\n   884→        total_batches: int,\n   885→        project_id: str | None = None,\n   886→    ) -> None:\n   887→        \"\"\"Log batch processing session start at INFO level.\"\"\"\n   888→        self.logger.info(\n   889→            f\"Starting batch LLM categorization: {total_pages} pages in {total_batches} batches\",\n   890→            extra={\n   891→                \"total_pages\": total_pages,\n   892→                \"batch_size\": batch_size,\n   893→                \"total_batches\": total_batches,\n   894→                \"project_id\": project_id,\n   895→            },\n   896→        )\n   897→\n   898→    def batch_processing_complete(\n   899→        self,\n   900→        total_pages: int,\n   901→        total_batches: int,\n   902→        success_count: int,\n   903→        failure_count: int,\n   904→        duration_ms: float,\n   905→        total_input_tokens: int,\n   906→        total_output_tokens: int,\n   907→        project_id: str | None = None,\n   908→    ) -> None:\n   909→        \"\"\"Log batch processing session completion at INFO level.\"\"\"\n   910→        self.logger.info(\n   911→            f\"Batch LLM categorization complete: {success_count}/{total_pages} succeeded\",\n   912→            extra={\n   913→                \"total_pages\": total_pages,\n   914→                \"total_batches\": total_batches,\n   915→                \"success_count\": success_count,\n   916→                \"failure_count\": failure_count,\n   917→                \"duration_ms\": round(duration_ms, 2),\n   918→                \"total_input_tokens\": total_input_tokens,\n   919→                \"total_output_tokens\": total_output_tokens,\n   920→                \"average_ms_per_page\": round(duration_ms / total_pages, 2) if total_pages > 0 else 0,\n   921→                \"project_id\": project_id,\n   922→            },\n   923→        )\n   924→\n   925→\n   926→# Singleton Claude logger\n   927→claude_logger = ClaudeLogger()\n   928→\n   929→\n   930→class PerplexityLogger:\n   931→    \"\"\"Logger for Perplexity API operations with required error logging.\n   932→\n   933→    Logs all outbound API calls with endpoint, method, timing.\n   934→    Logs request/response bodies at DEBUG level (truncate large responses).\n   935→    Handles timeouts, rate limits (429), auth failures (401/403).\n   936→    Includes retry attempt number in logs.\n   937→    Masks API keys in all logs.\n   938→    Logs circuit breaker state changes.\n   939→    \"\"\"\n   940→\n   941→    def __init__(self) -> None:\n   942→        self.logger = get_logger(\"perplexity\")\n   943→\n   944→    def api_call_start(\n   945→        self,\n   946→        model: str,\n   947→        prompt_length: int,\n   948→        retry_attempt: int = 0,\n   949→        request_id: str | None = None,\n   950→    ) -> None:\n   951→        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n   952→        self.logger.debug(\n   953→            f\"Perplexity API call: {model}\",\n   954→            extra={\n   955→                \"model\": model,\n   956→                \"prompt_length\": prompt_length,\n   957→                \"retry_attempt\": retry_attempt,\n   958→                \"request_id\": request_id,\n   959→            },\n   960→        )\n   961→\n   962→    def api_call_success(\n   963→        self,\n   964→        model: str,\n   965→        duration_ms: float,\n   966→        input_tokens: int | None = None,\n   967→        output_tokens: int | None = None,\n   968→        request_id: str | None = None,\n   969→    ) -> None:\n   970→        \"\"\"Log successful API call at DEBUG level with token usage.\"\"\"\n   971→        self.logger.debug(\n   972→            f\"Perplexity API call completed: {model}\",\n   973→            extra={\n   974→                \"model\": model,\n   975→                \"duration_ms\": round(duration_ms, 2),\n   976→                \"input_tokens\": input_tokens,\n   977→                \"output_tokens\": output_tokens,\n   978→                \"total_tokens\": (input_tokens or 0) + (output_tokens or 0),\n   979→                \"request_id\": request_id,\n   980→                \"success\": True,\n   981→            },\n   982→        )\n   983→\n   984→    def api_call_error(\n   985→        self,\n   986→        model: str,\n   987→        duration_ms: float,\n   988→        status_code: int | None,\n   989→        error: str,\n   990→        error_type: str,\n   991→        retry_attempt: int = 0,\n   992→        request_id: str | None = None,\n   993→    ) -> None:\n   994→        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n   995→        # 4xx at WARNING, 5xx and others at ERROR\n   996→        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n   997→        self.logger.log(\n   998→            level,\n   999→            f\"Perplexity API call failed: {model}\",\n  1000→            extra={\n  1001→                \"model\": model,\n  1002→                \"duration_ms\": round(duration_ms, 2),\n  1003→                \"status_code\": status_code,\n  1004→                \"error\": error,\n  1005→                \"error_type\": error_type,\n  1006→                \"retry_attempt\": retry_attempt,\n  1007→                \"request_id\": request_id,\n  1008→                \"success\": False,\n  1009→            },\n  1010→        )\n  1011→\n  1012→    def timeout(self, model: str, timeout_seconds: float) -> None:\n  1013→        \"\"\"Log request timeout at WARNING level.\"\"\"\n  1014→        self.logger.warning(\n  1015→            \"Perplexity API request timeout\",\n  1016→            extra={\n  1017→                \"model\": model,\n  1018→                \"timeout_seconds\": timeout_seconds,\n  1019→            },\n  1020→        )\n  1021→\n  1022→    def rate_limit(\n  1023→        self,\n  1024→        model: str,\n  1025→        retry_after: float | None = None,\n  1026→        request_id: str | None = None,\n  1027→    ) -> None:\n  1028→        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n  1029→        self.logger.warning(\n  1030→            \"Perplexity API rate limit hit (429)\",\n  1031→            extra={\n  1032→                \"model\": model,\n  1033→                \"retry_after_seconds\": retry_after,\n  1034→                \"request_id\": request_id,\n  1035→            },\n  1036→        )\n  1037→\n  1038→    def auth_failure(self, status_code: int) -> None:\n  1039→        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n  1040→        self.logger.warning(\n  1041→            f\"Perplexity API authentication failed ({status_code})\",\n  1042→            extra={\n  1043→                \"status_code\": status_code,\n  1044→            },\n  1045→        )\n  1046→\n  1047→    def request_body(self, model: str, system_prompt: str, user_prompt: str) -> None:\n  1048→        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n  1049→        self.logger.debug(\n  1050→            \"Perplexity API request body\",\n  1051→            extra={\n  1052→                \"model\": model,\n  1053→                \"system_prompt\": self._truncate_text(system_prompt, 200),\n  1054→                \"user_prompt\": self._truncate_text(user_prompt, 500),\n  1055→            },\n  1056→        )\n  1057→\n  1058→    def response_body(\n  1059→        self,\n  1060→        model: str,\n  1061→        response_text: str,\n  1062→        duration_ms: float,\n  1063→        citations: list[str] | None = None,\n  1064→    ) -> None:\n  1065→        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n  1066→        self.logger.debug(\n  1067→            \"Perplexity API response body\",\n  1068→            extra={\n  1069→                \"model\": model,\n  1070→                \"response_text\": self._truncate_text(response_text, 500),\n  1071→                \"duration_ms\": round(duration_ms, 2),\n  1072→                \"citation_count\": len(citations) if citations else 0,\n  1073→            },\n  1074→        )\n  1075→\n  1076→    def _truncate_text(self, text: str, max_length: int = 500) -> str:\n  1077→        \"\"\"Truncate text for logging.\"\"\"\n  1078→        if len(text) <= max_length:\n  1079→            return text\n  1080→        return text[:max_length] + f\"... (truncated, {len(text)} chars)\"\n  1081→\n  1082→    def token_usage(\n  1083→        self,\n  1084→        model: str,\n  1085→        input_tokens: int,\n  1086→        output_tokens: int,\n  1087→    ) -> None:\n  1088→        \"\"\"Log API token usage at INFO level.\"\"\"\n  1089→        self.logger.info(\n  1090→            \"Perplexity API token usage\",\n  1091→            extra={\n  1092→                \"model\": model,\n  1093→                \"input_tokens\": input_tokens,\n  1094→                \"output_tokens\": output_tokens,\n  1095→                \"total_tokens\": input_tokens + output_tokens,\n  1096→            },\n  1097→        )\n  1098→\n  1099→    def circuit_state_change(\n  1100→        self, previous_state: str, new_state: str, failure_count: int\n  1101→    ) -> None:\n  1102→        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n  1103→        self.logger.warning(\n  1104→            \"Perplexity circuit breaker state changed\",\n  1105→            extra={\n  1106→                \"previous_state\": previous_state,\n  1107→                \"new_state\": new_state,\n  1108→                \"failure_count\": failure_count,\n  1109→            },\n  1110→        )\n  1111→\n  1112→    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n  1113→        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n  1114→        self.logger.error(\n  1115→            \"Perplexity circuit breaker opened - API calls disabled\",\n  1116→            extra={\n  1117→                \"failure_count\": failure_count,\n  1118→                \"recovery_timeout_seconds\": recovery_timeout,\n  1119→            },\n  1120→        )\n  1121→\n  1122→    def circuit_recovery_attempt(self) -> None:\n  1123→        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n  1124→        self.logger.info(\"Perplexity circuit breaker attempting recovery\")\n  1125→\n  1126→    def circuit_closed(self) -> None:\n  1127→        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n  1128→        self.logger.info(\"Perplexity circuit breaker closed - API calls restored\")\n  1129→\n  1130→    def graceful_fallback(self, operation: str, reason: str) -> None:\n  1131→        \"\"\"Log graceful fallback when Perplexity is unavailable.\"\"\"\n  1132→        self.logger.info(\n  1133→            \"Perplexity unavailable, using fallback\",\n  1134→            extra={\n  1135→                \"operation\": operation,\n  1136→                \"reason\": reason,\n  1137→            },\n  1138→        )\n  1139→\n  1140→    def analysis_start(self, url: str) -> None:\n  1141→        \"\"\"Log website analysis operation start at INFO level.\"\"\"\n  1142→        self.logger.info(\n  1143→            \"Starting website analysis\",\n  1144→            extra={\n  1145→                \"target_url\": url[:200],\n  1146→            },\n  1147→        )\n  1148→\n  1149→    def analysis_complete(\n  1150→        self,\n  1151→        url: str,\n  1152→        duration_ms: float,\n  1153→        success: bool,\n  1154→        citation_count: int = 0,\n  1155→    ) -> None:\n  1156→        \"\"\"Log website analysis operation completion.\"\"\"\n  1157→        level = logging.INFO if success else logging.WARNING\n  1158→        self.logger.log(\n  1159→            level,\n  1160→            \"Website analysis completed\" if success else \"Website analysis failed\",\n  1161→            extra={\n  1162→                \"target_url\": url[:200],\n  1163→                \"duration_ms\": round(duration_ms, 2),\n  1164→                \"success\": success,\n  1165→                \"citation_count\": citation_count,\n  1166→            },\n  1167→        )\n  1168→\n  1169→\n  1170→# Singleton Perplexity logger\n  1171→perplexity_logger = PerplexityLogger()\n  1172→\n  1173→\n  1174→class SchedulerLogger:\n  1175→    \"\"\"Logger for APScheduler operations with required error logging.\n  1176→\n  1177→    Logs scheduler lifecycle events (start, stop, pause, resume).\n  1178→    Logs job execution (start, success, error, missed).\n  1179→    Logs job store operations at DEBUG level.\n  1180→    Handles graceful degradation when scheduler is unavailable.\n  1181→    \"\"\"\n  1182→\n  1183→    def __init__(self) -> None:\n  1184→        self.logger = get_logger(\"scheduler\")\n  1185→\n  1186→    def scheduler_start(self, job_count: int) -> None:\n  1187→        \"\"\"Log scheduler start at INFO level.\"\"\"\n  1188→        self.logger.info(\n  1189→            \"Scheduler started\",\n  1190→            extra={\n  1191→                \"job_count\": job_count,\n  1192→            },\n  1193→        )\n  1194→\n  1195→    def scheduler_stop(self, graceful: bool) -> None:\n  1196→        \"\"\"Log scheduler stop at INFO level.\"\"\"\n  1197→        self.logger.info(\n  1198→            \"Scheduler stopped\",\n  1199→            extra={\n  1200→                \"graceful\": graceful,\n  1201→            },\n  1202→        )\n  1203→\n  1204→    def scheduler_pause(self) -> None:\n  1205→        \"\"\"Log scheduler pause at INFO level.\"\"\"\n  1206→        self.logger.info(\"Scheduler paused\")\n  1207→\n  1208→    def scheduler_resume(self) -> None:\n  1209→        \"\"\"Log scheduler resume at INFO level.\"\"\"\n  1210→        self.logger.info(\"Scheduler resumed\")\n  1211→\n  1212→    def job_added(\n  1213→        self,\n  1214→        job_id: str,\n  1215→        job_name: str | None,\n  1216→        trigger: str,\n  1217→        next_run: str | None = None,\n  1218→    ) -> None:\n  1219→        \"\"\"Log job addition at INFO level.\"\"\"\n  1220→        self.logger.info(\n  1221→            \"Job added to scheduler\",\n  1222→            extra={\n  1223→                \"job_id\": job_id,\n  1224→                \"job_name\": job_name,\n  1225→                \"trigger\": trigger,\n  1226→                \"next_run\": next_run,\n  1227→            },\n  1228→        )\n  1229→\n  1230→    def job_removed(self, job_id: str, job_name: str | None = None) -> None:\n  1231→        \"\"\"Log job removal at INFO level.\"\"\"\n  1232→        self.logger.info(\n  1233→            \"Job removed from scheduler\",\n  1234→            extra={\n  1235→                \"job_id\": job_id,\n  1236→                \"job_name\": job_name,\n  1237→            },\n  1238→        )\n  1239→\n  1240→    def job_modified(\n  1241→        self,\n  1242→        job_id: str,\n  1243→        job_name: str | None,\n  1244→        changes: dict[str, Any] | None = None,\n  1245→    ) -> None:\n  1246→        \"\"\"Log job modification at INFO level.\"\"\"\n  1247→        self.logger.info(\n  1248→            \"Job modified\",\n  1249→            extra={\n  1250→                \"job_id\": job_id,\n  1251→                \"job_name\": job_name,\n  1252→                \"changes\": changes,\n  1253→            },\n  1254→        )\n  1255→\n  1256→    def job_execution_start(\n  1257→        self,\n  1258→        job_id: str,\n  1259→        job_name: str | None,\n  1260→        scheduled_time: str | None = None,\n  1261→    ) -> None:\n  1262→        \"\"\"Log job execution start at DEBUG level.\"\"\"\n  1263→        self.logger.debug(\n  1264→            \"Job execution started\",\n  1265→            extra={\n  1266→                \"job_id\": job_id,\n  1267→                \"job_name\": job_name,\n  1268→                \"scheduled_time\": scheduled_time,\n  1269→            },\n  1270→        )\n  1271→\n  1272→    def job_execution_success(\n  1273→        self,\n  1274→        job_id: str,\n  1275→        job_name: str | None,\n  1276→        duration_ms: float,\n  1277→        result: Any = None,\n  1278→    ) -> None:\n  1279→        \"\"\"Log job execution success at INFO level.\"\"\"\n  1280→        self.logger.info(\n  1281→            \"Job execution completed\",\n  1282→            extra={\n  1283→                \"job_id\": job_id,\n  1284→                \"job_name\": job_name,\n  1285→                \"duration_ms\": round(duration_ms, 2),\n  1286→                \"success\": True,\n  1287→                \"result\": str(result)[:200] if result else None,\n  1288→            },\n  1289→        )\n  1290→\n  1291→    def job_execution_error(\n  1292→        self,\n  1293→        job_id: str,\n  1294→        job_name: str | None,\n  1295→        duration_ms: float,\n  1296→        error: str,\n  1297→        error_type: str,\n  1298→    ) -> None:\n  1299→        \"\"\"Log job execution error at ERROR level.\"\"\"\n  1300→        self.logger.error(\n  1301→            \"Job execution failed\",\n  1302→            extra={\n  1303→                \"job_id\": job_id,\n  1304→                \"job_name\": job_name,\n  1305→                \"duration_ms\": round(duration_ms, 2),\n  1306→                \"success\": False,\n  1307→                \"error\": error,\n  1308→                \"error_type\": error_type,\n  1309→            },\n  1310→        )\n  1311→\n  1312→    def job_missed(\n  1313→        self,\n  1314→        job_id: str,\n  1315→        job_name: str | None,\n  1316→        scheduled_time: str,\n  1317→        misfire_grace_time: int,\n  1318→    ) -> None:\n  1319→        \"\"\"Log missed job execution at WARNING level.\"\"\"\n  1320→        self.logger.warning(\n  1321→            \"Job execution missed\",\n  1322→            extra={\n  1323→                \"job_id\": job_id,\n  1324→                \"job_name\": job_name,\n  1325→                \"scheduled_time\": scheduled_time,\n  1326→                \"misfire_grace_time\": misfire_grace_time,\n  1327→            },\n  1328→        )\n  1329→\n  1330→    def job_max_instances_reached(\n  1331→        self,\n  1332→        job_id: str,\n  1333→        job_name: str | None,\n  1334→        max_instances: int,\n  1335→    ) -> None:\n  1336→        \"\"\"Log when job max instances reached at WARNING level.\"\"\"\n  1337→        self.logger.warning(\n  1338→            \"Job max instances reached, skipping execution\",\n  1339→            extra={\n  1340→                \"job_id\": job_id,\n  1341→                \"job_name\": job_name,\n  1342→                \"max_instances\": max_instances,\n  1343→            },\n  1344→        )\n  1345→\n  1346→    def jobstore_operation(\n  1347→        self,\n  1348→        operation: str,\n  1349→        store_name: str,\n  1350→        job_id: str | None = None,\n  1351→        success: bool = True,\n  1352→        duration_ms: float | None = None,\n  1353→    ) -> None:\n  1354→        \"\"\"Log job store operation at DEBUG level.\"\"\"\n  1355→        level = logging.DEBUG if success else logging.WARNING\n  1356→        self.logger.log(\n  1357→            level,\n  1358→            f\"Job store operation: {operation}\",\n  1359→            extra={\n  1360→                \"operation\": operation,\n  1361→                \"store_name\": store_name,\n  1362→                \"job_id\": job_id,\n  1363→                \"success\": success,\n  1364→                \"duration_ms\": round(duration_ms, 2) if duration_ms else None,\n  1365→            },\n  1366→        )\n  1367→\n  1368→    def jobstore_connection_error(\n  1369→        self,\n  1370→        store_name: str,\n  1371→        error: str,\n  1372→        error_type: str,\n  1373→        connection_string: str | None = None,\n  1374→    ) -> None:\n  1375→        \"\"\"Log job store connection error at ERROR level.\"\"\"\n  1376→        self.logger.error(\n  1377→            \"Job store connection failed\",\n  1378→            extra={\n  1379→                \"store_name\": store_name,\n  1380→                \"error\": error,\n  1381→                \"error_type\": error_type,\n  1382→                \"connection_string\": (\n  1383→                    mask_connection_string(connection_string)\n  1384→                    if connection_string\n  1385→                    else None\n  1386→                ),\n  1387→            },\n  1388→        )\n  1389→\n  1390→    def slow_job_execution(\n  1391→        self,\n  1392→        job_id: str,\n  1393→        job_name: str | None,\n  1394→        duration_ms: float,\n  1395→        threshold_ms: int = 1000,\n  1396→    ) -> None:\n  1397→        \"\"\"Log slow job execution at WARNING level.\"\"\"\n  1398→        self.logger.warning(\n  1399→            \"Slow job execution detected\",\n  1400→            extra={\n  1401→                \"job_id\": job_id,\n  1402→                \"job_name\": job_name,\n  1403→                \"duration_ms\": round(duration_ms, 2),\n  1404→                \"threshold_ms\": threshold_ms,\n  1405→            },\n  1406→        )\n  1407→\n  1408→    def scheduler_not_available(self, operation: str, reason: str) -> None:\n  1409→        \"\"\"Log scheduler unavailable at WARNING level.\"\"\"\n  1410→        self.logger.warning(\n  1411→            \"Scheduler not available\",\n  1412→            extra={\n  1413→                \"operation\": operation,\n  1414→                \"reason\": reason,\n  1415→            },\n  1416→        )\n  1417→\n  1418→\n  1419→# Singleton scheduler logger\n  1420→scheduler_logger = SchedulerLogger()\n  1421→\n  1422→\n  1423→class KeywordsEverywhereLogger:\n  1424→    \"\"\"Logger for Keywords Everywhere API operations with required error logging.\n  1425→\n  1426→    Logs all outbound API calls with endpoint, method, timing.\n  1427→    Logs request/response bodies at DEBUG level (truncate large responses).\n  1428→    Handles timeouts, rate limits (429), auth failures (401/403).\n  1429→    Includes retry attempt number in logs.\n  1430→    Masks API keys in all logs.\n  1431→    Logs circuit breaker state changes.\n  1432→    Logs credit usage for quota tracking.\n  1433→    \"\"\"\n  1434→\n  1435→    def __init__(self) -> None:\n  1436→        self.logger = get_logger(\"keywords_everywhere\")\n  1437→\n  1438→    def api_call_start(\n  1439→        self,\n  1440→        endpoint: str,\n  1441→        keyword_count: int,\n  1442→        retry_attempt: int = 0,\n  1443→        request_id: str | None = None,\n  1444→    ) -> None:\n  1445→        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n  1446→        self.logger.debug(\n  1447→            f\"Keywords Everywhere API call: {endpoint}\",\n  1448→            extra={\n  1449→                \"endpoint\": endpoint,\n  1450→                \"keyword_count\": keyword_count,\n  1451→                \"retry_attempt\": retry_attempt,\n  1452→                \"request_id\": request_id,\n  1453→            },\n  1454→        )\n  1455→\n  1456→    def api_call_success(\n  1457→        self,\n  1458→        endpoint: str,\n  1459→        duration_ms: float,\n  1460→        keyword_count: int,\n  1461→        credits_used: int | None = None,\n  1462→        request_id: str | None = None,\n  1463→    ) -> None:\n  1464→        \"\"\"Log successful API call at DEBUG level.\"\"\"\n  1465→        self.logger.debug(\n  1466→            f\"Keywords Everywhere API call completed: {endpoint}\",\n  1467→            extra={\n  1468→                \"endpoint\": endpoint,\n  1469→                \"duration_ms\": round(duration_ms, 2),\n  1470→                \"keyword_count\": keyword_count,\n  1471→                \"credits_used\": credits_used,\n  1472→                \"request_id\": request_id,\n  1473→                \"success\": True,\n  1474→            },\n  1475→        )\n  1476→\n  1477→    def api_call_error(\n  1478→        self,\n  1479→        endpoint: str,\n  1480→        duration_ms: float,\n  1481→        status_code: int | None,\n  1482→        error: str,\n  1483→        error_type: str,\n  1484→        retry_attempt: int = 0,\n  1485→        request_id: str | None = None,\n  1486→    ) -> None:\n  1487→        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n  1488→        # 4xx at WARNING, 5xx and others at ERROR\n  1489→        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n  1490→        self.logger.log(\n  1491→            level,\n  1492→            f\"Keywords Everywhere API call failed: {endpoint}\",\n  1493→            extra={\n  1494→                \"endpoint\": endpoint,\n  1495→                \"duration_ms\": round(duration_ms, 2),\n  1496→                \"status_code\": status_code,\n  1497→                \"error\": error,\n  1498→                \"error_type\": error_type,\n  1499→                \"retry_attempt\": retry_attempt,\n  1500→                \"request_id\": request_id,\n  1501→                \"success\": False,\n  1502→            },\n  1503→        )\n  1504→\n  1505→    def timeout(self, endpoint: str, timeout_seconds: float) -> None:\n  1506→        \"\"\"Log request timeout at WARNING level.\"\"\"\n  1507→        self.logger.warning(\n  1508→            \"Keywords Everywhere API request timeout\",\n  1509→            extra={\n  1510→                \"endpoint\": endpoint,\n  1511→                \"timeout_seconds\": timeout_seconds,\n  1512→            },\n  1513→        )\n  1514→\n  1515→    def rate_limit(\n  1516→        self,\n  1517→        endpoint: str,\n  1518→        retry_after: float | None = None,\n  1519→        request_id: str | None = None,\n  1520→    ) -> None:\n  1521→        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n  1522→        self.logger.warning(\n  1523→            \"Keywords Everywhere API rate limit hit (429)\",\n  1524→            extra={\n  1525→                \"endpoint\": endpoint,\n  1526→                \"retry_after_seconds\": retry_after,\n  1527→                \"request_id\": request_id,\n  1528→            },\n  1529→        )\n  1530→\n  1531→    def auth_failure(self, status_code: int) -> None:\n  1532→        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n  1533→        self.logger.warning(\n  1534→            f\"Keywords Everywhere API authentication failed ({status_code})\",\n  1535→            extra={\n  1536→                \"status_code\": status_code,\n  1537→            },\n  1538→        )\n  1539→\n  1540→    def request_body(\n  1541→        self,\n  1542→        endpoint: str,\n  1543→        keywords: list[str],\n  1544→        country: str,\n  1545→        data_source: str,\n  1546→    ) -> None:\n  1547→        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n  1548→        # Show first 10 keywords for logging\n  1549→        keywords_preview = keywords[:10]\n  1550→        if len(keywords) > 10:\n  1551→            keywords_preview_str = f\"{keywords_preview}... ({len(keywords)} total)\"\n  1552→        else:\n  1553→            keywords_preview_str = str(keywords_preview)\n  1554→\n  1555→        self.logger.debug(\n  1556→            \"Keywords Everywhere API request body\",\n  1557→            extra={\n  1558→                \"endpoint\": endpoint,\n  1559→                \"keywords\": keywords_preview_str,\n  1560→                \"keyword_count\": len(keywords),\n  1561→                \"country\": country,\n  1562→                \"data_source\": data_source,\n  1563→            },\n  1564→        )\n  1565→\n  1566→    def response_body(\n  1567→        self,\n  1568→        endpoint: str,\n  1569→        keyword_count: int,\n  1570→        duration_ms: float,\n  1571→        credits_used: int | None = None,\n  1572→    ) -> None:\n  1573→        \"\"\"Log response body at DEBUG level.\"\"\"\n  1574→        self.logger.debug(\n  1575→            \"Keywords Everywhere API response body\",\n  1576→            extra={\n  1577→                \"endpoint\": endpoint,\n  1578→                \"keyword_count\": keyword_count,\n  1579→                \"duration_ms\": round(duration_ms, 2),\n  1580→                \"credits_used\": credits_used,\n  1581→            },\n  1582→        )\n  1583→\n  1584→    def credit_usage(\n  1585→        self,\n  1586→        credits_used: int,\n  1587→        credits_remaining: int | None = None,\n  1588→    ) -> None:\n  1589→        \"\"\"Log API credit usage at INFO level.\"\"\"\n  1590→        self.logger.info(\n  1591→            \"Keywords Everywhere API credit usage\",\n  1592→            extra={\n  1593→                \"credits_used\": credits_used,\n  1594→                \"credits_remaining\": credits_remaining,\n  1595→            },\n  1596→        )\n  1597→\n  1598→    def circuit_state_change(\n  1599→        self, previous_state: str, new_state: str, failure_count: int\n  1600→    ) -> None:\n  1601→        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n  1602→        self.logger.warning(\n  1603→            \"Keywords Everywhere circuit breaker state changed\",\n  1604→            extra={\n  1605→                \"previous_state\": previous_state,\n  1606→                \"new_state\": new_state,\n  1607→                \"failure_count\": failure_count,\n  1608→            },\n  1609→        )\n  1610→\n  1611→    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n  1612→        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n  1613→        self.logger.error(\n  1614→            \"Keywords Everywhere circuit breaker opened - API calls disabled\",\n  1615→            extra={\n  1616→                \"failure_count\": failure_count,\n  1617→                \"recovery_timeout_seconds\": recovery_timeout,\n  1618→            },\n  1619→        )\n  1620→\n  1621→    def circuit_recovery_attempt(self) -> None:\n  1622→        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n  1623→        self.logger.info(\"Keywords Everywhere circuit breaker attempting recovery\")\n  1624→\n  1625→    def circuit_closed(self) -> None:\n  1626→        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n  1627→        self.logger.info(\"Keywords Everywhere circuit breaker closed - API calls restored\")\n  1628→\n  1629→    def graceful_fallback(self, operation: str, reason: str) -> None:\n  1630→        \"\"\"Log graceful fallback when Keywords Everywhere is unavailable.\"\"\"\n  1631→        self.logger.info(\n  1632→            \"Keywords Everywhere unavailable, using fallback\",\n  1633→            extra={\n  1634→                \"operation\": operation,\n  1635→                \"reason\": reason,\n  1636→            },\n  1637→        )\n  1638→\n  1639→    def keyword_lookup_start(self, keyword_count: int, country: str) -> None:\n  1640→        \"\"\"Log keyword lookup operation start at INFO level.\"\"\"\n  1641→        self.logger.info(\n  1642→            \"Starting keyword data lookup\",\n  1643→            extra={\n  1644→                \"keyword_count\": keyword_count,\n  1645→                \"country\": country,\n  1646→            },\n  1647→        )\n  1648→\n  1649→    def keyword_lookup_complete(\n  1650→        self,\n  1651→        keyword_count: int,\n  1652→        duration_ms: float,\n  1653→        success: bool,\n  1654→        results_count: int = 0,\n  1655→    ) -> None:\n  1656→        \"\"\"Log keyword lookup operation completion.\"\"\"\n  1657→        level = logging.INFO if success else logging.WARNING\n  1658→        self.logger.log(\n  1659→            level,\n  1660→            \"Keyword lookup completed\" if success else \"Keyword lookup failed\",\n  1661→            extra={\n  1662→                \"keyword_count\": keyword_count,\n  1663→                \"duration_ms\": round(duration_ms, 2),\n  1664→                \"success\": success,\n  1665→                \"results_count\": results_count,\n  1666→            },\n  1667→        )\n  1668→\n  1669→    def batch_start(\n  1670→        self,\n  1671→        batch_index: int,\n  1672→        batch_size: int,\n  1673→        total_batches: int,\n  1674→        total_keywords: int,\n  1675→    ) -> None:\n  1676→        \"\"\"Log batch processing start at DEBUG level.\"\"\"\n  1677→        self.logger.debug(\n  1678→            f\"Starting keyword batch {batch_index + 1}/{total_batches}\",\n  1679→            extra={\n  1680→                \"batch_index\": batch_index,\n  1681→                \"batch_size\": batch_size,\n  1682→                \"total_batches\": total_batches,\n  1683→                \"total_keywords\": total_keywords,\n  1684→            },\n  1685→        )\n  1686→\n  1687→    def batch_complete(\n  1688→        self,\n  1689→        batch_index: int,\n  1690→        batch_size: int,\n  1691→        total_batches: int,\n  1692→        success_count: int,\n  1693→        duration_ms: float,\n  1694→        credits_used: int | None = None,\n  1695→    ) -> None:\n  1696→        \"\"\"Log batch processing completion at INFO level.\"\"\"\n  1697→        self.logger.info(\n  1698→            f\"Keyword batch {batch_index + 1}/{total_batches} complete\",\n  1699→            extra={\n  1700→                \"batch_index\": batch_index,\n  1701→                \"batch_size\": batch_size,\n  1702→                \"total_batches\": total_batches,\n  1703→                \"success_count\": success_count,\n  1704→                \"duration_ms\": round(duration_ms, 2),\n  1705→                \"credits_used\": credits_used,\n  1706→            },\n  1707→        )\n  1708→\n  1709→\n  1710→# Singleton Keywords Everywhere logger\n  1711→keywords_everywhere_logger = KeywordsEverywhereLogger()\n  1712→\n  1713→\n  1714→class DataForSEOLogger:\n  1715→    \"\"\"Logger for DataForSEO API operations with required error logging.\n  1716→\n  1717→    Logs all outbound API calls with endpoint, method, timing.\n  1718→    Logs request/response bodies at DEBUG level (truncate large responses).\n  1719→    Handles timeouts, rate limits (429), auth failures (401/403).\n  1720→    Includes retry attempt number in logs.\n  1721→    Masks API credentials in all logs.\n  1722→    Logs circuit breaker state changes.\n  1723→    Logs credit/cost usage for quota tracking.\n  1724→    \"\"\"\n  1725→\n  1726→    def __init__(self) -> None:\n  1727→        self.logger = get_logger(\"dataforseo\")\n  1728→\n  1729→    def api_call_start(\n  1730→        self,\n  1731→        endpoint: str,\n  1732→        method: str = \"POST\",\n  1733→        retry_attempt: int = 0,\n  1734→        request_id: str | None = None,\n  1735→    ) -> None:\n  1736→        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n  1737→        self.logger.debug(\n  1738→            f\"DataForSEO API call: {method} {endpoint}\",\n  1739→            extra={\n  1740→                \"endpoint\": endpoint,\n  1741→                \"method\": method,\n  1742→                \"retry_attempt\": retry_attempt,\n  1743→                \"request_id\": request_id,\n  1744→            },\n  1745→        )\n  1746→\n  1747→    def api_call_success(\n  1748→        self,\n  1749→        endpoint: str,\n  1750→        duration_ms: float,\n  1751→        method: str = \"POST\",\n  1752→        cost: float | None = None,\n  1753→        tasks_count: int | None = None,\n  1754→        request_id: str | None = None,\n  1755→    ) -> None:\n  1756→        \"\"\"Log successful API call at DEBUG level.\"\"\"\n  1757→        self.logger.debug(\n  1758→            f\"DataForSEO API call completed: {method} {endpoint}\",\n  1759→            extra={\n  1760→                \"endpoint\": endpoint,\n  1761→                \"method\": method,\n  1762→                \"duration_ms\": round(duration_ms, 2),\n  1763→                \"cost\": cost,\n  1764→                \"tasks_count\": tasks_count,\n  1765→                \"request_id\": request_id,\n  1766→                \"success\": True,\n  1767→            },\n  1768→        )\n  1769→\n  1770→    def api_call_error(\n  1771→        self,\n  1772→        endpoint: str,\n  1773→        duration_ms: float,\n  1774→        status_code: int | None,\n  1775→        error: str,\n  1776→        error_type: str,\n  1777→        method: str = \"POST\",\n  1778→        retry_attempt: int = 0,\n  1779→        request_id: str | None = None,\n  1780→    ) -> None:\n  1781→        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n  1782→        # 4xx at WARNING, 5xx and others at ERROR\n  1783→        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n  1784→        self.logger.log(\n  1785→            level,\n  1786→            f\"DataForSEO API call failed: {method} {endpoint}\",\n  1787→            extra={\n  1788→                \"endpoint\": endpoint,\n  1789→                \"method\": method,\n  1790→                \"duration_ms\": round(duration_ms, 2),\n  1791→                \"status_code\": status_code,\n  1792→                \"error\": error,\n  1793→                \"error_type\": error_type,\n  1794→                \"retry_attempt\": retry_attempt,\n  1795→                \"request_id\": request_id,\n  1796→                \"success\": False,\n  1797→            },\n  1798→        )\n  1799→\n  1800→    def timeout(self, endpoint: str, timeout_seconds: float) -> None:\n  1801→        \"\"\"Log request timeout at WARNING level.\"\"\"\n  1802→        self.logger.warning(\n  1803→            \"DataForSEO API request timeout\",\n  1804→            extra={\n  1805→                \"endpoint\": endpoint,\n  1806→                \"timeout_seconds\": timeout_seconds,\n  1807→            },\n  1808→        )\n  1809→\n  1810→    def rate_limit(\n  1811→        self,\n  1812→        endpoint: str,\n  1813→        retry_after: float | None = None,\n  1814→        request_id: str | None = None,\n  1815→    ) -> None:\n  1816→        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n  1817→        self.logger.warning(\n  1818→            \"DataForSEO API rate limit hit (429)\",\n  1819→            extra={\n  1820→                \"endpoint\": endpoint,\n  1821→                \"retry_after_seconds\": retry_after,\n  1822→                \"request_id\": request_id,\n  1823→            },\n  1824→        )\n  1825→\n  1826→    def auth_failure(self, status_code: int) -> None:\n  1827→        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n  1828→        self.logger.warning(\n  1829→            f\"DataForSEO API authentication failed ({status_code})\",\n  1830→            extra={\n  1831→                \"status_code\": status_code,\n  1832→            },\n  1833→        )\n  1834→\n  1835→    def request_body(\n  1836→        self,\n  1837→        endpoint: str,\n  1838→        body: dict[str, Any] | list[dict[str, Any]],\n  1839→    ) -> None:\n  1840→        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n  1841→        truncated_body = self._truncate_body(body)\n  1842→        self.logger.debug(\n  1843→            \"DataForSEO API request body\",\n  1844→            extra={\n  1845→                \"endpoint\": endpoint,\n  1846→                \"request_body\": truncated_body,\n  1847→            },\n  1848→        )\n  1849→\n  1850→    def response_body(\n  1851→        self,\n  1852→        endpoint: str,\n  1853→        body: dict[str, Any],\n  1854→        duration_ms: float,\n  1855→        cost: float | None = None,\n  1856→    ) -> None:\n  1857→        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n  1858→        truncated_body = self._truncate_body(body)\n  1859→        self.logger.debug(\n  1860→            \"DataForSEO API response body\",\n  1861→            extra={\n  1862→                \"endpoint\": endpoint,\n  1863→                \"response_body\": truncated_body,\n  1864→                \"duration_ms\": round(duration_ms, 2),\n  1865→                \"cost\": cost,\n  1866→            },\n  1867→        )\n  1868→\n  1869→    def _truncate_body(\n  1870→        self, body: dict[str, Any] | list[Any], max_length: int = 500\n  1871→    ) -> dict[str, Any] | list[Any] | str:\n  1872→        \"\"\"Truncate large values in body for logging.\"\"\"\n  1873→        if isinstance(body, list):\n  1874→            if len(body) > 10:\n  1875→                return f\"[list with {len(body)} items]\"\n  1876→            return [self._truncate_body(item, max_length) for item in body[:10]]\n  1877→\n  1878→        if isinstance(body, dict):\n  1879→            result: dict[str, Any] = {}\n  1880→            for key, value in body.items():\n  1881→                if isinstance(value, str) and len(value) > max_length:\n  1882→                    result[key] = value[:max_length] + f\"... (truncated, {len(value)} chars)\"\n  1883→                elif isinstance(value, dict):\n  1884→                    result[key] = self._truncate_body(value, max_length)\n  1885→                elif isinstance(value, list) and len(value) > 10:\n  1886→                    result[key] = f\"[list with {len(value)} items]\"\n  1887→                else:\n  1888→                    result[key] = value\n  1889→            return result\n  1890→\n  1891→        return body\n  1892→\n  1893→    def cost_usage(\n  1894→        self,\n  1895→        cost: float,\n  1896→        endpoint: str | None = None,\n  1897→    ) -> None:\n  1898→        \"\"\"Log API cost usage at INFO level.\"\"\"\n  1899→        self.logger.info(\n  1900→            \"DataForSEO API cost usage\",\n  1901→            extra={\n  1902→                \"cost\": cost,\n  1903→                \"endpoint\": endpoint,\n  1904→            },\n  1905→        )\n  1906→\n  1907→    def circuit_state_change(\n  1908→        self, previous_state: str, new_state: str, failure_count: int\n  1909→    ) -> None:\n  1910→        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n  1911→        self.logger.warning(\n  1912→            \"DataForSEO circuit breaker state changed\",\n  1913→            extra={\n  1914→                \"previous_state\": previous_state,\n  1915→                \"new_state\": new_state,\n  1916→                \"failure_count\": failure_count,\n  1917→            },\n  1918→        )\n  1919→\n  1920→    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n  1921→        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n  1922→        self.logger.error(\n  1923→            \"DataForSEO circuit breaker opened - API calls disabled\",\n  1924→            extra={\n  1925→                \"failure_count\": failure_count,\n  1926→                \"recovery_timeout_seconds\": recovery_timeout,\n  1927→            },\n  1928→        )\n  1929→\n  1930→    def circuit_recovery_attempt(self) -> None:\n  1931→        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n  1932→        self.logger.info(\"DataForSEO circuit breaker attempting recovery\")\n  1933→\n  1934→    def circuit_closed(self) -> None:\n  1935→        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n  1936→        self.logger.info(\"DataForSEO circuit breaker closed - API calls restored\")\n  1937→\n  1938→    def graceful_fallback(self, operation: str, reason: str) -> None:\n  1939→        \"\"\"Log graceful fallback when DataForSEO is unavailable.\"\"\"\n  1940→        self.logger.info(\n  1941→            \"DataForSEO unavailable, using fallback\",\n  1942→            extra={\n  1943→                \"operation\": operation,\n  1944→                \"reason\": reason,\n  1945→            },\n  1946→        )\n  1947→\n  1948→    def keyword_search_start(\n  1949→        self,\n  1950→        keywords: list[str],\n  1951→        location_code: int,\n  1952→    ) -> None:\n  1953→        \"\"\"Log keyword search operation start at INFO level.\"\"\"\n  1954→        keywords_preview = keywords[:5]\n  1955→        if len(keywords) > 5:\n  1956→            keywords_str = f\"{keywords_preview}... ({len(keywords)} total)\"\n  1957→        else:\n  1958→            keywords_str = str(keywords_preview)\n  1959→        self.logger.info(\n  1960→            \"Starting DataForSEO keyword search\",\n  1961→            extra={\n  1962→                \"keywords\": keywords_str,\n  1963→                \"keyword_count\": len(keywords),\n  1964→                \"location_code\": location_code,\n  1965→            },\n  1966→        )\n  1967→\n  1968→    def keyword_search_complete(\n  1969→        self,\n  1970→        keyword_count: int,\n  1971→        duration_ms: float,\n  1972→        success: bool,\n  1973→        results_count: int = 0,\n  1974→        cost: float | None = None,\n  1975→    ) -> None:\n  1976→        \"\"\"Log keyword search operation completion.\"\"\"\n  1977→        level = logging.INFO if success else logging.WARNING\n  1978→        self.logger.log(\n  1979→            level,\n  1980→            \"DataForSEO keyword search completed\" if success else \"DataForSEO keyword search failed\",\n  1981→            extra={\n  1982→                \"keyword_count\": keyword_count,\n  1983→                \"duration_ms\": round(duration_ms, 2),\n  1984→                \"success\": success,\n  1985→                \"results_count\": results_count,\n  1986→                \"cost\": cost,\n  1987→            },\n  1988→        )\n  1989→\n  1990→    def serp_search_start(\n  1991→        self,\n  1992→        keyword: str,\n  1993→        location_code: int,\n  1994→        search_engine: str = \"google\",\n  1995→    ) -> None:\n  1996→        \"\"\"Log SERP search operation start at INFO level.\"\"\"\n  1997→        self.logger.info(\n  1998→            \"Starting DataForSEO SERP search\",\n  1999→            extra={\n  2000→                \"keyword\": keyword[:100],\n  2001→                \"location_code\": location_code,\n  2002→                \"search_engine\": search_engine,\n  2003→            },\n  2004→        )\n  2005→\n  2006→    def serp_search_complete(\n  2007→        self,\n  2008→        keyword: str,\n  2009→        duration_ms: float,\n  2010→        success: bool,\n  2011→        results_count: int = 0,\n  2012→        cost: float | None = None,\n  2013→    ) -> None:\n  2014→        \"\"\"Log SERP search operation completion.\"\"\"\n  2015→        level = logging.INFO if success else logging.WARNING\n  2016→        self.logger.log(\n  2017→            level,\n  2018→            \"DataForSEO SERP search completed\" if success else \"DataForSEO SERP search failed\",\n  2019→            extra={\n  2020→                \"keyword\": keyword[:100],\n  2021→                \"duration_ms\": round(duration_ms, 2),\n  2022→                \"success\": success,\n  2023→                \"results_count\": results_count,\n  2024→                \"cost\": cost,\n  2025→            },\n  2026→        )\n  2027→\n  2028→    def batch_start(\n  2029→        self,\n  2030→        batch_index: int,\n  2031→        batch_size: int,\n  2032→        total_batches: int,\n  2033→        total_items: int,\n  2034→        operation: str = \"keyword_search\",\n  2035→    ) -> None:\n  2036→        \"\"\"Log batch processing start at DEBUG level.\"\"\"\n  2037→        self.logger.debug(\n  2038→            f\"Starting DataForSEO batch {batch_index + 1}/{total_batches}\",\n  2039→            extra={\n  2040→                \"batch_index\": batch_index,\n  2041→                \"batch_size\": batch_size,\n  2042→                \"total_batches\": total_batches,\n  2043→                \"total_items\": total_items,\n  2044→                \"operation\": operation,\n  2045→            },\n  2046→        )\n  2047→\n  2048→    def batch_complete(\n  2049→        self,\n  2050→        batch_index: int,\n  2051→        batch_size: int,\n  2052→        total_batches: int,\n  2053→        success_count: int,\n  2054→        duration_ms: float,\n  2055→        cost: float | None = None,\n  2056→        operation: str = \"keyword_search\",\n  2057→    ) -> None:\n  2058→        \"\"\"Log batch processing completion at INFO level.\"\"\"\n  2059→        self.logger.info(\n  2060→            f\"DataForSEO batch {batch_index + 1}/{total_batches} complete\",\n  2061→            extra={\n  2062→                \"batch_index\": batch_index,\n  2063→                \"batch_size\": batch_size,\n  2064→                \"total_batches\": total_batches,\n  2065→                \"success_count\": success_count,\n  2066→                \"duration_ms\": round(duration_ms, 2),\n  2067→                \"cost\": cost,\n  2068→                \"operation\": operation,\n  2069→            },\n  2070→        )\n  2071→\n  2072→\n  2073→# Singleton DataForSEO logger\n  2074→dataforseo_logger = DataForSEOLogger()\n  2075→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"f838e259-2c56-4636-b704-db6efff4c705","tool_use_result":{"type":"text","file":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/app/core/logging.py","content":"\"\"\"Structured logging configuration for Railway deployment.\n\nAll logs go to stdout/stderr for Railway to capture.\nUses JSON format for structured logging in production.\n\nERROR LOGGING REQUIREMENTS:\n- Database connection errors with masked connection string\n- Slow queries (>100ms) at WARNING level\n- Transaction failures with rollback context\n- Migration start/end with version info\n- Table/model name in database error logs\n- Connection pool exhaustion at CRITICAL level\n\"\"\"\n\nimport logging\nimport sys\nfrom datetime import UTC, datetime\nfrom typing import Any\n\nfrom pythonjsonlogger import jsonlogger\n\nfrom app.core.config import get_settings\n\n\nclass CustomJsonFormatter(jsonlogger.JsonFormatter):\n    \"\"\"Custom JSON formatter with additional fields.\"\"\"\n\n    def add_fields(\n        self,\n        log_record: dict[str, Any],\n        record: logging.LogRecord,\n        message_dict: dict[str, Any],\n    ) -> None:\n        super().add_fields(log_record, record, message_dict)\n        log_record[\"timestamp\"] = datetime.now(UTC).isoformat()\n        log_record[\"level\"] = record.levelname\n        log_record[\"logger\"] = record.name\n        if record.exc_info:\n            log_record[\"exception\"] = self.formatException(record.exc_info)\n\n\ndef mask_connection_string(conn_str: str) -> str:\n    \"\"\"Mask sensitive parts of database connection string.\"\"\"\n    if not conn_str:\n        return \"\"\n    # Mask password in postgresql://user:password@host:port/db format\n    import re\n\n    pattern = r\"(://[^:]+:)([^@]+)(@)\"\n    return re.sub(pattern, r\"\\1****\\3\", conn_str)\n\n\ndef setup_logging() -> None:\n    \"\"\"Configure application logging.\n\n    Outputs to stdout/stderr only for Railway deployment.\n    Uses JSON format in production, text format in development.\n    \"\"\"\n    settings = get_settings()\n\n    # Root logger configuration\n    root_logger = logging.getLogger()\n    root_logger.setLevel(getattr(logging, settings.log_level.upper()))\n\n    # Clear existing handlers\n    root_logger.handlers.clear()\n\n    # Create stdout handler (Railway captures stdout/stderr)\n    handler = logging.StreamHandler(sys.stdout)\n\n    formatter: logging.Formatter\n    if settings.log_format == \"json\":\n        formatter = CustomJsonFormatter(\"%(timestamp)s %(level)s %(name)s %(message)s\")\n    else:\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        )\n\n    handler.setFormatter(formatter)\n    root_logger.addHandler(handler)\n\n    # Set log levels for noisy libraries\n    logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"Get a logger with the given name.\"\"\"\n    return logging.getLogger(name)\n\n\n# Database-specific logging helpers\nclass DatabaseLogger:\n    \"\"\"Logger for database operations with required error logging.\"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"database\")\n\n    def connection_error(self, error: Exception, connection_string: str) -> None:\n        \"\"\"Log database connection error with masked connection string.\"\"\"\n        self.logger.error(\n            \"Database connection failed\",\n            extra={\n                \"connection_string\": mask_connection_string(connection_string),\n                \"error_type\": type(error).__name__,\n                \"error_message\": str(error),\n            },\n        )\n\n    def slow_query(\n        self, query: str, duration_ms: float, table: str | None = None\n    ) -> None:\n        \"\"\"Log slow query at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Slow query detected\",\n            extra={\n                \"duration_ms\": duration_ms,\n                \"query\": query[:500],  # Truncate long queries\n                \"table\": table,\n            },\n        )\n\n    def transaction_failure(\n        self, error: Exception, table: str | None = None, context: str | None = None\n    ) -> None:\n        \"\"\"Log transaction failure with rollback context.\"\"\"\n        self.logger.error(\n            \"Transaction failed, rolling back\",\n            extra={\n                \"error_type\": type(error).__name__,\n                \"error_message\": str(error),\n                \"table\": table,\n                \"rollback_context\": context,\n            },\n        )\n\n    def migration_start(self, version: str, description: str) -> None:\n        \"\"\"Log migration start.\"\"\"\n        self.logger.info(\n            \"Starting database migration\",\n            extra={\n                \"migration_version\": version,\n                \"description\": description,\n            },\n        )\n\n    def migration_end(self, version: str, success: bool) -> None:\n        \"\"\"Log migration completion.\"\"\"\n        level = logging.INFO if success else logging.ERROR\n        self.logger.log(\n            level,\n            \"Database migration completed\",\n            extra={\n                \"migration_version\": version,\n                \"success\": success,\n            },\n        )\n\n    def migration_step(\n        self, step: str, revision: str, success: bool, duration_ms: float | None = None\n    ) -> None:\n        \"\"\"Log individual migration step.\"\"\"\n        level = logging.INFO if success else logging.ERROR\n        self.logger.log(\n            level,\n            f\"Migration step: {step}\",\n            extra={\n                \"step\": step,\n                \"revision\": revision,\n                \"success\": success,\n                \"duration_ms\": duration_ms,\n            },\n        )\n\n    def rollback_triggered(\n        self, reason: str, from_version: str, to_version: str | None = None\n    ) -> None:\n        \"\"\"Log rollback trigger.\"\"\"\n        self.logger.warning(\n            \"Database rollback triggered\",\n            extra={\n                \"reason\": reason,\n                \"from_version\": from_version,\n                \"to_version\": to_version or \"previous\",\n            },\n        )\n\n    def rollback_executed(self, from_version: str, to_version: str, success: bool) -> None:\n        \"\"\"Log rollback execution.\"\"\"\n        level = logging.INFO if success else logging.ERROR\n        self.logger.log(\n            level,\n            \"Database rollback executed\",\n            extra={\n                \"from_version\": from_version,\n                \"to_version\": to_version,\n                \"success\": success,\n            },\n        )\n\n    def pool_exhausted(self, pool_size: int, waiting: int) -> None:\n        \"\"\"Log connection pool exhaustion at CRITICAL level.\"\"\"\n        self.logger.critical(\n            \"Database connection pool exhausted\",\n            extra={\n                \"pool_size\": pool_size,\n                \"waiting_connections\": waiting,\n            },\n        )\n\n\n# Singleton database logger\ndb_logger = DatabaseLogger()\n\n\nclass RedisLogger:\n    \"\"\"Logger for Redis operations with required error logging.\"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"redis\")\n\n    def connection_error(self, error: Exception, connection_string: str) -> None:\n        \"\"\"Log Redis connection error with masked connection string.\"\"\"\n        self.logger.error(\n            \"Redis connection failed\",\n            extra={\n                \"connection_string\": mask_connection_string(connection_string),\n                \"error_type\": type(error).__name__,\n                \"error_message\": str(error),\n            },\n        )\n\n    def connection_success(self) -> None:\n        \"\"\"Log successful Redis connection.\"\"\"\n        self.logger.info(\"Redis connection established successfully\")\n\n    def operation(\n        self,\n        operation: str,\n        key: str,\n        duration_ms: float,\n        success: bool,\n        retry_attempt: int = 0,\n    ) -> None:\n        \"\"\"Log Redis operation with timing.\"\"\"\n        level = logging.DEBUG if success else logging.WARNING\n        self.logger.log(\n            level,\n            f\"Redis {operation}\",\n            extra={\n                \"operation\": operation,\n                \"key\": key[:100] if key else None,  # Truncate long keys\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": success,\n                \"retry_attempt\": retry_attempt,\n            },\n        )\n\n    def timeout(self, operation: str, key: str, timeout_seconds: float) -> None:\n        \"\"\"Log Redis timeout.\"\"\"\n        self.logger.warning(\n            \"Redis operation timeout\",\n            extra={\n                \"operation\": operation,\n                \"key\": key[:100] if key else None,\n                \"timeout_seconds\": timeout_seconds,\n            },\n        )\n\n    def circuit_state_change(\n        self, previous_state: str, new_state: str, failure_count: int\n    ) -> None:\n        \"\"\"Log circuit breaker state change.\"\"\"\n        self.logger.warning(\n            \"Redis circuit breaker state changed\",\n            extra={\n                \"previous_state\": previous_state,\n                \"new_state\": new_state,\n                \"failure_count\": failure_count,\n            },\n        )\n\n    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n        \"\"\"Log circuit breaker opening.\"\"\"\n        self.logger.error(\n            \"Redis circuit breaker opened - Redis operations disabled\",\n            extra={\n                \"failure_count\": failure_count,\n                \"recovery_timeout_seconds\": recovery_timeout,\n            },\n        )\n\n    def circuit_recovery_attempt(self) -> None:\n        \"\"\"Log circuit breaker recovery attempt.\"\"\"\n        self.logger.info(\"Redis circuit breaker attempting recovery\")\n\n    def circuit_closed(self) -> None:\n        \"\"\"Log circuit breaker closing (recovery successful).\"\"\"\n        self.logger.info(\"Redis circuit breaker closed - Redis operations restored\")\n\n    def pool_exhausted(self, pool_size: int) -> None:\n        \"\"\"Log Redis connection pool exhaustion at CRITICAL level.\"\"\"\n        self.logger.critical(\n            \"Redis connection pool exhausted\",\n            extra={\n                \"pool_size\": pool_size,\n            },\n        )\n\n    def graceful_fallback(self, operation: str, reason: str) -> None:\n        \"\"\"Log graceful fallback when Redis is unavailable.\"\"\"\n        self.logger.info(\n            \"Redis unavailable, using fallback\",\n            extra={\n                \"operation\": operation,\n                \"reason\": reason,\n            },\n        )\n\n\n# Singleton Redis logger\nredis_logger = RedisLogger()\n\n\nclass Crawl4AILogger:\n    \"\"\"Logger for Crawl4AI operations with required error logging.\n\n    Logs all outbound API calls with endpoint, method, timing.\n    Logs request/response bodies at DEBUG level.\n    Handles timeouts, rate limits (429), auth failures (401/403).\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"crawl4ai\")\n\n    def api_call_start(\n        self,\n        method: str,\n        endpoint: str,\n        url: str | None = None,\n        retry_attempt: int = 0,\n    ) -> None:\n        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Crawl4AI API call: {method} {endpoint}\",\n            extra={\n                \"method\": method,\n                \"endpoint\": endpoint,\n                \"target_url\": url,\n                \"retry_attempt\": retry_attempt,\n            },\n        )\n\n    def api_call_success(\n        self,\n        method: str,\n        endpoint: str,\n        duration_ms: float,\n        status_code: int,\n        url: str | None = None,\n    ) -> None:\n        \"\"\"Log successful API call at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Crawl4AI API call completed: {method} {endpoint}\",\n            extra={\n                \"method\": method,\n                \"endpoint\": endpoint,\n                \"target_url\": url,\n                \"duration_ms\": round(duration_ms, 2),\n                \"status_code\": status_code,\n                \"success\": True,\n            },\n        )\n\n    def api_call_error(\n        self,\n        method: str,\n        endpoint: str,\n        duration_ms: float,\n        status_code: int | None,\n        error: str,\n        error_type: str,\n        url: str | None = None,\n        retry_attempt: int = 0,\n    ) -> None:\n        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n        # 4xx at WARNING, 5xx and others at ERROR\n        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n        self.logger.log(\n            level,\n            f\"Crawl4AI API call failed: {method} {endpoint}\",\n            extra={\n                \"method\": method,\n                \"endpoint\": endpoint,\n                \"target_url\": url,\n                \"duration_ms\": round(duration_ms, 2),\n                \"status_code\": status_code,\n                \"error\": error,\n                \"error_type\": error_type,\n                \"retry_attempt\": retry_attempt,\n                \"success\": False,\n            },\n        )\n\n    def timeout(\n        self, endpoint: str, timeout_seconds: float, url: str | None = None\n    ) -> None:\n        \"\"\"Log request timeout at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Crawl4AI request timeout\",\n            extra={\n                \"endpoint\": endpoint,\n                \"target_url\": url,\n                \"timeout_seconds\": timeout_seconds,\n            },\n        )\n\n    def rate_limit(\n        self,\n        endpoint: str,\n        retry_after: int | None = None,\n        url: str | None = None,\n    ) -> None:\n        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Crawl4AI rate limit hit (429)\",\n            extra={\n                \"endpoint\": endpoint,\n                \"target_url\": url,\n                \"retry_after_seconds\": retry_after,\n            },\n        )\n\n    def auth_failure(\n        self, endpoint: str, status_code: int, url: str | None = None\n    ) -> None:\n        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n        self.logger.warning(\n            f\"Crawl4AI authentication failed ({status_code})\",\n            extra={\n                \"endpoint\": endpoint,\n                \"target_url\": url,\n                \"status_code\": status_code,\n            },\n        )\n\n    def request_body(self, endpoint: str, body: dict[str, Any]) -> None:\n        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n        # Truncate large string values for logging\n        truncated_body = self._truncate_body(body)\n        self.logger.debug(\n            \"Crawl4AI request body\",\n            extra={\n                \"endpoint\": endpoint,\n                \"request_body\": truncated_body,\n            },\n        )\n\n    def response_body(\n        self, endpoint: str, body: dict[str, Any], duration_ms: float\n    ) -> None:\n        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n        truncated_body = self._truncate_body(body)\n        self.logger.debug(\n            \"Crawl4AI response body\",\n            extra={\n                \"endpoint\": endpoint,\n                \"response_body\": truncated_body,\n                \"duration_ms\": round(duration_ms, 2),\n            },\n        )\n\n    def _truncate_body(self, body: dict[str, Any], max_length: int = 500) -> dict[str, Any]:\n        \"\"\"Truncate large string values in body for logging.\"\"\"\n        result: dict[str, Any] = {}\n        for key, value in body.items():\n            if isinstance(value, str) and len(value) > max_length:\n                result[key] = value[:max_length] + f\"... (truncated, {len(value)} chars)\"\n            elif isinstance(value, dict):\n                result[key] = self._truncate_body(value, max_length)\n            elif isinstance(value, list) and len(value) > 10:\n                result[key] = f\"[list with {len(value)} items]\"\n            else:\n                result[key] = value\n        return result\n\n    def circuit_state_change(\n        self, previous_state: str, new_state: str, failure_count: int\n    ) -> None:\n        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Crawl4AI circuit breaker state changed\",\n            extra={\n                \"previous_state\": previous_state,\n                \"new_state\": new_state,\n                \"failure_count\": failure_count,\n            },\n        )\n\n    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n        self.logger.error(\n            \"Crawl4AI circuit breaker opened - API calls disabled\",\n            extra={\n                \"failure_count\": failure_count,\n                \"recovery_timeout_seconds\": recovery_timeout,\n            },\n        )\n\n    def circuit_recovery_attempt(self) -> None:\n        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n        self.logger.info(\"Crawl4AI circuit breaker attempting recovery\")\n\n    def circuit_closed(self) -> None:\n        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n        self.logger.info(\"Crawl4AI circuit breaker closed - API calls restored\")\n\n    def graceful_fallback(self, operation: str, reason: str) -> None:\n        \"\"\"Log graceful fallback when Crawl4AI is unavailable.\"\"\"\n        self.logger.info(\n            \"Crawl4AI unavailable, using fallback\",\n            extra={\n                \"operation\": operation,\n                \"reason\": reason,\n            },\n        )\n\n    def crawl_start(self, url: str, options: dict[str, Any] | None = None) -> None:\n        \"\"\"Log crawl operation start at INFO level.\"\"\"\n        self.logger.info(\n            \"Starting crawl\",\n            extra={\n                \"target_url\": url,\n                \"options\": options or {},\n            },\n        )\n\n    def crawl_complete(\n        self,\n        url: str,\n        duration_ms: float,\n        success: bool,\n        pages_crawled: int = 1,\n    ) -> None:\n        \"\"\"Log crawl operation completion.\"\"\"\n        level = logging.INFO if success else logging.WARNING\n        self.logger.log(\n            level,\n            \"Crawl completed\" if success else \"Crawl failed\",\n            extra={\n                \"target_url\": url,\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": success,\n                \"pages_crawled\": pages_crawled,\n            },\n        )\n\n\n# Singleton Crawl4AI logger\ncrawl4ai_logger = Crawl4AILogger()\n\n\nclass ClaudeLogger:\n    \"\"\"Logger for Claude/Anthropic LLM operations with required error logging.\n\n    Logs all outbound API calls with endpoint, method, timing.\n    Logs request/response bodies at DEBUG level (truncate large responses).\n    Handles timeouts, rate limits (429), auth failures (401/403).\n    Includes retry attempt number in logs.\n    Masks API keys in all logs.\n    Logs circuit breaker state changes.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"claude\")\n\n    def api_call_start(\n        self,\n        model: str,\n        prompt_length: int,\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Claude API call: {model}\",\n            extra={\n                \"model\": model,\n                \"prompt_length\": prompt_length,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n            },\n        )\n\n    def api_call_success(\n        self,\n        model: str,\n        duration_ms: float,\n        input_tokens: int | None = None,\n        output_tokens: int | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log successful API call at DEBUG level with token usage.\"\"\"\n        self.logger.debug(\n            f\"Claude API call completed: {model}\",\n            extra={\n                \"model\": model,\n                \"duration_ms\": round(duration_ms, 2),\n                \"input_tokens\": input_tokens,\n                \"output_tokens\": output_tokens,\n                \"total_tokens\": (input_tokens or 0) + (output_tokens or 0),\n                \"request_id\": request_id,\n                \"success\": True,\n            },\n        )\n\n    def api_call_error(\n        self,\n        model: str,\n        duration_ms: float,\n        status_code: int | None,\n        error: str,\n        error_type: str,\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n        # 4xx at WARNING, 5xx and others at ERROR\n        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n        self.logger.log(\n            level,\n            f\"Claude API call failed: {model}\",\n            extra={\n                \"model\": model,\n                \"duration_ms\": round(duration_ms, 2),\n                \"status_code\": status_code,\n                \"error\": error,\n                \"error_type\": error_type,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n                \"success\": False,\n            },\n        )\n\n    def timeout(self, model: str, timeout_seconds: float) -> None:\n        \"\"\"Log request timeout at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Claude API request timeout\",\n            extra={\n                \"model\": model,\n                \"timeout_seconds\": timeout_seconds,\n            },\n        )\n\n    def rate_limit(\n        self,\n        model: str,\n        retry_after: float | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Claude API rate limit hit (429)\",\n            extra={\n                \"model\": model,\n                \"retry_after_seconds\": retry_after,\n                \"request_id\": request_id,\n            },\n        )\n\n    def auth_failure(self, status_code: int) -> None:\n        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n        self.logger.warning(\n            f\"Claude API authentication failed ({status_code})\",\n            extra={\n                \"status_code\": status_code,\n            },\n        )\n\n    def request_body(self, model: str, system_prompt: str, user_prompt: str) -> None:\n        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n        self.logger.debug(\n            \"Claude API request body\",\n            extra={\n                \"model\": model,\n                \"system_prompt\": self._truncate_text(system_prompt, 200),\n                \"user_prompt\": self._truncate_text(user_prompt, 500),\n            },\n        )\n\n    def response_body(\n        self,\n        model: str,\n        response_text: str,\n        duration_ms: float,\n        stop_reason: str | None = None,\n    ) -> None:\n        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n        self.logger.debug(\n            \"Claude API response body\",\n            extra={\n                \"model\": model,\n                \"response_text\": self._truncate_text(response_text, 500),\n                \"duration_ms\": round(duration_ms, 2),\n                \"stop_reason\": stop_reason,\n            },\n        )\n\n    def _truncate_text(self, text: str, max_length: int = 500) -> str:\n        \"\"\"Truncate text for logging.\"\"\"\n        if len(text) <= max_length:\n            return text\n        return text[:max_length] + f\"... (truncated, {len(text)} chars)\"\n\n    def token_usage(\n        self,\n        model: str,\n        input_tokens: int,\n        output_tokens: int,\n        cache_creation_input_tokens: int | None = None,\n        cache_read_input_tokens: int | None = None,\n    ) -> None:\n        \"\"\"Log API quota/credit usage at INFO level.\"\"\"\n        extra: dict[str, Any] = {\n            \"model\": model,\n            \"input_tokens\": input_tokens,\n            \"output_tokens\": output_tokens,\n            \"total_tokens\": input_tokens + output_tokens,\n        }\n        if cache_creation_input_tokens:\n            extra[\"cache_creation_input_tokens\"] = cache_creation_input_tokens\n        if cache_read_input_tokens:\n            extra[\"cache_read_input_tokens\"] = cache_read_input_tokens\n\n        self.logger.info(\"Claude API token usage\", extra=extra)\n\n    def circuit_state_change(\n        self, previous_state: str, new_state: str, failure_count: int\n    ) -> None:\n        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Claude circuit breaker state changed\",\n            extra={\n                \"previous_state\": previous_state,\n                \"new_state\": new_state,\n                \"failure_count\": failure_count,\n            },\n        )\n\n    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n        self.logger.error(\n            \"Claude circuit breaker opened - API calls disabled\",\n            extra={\n                \"failure_count\": failure_count,\n                \"recovery_timeout_seconds\": recovery_timeout,\n            },\n        )\n\n    def circuit_recovery_attempt(self) -> None:\n        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n        self.logger.info(\"Claude circuit breaker attempting recovery\")\n\n    def circuit_closed(self) -> None:\n        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n        self.logger.info(\"Claude circuit breaker closed - API calls restored\")\n\n    def graceful_fallback(self, operation: str, reason: str) -> None:\n        \"\"\"Log graceful fallback when Claude is unavailable.\"\"\"\n        self.logger.info(\n            \"Claude unavailable, using fallback\",\n            extra={\n                \"operation\": operation,\n                \"reason\": reason,\n            },\n        )\n\n    def categorization_start(self, url: str, category_count: int) -> None:\n        \"\"\"Log categorization operation start at INFO level.\"\"\"\n        self.logger.info(\n            \"Starting page categorization\",\n            extra={\n                \"target_url\": url[:200],\n                \"category_count\": category_count,\n            },\n        )\n\n    def categorization_complete(\n        self,\n        url: str,\n        category: str,\n        confidence: float,\n        duration_ms: float,\n        success: bool,\n    ) -> None:\n        \"\"\"Log categorization operation completion.\"\"\"\n        level = logging.INFO if success else logging.WARNING\n        self.logger.log(\n            level,\n            \"Categorization completed\" if success else \"Categorization failed\",\n            extra={\n                \"target_url\": url[:200],\n                \"category\": category,\n                \"confidence\": round(confidence, 3),\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": success,\n            },\n        )\n\n    def batch_start(\n        self,\n        batch_index: int,\n        batch_size: int,\n        total_batches: int,\n        total_pages: int,\n        project_id: str | None = None,\n    ) -> None:\n        \"\"\"Log batch processing start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Starting LLM batch {batch_index + 1}/{total_batches}\",\n            extra={\n                \"batch_index\": batch_index,\n                \"batch_size\": batch_size,\n                \"total_batches\": total_batches,\n                \"total_pages\": total_pages,\n                \"project_id\": project_id,\n            },\n        )\n\n    def batch_complete(\n        self,\n        batch_index: int,\n        batch_size: int,\n        total_batches: int,\n        success_count: int,\n        failure_count: int,\n        duration_ms: float,\n        total_input_tokens: int,\n        total_output_tokens: int,\n        project_id: str | None = None,\n    ) -> None:\n        \"\"\"Log batch processing completion at INFO level.\"\"\"\n        self.logger.info(\n            f\"LLM batch {batch_index + 1}/{total_batches} complete\",\n            extra={\n                \"batch_index\": batch_index,\n                \"batch_size\": batch_size,\n                \"total_batches\": total_batches,\n                \"success_count\": success_count,\n                \"failure_count\": failure_count,\n                \"duration_ms\": round(duration_ms, 2),\n                \"total_input_tokens\": total_input_tokens,\n                \"total_output_tokens\": total_output_tokens,\n                \"project_id\": project_id,\n            },\n        )\n\n    def batch_error(\n        self,\n        batch_index: int,\n        total_batches: int,\n        error: str,\n        error_type: str,\n        duration_ms: float,\n        project_id: str | None = None,\n    ) -> None:\n        \"\"\"Log batch processing error at ERROR level.\"\"\"\n        self.logger.error(\n            f\"LLM batch {batch_index + 1}/{total_batches} failed\",\n            extra={\n                \"batch_index\": batch_index,\n                \"total_batches\": total_batches,\n                \"error\": error,\n                \"error_type\": error_type,\n                \"duration_ms\": round(duration_ms, 2),\n                \"project_id\": project_id,\n            },\n        )\n\n    def batch_processing_start(\n        self,\n        total_pages: int,\n        batch_size: int,\n        total_batches: int,\n        project_id: str | None = None,\n    ) -> None:\n        \"\"\"Log batch processing session start at INFO level.\"\"\"\n        self.logger.info(\n            f\"Starting batch LLM categorization: {total_pages} pages in {total_batches} batches\",\n            extra={\n                \"total_pages\": total_pages,\n                \"batch_size\": batch_size,\n                \"total_batches\": total_batches,\n                \"project_id\": project_id,\n            },\n        )\n\n    def batch_processing_complete(\n        self,\n        total_pages: int,\n        total_batches: int,\n        success_count: int,\n        failure_count: int,\n        duration_ms: float,\n        total_input_tokens: int,\n        total_output_tokens: int,\n        project_id: str | None = None,\n    ) -> None:\n        \"\"\"Log batch processing session completion at INFO level.\"\"\"\n        self.logger.info(\n            f\"Batch LLM categorization complete: {success_count}/{total_pages} succeeded\",\n            extra={\n                \"total_pages\": total_pages,\n                \"total_batches\": total_batches,\n                \"success_count\": success_count,\n                \"failure_count\": failure_count,\n                \"duration_ms\": round(duration_ms, 2),\n                \"total_input_tokens\": total_input_tokens,\n                \"total_output_tokens\": total_output_tokens,\n                \"average_ms_per_page\": round(duration_ms / total_pages, 2) if total_pages > 0 else 0,\n                \"project_id\": project_id,\n            },\n        )\n\n\n# Singleton Claude logger\nclaude_logger = ClaudeLogger()\n\n\nclass PerplexityLogger:\n    \"\"\"Logger for Perplexity API operations with required error logging.\n\n    Logs all outbound API calls with endpoint, method, timing.\n    Logs request/response bodies at DEBUG level (truncate large responses).\n    Handles timeouts, rate limits (429), auth failures (401/403).\n    Includes retry attempt number in logs.\n    Masks API keys in all logs.\n    Logs circuit breaker state changes.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"perplexity\")\n\n    def api_call_start(\n        self,\n        model: str,\n        prompt_length: int,\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Perplexity API call: {model}\",\n            extra={\n                \"model\": model,\n                \"prompt_length\": prompt_length,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n            },\n        )\n\n    def api_call_success(\n        self,\n        model: str,\n        duration_ms: float,\n        input_tokens: int | None = None,\n        output_tokens: int | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log successful API call at DEBUG level with token usage.\"\"\"\n        self.logger.debug(\n            f\"Perplexity API call completed: {model}\",\n            extra={\n                \"model\": model,\n                \"duration_ms\": round(duration_ms, 2),\n                \"input_tokens\": input_tokens,\n                \"output_tokens\": output_tokens,\n                \"total_tokens\": (input_tokens or 0) + (output_tokens or 0),\n                \"request_id\": request_id,\n                \"success\": True,\n            },\n        )\n\n    def api_call_error(\n        self,\n        model: str,\n        duration_ms: float,\n        status_code: int | None,\n        error: str,\n        error_type: str,\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n        # 4xx at WARNING, 5xx and others at ERROR\n        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n        self.logger.log(\n            level,\n            f\"Perplexity API call failed: {model}\",\n            extra={\n                \"model\": model,\n                \"duration_ms\": round(duration_ms, 2),\n                \"status_code\": status_code,\n                \"error\": error,\n                \"error_type\": error_type,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n                \"success\": False,\n            },\n        )\n\n    def timeout(self, model: str, timeout_seconds: float) -> None:\n        \"\"\"Log request timeout at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Perplexity API request timeout\",\n            extra={\n                \"model\": model,\n                \"timeout_seconds\": timeout_seconds,\n            },\n        )\n\n    def rate_limit(\n        self,\n        model: str,\n        retry_after: float | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Perplexity API rate limit hit (429)\",\n            extra={\n                \"model\": model,\n                \"retry_after_seconds\": retry_after,\n                \"request_id\": request_id,\n            },\n        )\n\n    def auth_failure(self, status_code: int) -> None:\n        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n        self.logger.warning(\n            f\"Perplexity API authentication failed ({status_code})\",\n            extra={\n                \"status_code\": status_code,\n            },\n        )\n\n    def request_body(self, model: str, system_prompt: str, user_prompt: str) -> None:\n        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n        self.logger.debug(\n            \"Perplexity API request body\",\n            extra={\n                \"model\": model,\n                \"system_prompt\": self._truncate_text(system_prompt, 200),\n                \"user_prompt\": self._truncate_text(user_prompt, 500),\n            },\n        )\n\n    def response_body(\n        self,\n        model: str,\n        response_text: str,\n        duration_ms: float,\n        citations: list[str] | None = None,\n    ) -> None:\n        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n        self.logger.debug(\n            \"Perplexity API response body\",\n            extra={\n                \"model\": model,\n                \"response_text\": self._truncate_text(response_text, 500),\n                \"duration_ms\": round(duration_ms, 2),\n                \"citation_count\": len(citations) if citations else 0,\n            },\n        )\n\n    def _truncate_text(self, text: str, max_length: int = 500) -> str:\n        \"\"\"Truncate text for logging.\"\"\"\n        if len(text) <= max_length:\n            return text\n        return text[:max_length] + f\"... (truncated, {len(text)} chars)\"\n\n    def token_usage(\n        self,\n        model: str,\n        input_tokens: int,\n        output_tokens: int,\n    ) -> None:\n        \"\"\"Log API token usage at INFO level.\"\"\"\n        self.logger.info(\n            \"Perplexity API token usage\",\n            extra={\n                \"model\": model,\n                \"input_tokens\": input_tokens,\n                \"output_tokens\": output_tokens,\n                \"total_tokens\": input_tokens + output_tokens,\n            },\n        )\n\n    def circuit_state_change(\n        self, previous_state: str, new_state: str, failure_count: int\n    ) -> None:\n        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Perplexity circuit breaker state changed\",\n            extra={\n                \"previous_state\": previous_state,\n                \"new_state\": new_state,\n                \"failure_count\": failure_count,\n            },\n        )\n\n    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n        self.logger.error(\n            \"Perplexity circuit breaker opened - API calls disabled\",\n            extra={\n                \"failure_count\": failure_count,\n                \"recovery_timeout_seconds\": recovery_timeout,\n            },\n        )\n\n    def circuit_recovery_attempt(self) -> None:\n        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n        self.logger.info(\"Perplexity circuit breaker attempting recovery\")\n\n    def circuit_closed(self) -> None:\n        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n        self.logger.info(\"Perplexity circuit breaker closed - API calls restored\")\n\n    def graceful_fallback(self, operation: str, reason: str) -> None:\n        \"\"\"Log graceful fallback when Perplexity is unavailable.\"\"\"\n        self.logger.info(\n            \"Perplexity unavailable, using fallback\",\n            extra={\n                \"operation\": operation,\n                \"reason\": reason,\n            },\n        )\n\n    def analysis_start(self, url: str) -> None:\n        \"\"\"Log website analysis operation start at INFO level.\"\"\"\n        self.logger.info(\n            \"Starting website analysis\",\n            extra={\n                \"target_url\": url[:200],\n            },\n        )\n\n    def analysis_complete(\n        self,\n        url: str,\n        duration_ms: float,\n        success: bool,\n        citation_count: int = 0,\n    ) -> None:\n        \"\"\"Log website analysis operation completion.\"\"\"\n        level = logging.INFO if success else logging.WARNING\n        self.logger.log(\n            level,\n            \"Website analysis completed\" if success else \"Website analysis failed\",\n            extra={\n                \"target_url\": url[:200],\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": success,\n                \"citation_count\": citation_count,\n            },\n        )\n\n\n# Singleton Perplexity logger\nperplexity_logger = PerplexityLogger()\n\n\nclass SchedulerLogger:\n    \"\"\"Logger for APScheduler operations with required error logging.\n\n    Logs scheduler lifecycle events (start, stop, pause, resume).\n    Logs job execution (start, success, error, missed).\n    Logs job store operations at DEBUG level.\n    Handles graceful degradation when scheduler is unavailable.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"scheduler\")\n\n    def scheduler_start(self, job_count: int) -> None:\n        \"\"\"Log scheduler start at INFO level.\"\"\"\n        self.logger.info(\n            \"Scheduler started\",\n            extra={\n                \"job_count\": job_count,\n            },\n        )\n\n    def scheduler_stop(self, graceful: bool) -> None:\n        \"\"\"Log scheduler stop at INFO level.\"\"\"\n        self.logger.info(\n            \"Scheduler stopped\",\n            extra={\n                \"graceful\": graceful,\n            },\n        )\n\n    def scheduler_pause(self) -> None:\n        \"\"\"Log scheduler pause at INFO level.\"\"\"\n        self.logger.info(\"Scheduler paused\")\n\n    def scheduler_resume(self) -> None:\n        \"\"\"Log scheduler resume at INFO level.\"\"\"\n        self.logger.info(\"Scheduler resumed\")\n\n    def job_added(\n        self,\n        job_id: str,\n        job_name: str | None,\n        trigger: str,\n        next_run: str | None = None,\n    ) -> None:\n        \"\"\"Log job addition at INFO level.\"\"\"\n        self.logger.info(\n            \"Job added to scheduler\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"trigger\": trigger,\n                \"next_run\": next_run,\n            },\n        )\n\n    def job_removed(self, job_id: str, job_name: str | None = None) -> None:\n        \"\"\"Log job removal at INFO level.\"\"\"\n        self.logger.info(\n            \"Job removed from scheduler\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n            },\n        )\n\n    def job_modified(\n        self,\n        job_id: str,\n        job_name: str | None,\n        changes: dict[str, Any] | None = None,\n    ) -> None:\n        \"\"\"Log job modification at INFO level.\"\"\"\n        self.logger.info(\n            \"Job modified\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"changes\": changes,\n            },\n        )\n\n    def job_execution_start(\n        self,\n        job_id: str,\n        job_name: str | None,\n        scheduled_time: str | None = None,\n    ) -> None:\n        \"\"\"Log job execution start at DEBUG level.\"\"\"\n        self.logger.debug(\n            \"Job execution started\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"scheduled_time\": scheduled_time,\n            },\n        )\n\n    def job_execution_success(\n        self,\n        job_id: str,\n        job_name: str | None,\n        duration_ms: float,\n        result: Any = None,\n    ) -> None:\n        \"\"\"Log job execution success at INFO level.\"\"\"\n        self.logger.info(\n            \"Job execution completed\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": True,\n                \"result\": str(result)[:200] if result else None,\n            },\n        )\n\n    def job_execution_error(\n        self,\n        job_id: str,\n        job_name: str | None,\n        duration_ms: float,\n        error: str,\n        error_type: str,\n    ) -> None:\n        \"\"\"Log job execution error at ERROR level.\"\"\"\n        self.logger.error(\n            \"Job execution failed\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": False,\n                \"error\": error,\n                \"error_type\": error_type,\n            },\n        )\n\n    def job_missed(\n        self,\n        job_id: str,\n        job_name: str | None,\n        scheduled_time: str,\n        misfire_grace_time: int,\n    ) -> None:\n        \"\"\"Log missed job execution at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Job execution missed\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"scheduled_time\": scheduled_time,\n                \"misfire_grace_time\": misfire_grace_time,\n            },\n        )\n\n    def job_max_instances_reached(\n        self,\n        job_id: str,\n        job_name: str | None,\n        max_instances: int,\n    ) -> None:\n        \"\"\"Log when job max instances reached at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Job max instances reached, skipping execution\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"max_instances\": max_instances,\n            },\n        )\n\n    def jobstore_operation(\n        self,\n        operation: str,\n        store_name: str,\n        job_id: str | None = None,\n        success: bool = True,\n        duration_ms: float | None = None,\n    ) -> None:\n        \"\"\"Log job store operation at DEBUG level.\"\"\"\n        level = logging.DEBUG if success else logging.WARNING\n        self.logger.log(\n            level,\n            f\"Job store operation: {operation}\",\n            extra={\n                \"operation\": operation,\n                \"store_name\": store_name,\n                \"job_id\": job_id,\n                \"success\": success,\n                \"duration_ms\": round(duration_ms, 2) if duration_ms else None,\n            },\n        )\n\n    def jobstore_connection_error(\n        self,\n        store_name: str,\n        error: str,\n        error_type: str,\n        connection_string: str | None = None,\n    ) -> None:\n        \"\"\"Log job store connection error at ERROR level.\"\"\"\n        self.logger.error(\n            \"Job store connection failed\",\n            extra={\n                \"store_name\": store_name,\n                \"error\": error,\n                \"error_type\": error_type,\n                \"connection_string\": (\n                    mask_connection_string(connection_string)\n                    if connection_string\n                    else None\n                ),\n            },\n        )\n\n    def slow_job_execution(\n        self,\n        job_id: str,\n        job_name: str | None,\n        duration_ms: float,\n        threshold_ms: int = 1000,\n    ) -> None:\n        \"\"\"Log slow job execution at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Slow job execution detected\",\n            extra={\n                \"job_id\": job_id,\n                \"job_name\": job_name,\n                \"duration_ms\": round(duration_ms, 2),\n                \"threshold_ms\": threshold_ms,\n            },\n        )\n\n    def scheduler_not_available(self, operation: str, reason: str) -> None:\n        \"\"\"Log scheduler unavailable at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Scheduler not available\",\n            extra={\n                \"operation\": operation,\n                \"reason\": reason,\n            },\n        )\n\n\n# Singleton scheduler logger\nscheduler_logger = SchedulerLogger()\n\n\nclass KeywordsEverywhereLogger:\n    \"\"\"Logger for Keywords Everywhere API operations with required error logging.\n\n    Logs all outbound API calls with endpoint, method, timing.\n    Logs request/response bodies at DEBUG level (truncate large responses).\n    Handles timeouts, rate limits (429), auth failures (401/403).\n    Includes retry attempt number in logs.\n    Masks API keys in all logs.\n    Logs circuit breaker state changes.\n    Logs credit usage for quota tracking.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"keywords_everywhere\")\n\n    def api_call_start(\n        self,\n        endpoint: str,\n        keyword_count: int,\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Keywords Everywhere API call: {endpoint}\",\n            extra={\n                \"endpoint\": endpoint,\n                \"keyword_count\": keyword_count,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n            },\n        )\n\n    def api_call_success(\n        self,\n        endpoint: str,\n        duration_ms: float,\n        keyword_count: int,\n        credits_used: int | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log successful API call at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Keywords Everywhere API call completed: {endpoint}\",\n            extra={\n                \"endpoint\": endpoint,\n                \"duration_ms\": round(duration_ms, 2),\n                \"keyword_count\": keyword_count,\n                \"credits_used\": credits_used,\n                \"request_id\": request_id,\n                \"success\": True,\n            },\n        )\n\n    def api_call_error(\n        self,\n        endpoint: str,\n        duration_ms: float,\n        status_code: int | None,\n        error: str,\n        error_type: str,\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n        # 4xx at WARNING, 5xx and others at ERROR\n        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n        self.logger.log(\n            level,\n            f\"Keywords Everywhere API call failed: {endpoint}\",\n            extra={\n                \"endpoint\": endpoint,\n                \"duration_ms\": round(duration_ms, 2),\n                \"status_code\": status_code,\n                \"error\": error,\n                \"error_type\": error_type,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n                \"success\": False,\n            },\n        )\n\n    def timeout(self, endpoint: str, timeout_seconds: float) -> None:\n        \"\"\"Log request timeout at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Keywords Everywhere API request timeout\",\n            extra={\n                \"endpoint\": endpoint,\n                \"timeout_seconds\": timeout_seconds,\n            },\n        )\n\n    def rate_limit(\n        self,\n        endpoint: str,\n        retry_after: float | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Keywords Everywhere API rate limit hit (429)\",\n            extra={\n                \"endpoint\": endpoint,\n                \"retry_after_seconds\": retry_after,\n                \"request_id\": request_id,\n            },\n        )\n\n    def auth_failure(self, status_code: int) -> None:\n        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n        self.logger.warning(\n            f\"Keywords Everywhere API authentication failed ({status_code})\",\n            extra={\n                \"status_code\": status_code,\n            },\n        )\n\n    def request_body(\n        self,\n        endpoint: str,\n        keywords: list[str],\n        country: str,\n        data_source: str,\n    ) -> None:\n        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n        # Show first 10 keywords for logging\n        keywords_preview = keywords[:10]\n        if len(keywords) > 10:\n            keywords_preview_str = f\"{keywords_preview}... ({len(keywords)} total)\"\n        else:\n            keywords_preview_str = str(keywords_preview)\n\n        self.logger.debug(\n            \"Keywords Everywhere API request body\",\n            extra={\n                \"endpoint\": endpoint,\n                \"keywords\": keywords_preview_str,\n                \"keyword_count\": len(keywords),\n                \"country\": country,\n                \"data_source\": data_source,\n            },\n        )\n\n    def response_body(\n        self,\n        endpoint: str,\n        keyword_count: int,\n        duration_ms: float,\n        credits_used: int | None = None,\n    ) -> None:\n        \"\"\"Log response body at DEBUG level.\"\"\"\n        self.logger.debug(\n            \"Keywords Everywhere API response body\",\n            extra={\n                \"endpoint\": endpoint,\n                \"keyword_count\": keyword_count,\n                \"duration_ms\": round(duration_ms, 2),\n                \"credits_used\": credits_used,\n            },\n        )\n\n    def credit_usage(\n        self,\n        credits_used: int,\n        credits_remaining: int | None = None,\n    ) -> None:\n        \"\"\"Log API credit usage at INFO level.\"\"\"\n        self.logger.info(\n            \"Keywords Everywhere API credit usage\",\n            extra={\n                \"credits_used\": credits_used,\n                \"credits_remaining\": credits_remaining,\n            },\n        )\n\n    def circuit_state_change(\n        self, previous_state: str, new_state: str, failure_count: int\n    ) -> None:\n        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n        self.logger.warning(\n            \"Keywords Everywhere circuit breaker state changed\",\n            extra={\n                \"previous_state\": previous_state,\n                \"new_state\": new_state,\n                \"failure_count\": failure_count,\n            },\n        )\n\n    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n        self.logger.error(\n            \"Keywords Everywhere circuit breaker opened - API calls disabled\",\n            extra={\n                \"failure_count\": failure_count,\n                \"recovery_timeout_seconds\": recovery_timeout,\n            },\n        )\n\n    def circuit_recovery_attempt(self) -> None:\n        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n        self.logger.info(\"Keywords Everywhere circuit breaker attempting recovery\")\n\n    def circuit_closed(self) -> None:\n        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n        self.logger.info(\"Keywords Everywhere circuit breaker closed - API calls restored\")\n\n    def graceful_fallback(self, operation: str, reason: str) -> None:\n        \"\"\"Log graceful fallback when Keywords Everywhere is unavailable.\"\"\"\n        self.logger.info(\n            \"Keywords Everywhere unavailable, using fallback\",\n            extra={\n                \"operation\": operation,\n                \"reason\": reason,\n            },\n        )\n\n    def keyword_lookup_start(self, keyword_count: int, country: str) -> None:\n        \"\"\"Log keyword lookup operation start at INFO level.\"\"\"\n        self.logger.info(\n            \"Starting keyword data lookup\",\n            extra={\n                \"keyword_count\": keyword_count,\n                \"country\": country,\n            },\n        )\n\n    def keyword_lookup_complete(\n        self,\n        keyword_count: int,\n        duration_ms: float,\n        success: bool,\n        results_count: int = 0,\n    ) -> None:\n        \"\"\"Log keyword lookup operation completion.\"\"\"\n        level = logging.INFO if success else logging.WARNING\n        self.logger.log(\n            level,\n            \"Keyword lookup completed\" if success else \"Keyword lookup failed\",\n            extra={\n                \"keyword_count\": keyword_count,\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": success,\n                \"results_count\": results_count,\n            },\n        )\n\n    def batch_start(\n        self,\n        batch_index: int,\n        batch_size: int,\n        total_batches: int,\n        total_keywords: int,\n    ) -> None:\n        \"\"\"Log batch processing start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Starting keyword batch {batch_index + 1}/{total_batches}\",\n            extra={\n                \"batch_index\": batch_index,\n                \"batch_size\": batch_size,\n                \"total_batches\": total_batches,\n                \"total_keywords\": total_keywords,\n            },\n        )\n\n    def batch_complete(\n        self,\n        batch_index: int,\n        batch_size: int,\n        total_batches: int,\n        success_count: int,\n        duration_ms: float,\n        credits_used: int | None = None,\n    ) -> None:\n        \"\"\"Log batch processing completion at INFO level.\"\"\"\n        self.logger.info(\n            f\"Keyword batch {batch_index + 1}/{total_batches} complete\",\n            extra={\n                \"batch_index\": batch_index,\n                \"batch_size\": batch_size,\n                \"total_batches\": total_batches,\n                \"success_count\": success_count,\n                \"duration_ms\": round(duration_ms, 2),\n                \"credits_used\": credits_used,\n            },\n        )\n\n\n# Singleton Keywords Everywhere logger\nkeywords_everywhere_logger = KeywordsEverywhereLogger()\n\n\nclass DataForSEOLogger:\n    \"\"\"Logger for DataForSEO API operations with required error logging.\n\n    Logs all outbound API calls with endpoint, method, timing.\n    Logs request/response bodies at DEBUG level (truncate large responses).\n    Handles timeouts, rate limits (429), auth failures (401/403).\n    Includes retry attempt number in logs.\n    Masks API credentials in all logs.\n    Logs circuit breaker state changes.\n    Logs credit/cost usage for quota tracking.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.logger = get_logger(\"dataforseo\")\n\n    def api_call_start(\n        self,\n        endpoint: str,\n        method: str = \"POST\",\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log outbound API call start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"DataForSEO API call: {method} {endpoint}\",\n            extra={\n                \"endpoint\": endpoint,\n                \"method\": method,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n            },\n        )\n\n    def api_call_success(\n        self,\n        endpoint: str,\n        duration_ms: float,\n        method: str = \"POST\",\n        cost: float | None = None,\n        tasks_count: int | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log successful API call at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"DataForSEO API call completed: {method} {endpoint}\",\n            extra={\n                \"endpoint\": endpoint,\n                \"method\": method,\n                \"duration_ms\": round(duration_ms, 2),\n                \"cost\": cost,\n                \"tasks_count\": tasks_count,\n                \"request_id\": request_id,\n                \"success\": True,\n            },\n        )\n\n    def api_call_error(\n        self,\n        endpoint: str,\n        duration_ms: float,\n        status_code: int | None,\n        error: str,\n        error_type: str,\n        method: str = \"POST\",\n        retry_attempt: int = 0,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log failed API call at WARNING or ERROR level based on status.\"\"\"\n        # 4xx at WARNING, 5xx and others at ERROR\n        level = logging.WARNING if status_code and 400 <= status_code < 500 else logging.ERROR\n        self.logger.log(\n            level,\n            f\"DataForSEO API call failed: {method} {endpoint}\",\n            extra={\n                \"endpoint\": endpoint,\n                \"method\": method,\n                \"duration_ms\": round(duration_ms, 2),\n                \"status_code\": status_code,\n                \"error\": error,\n                \"error_type\": error_type,\n                \"retry_attempt\": retry_attempt,\n                \"request_id\": request_id,\n                \"success\": False,\n            },\n        )\n\n    def timeout(self, endpoint: str, timeout_seconds: float) -> None:\n        \"\"\"Log request timeout at WARNING level.\"\"\"\n        self.logger.warning(\n            \"DataForSEO API request timeout\",\n            extra={\n                \"endpoint\": endpoint,\n                \"timeout_seconds\": timeout_seconds,\n            },\n        )\n\n    def rate_limit(\n        self,\n        endpoint: str,\n        retry_after: float | None = None,\n        request_id: str | None = None,\n    ) -> None:\n        \"\"\"Log rate limit (429) at WARNING level.\"\"\"\n        self.logger.warning(\n            \"DataForSEO API rate limit hit (429)\",\n            extra={\n                \"endpoint\": endpoint,\n                \"retry_after_seconds\": retry_after,\n                \"request_id\": request_id,\n            },\n        )\n\n    def auth_failure(self, status_code: int) -> None:\n        \"\"\"Log authentication failure (401/403) at WARNING level.\"\"\"\n        self.logger.warning(\n            f\"DataForSEO API authentication failed ({status_code})\",\n            extra={\n                \"status_code\": status_code,\n            },\n        )\n\n    def request_body(\n        self,\n        endpoint: str,\n        body: dict[str, Any] | list[dict[str, Any]],\n    ) -> None:\n        \"\"\"Log request body at DEBUG level (truncate large values).\"\"\"\n        truncated_body = self._truncate_body(body)\n        self.logger.debug(\n            \"DataForSEO API request body\",\n            extra={\n                \"endpoint\": endpoint,\n                \"request_body\": truncated_body,\n            },\n        )\n\n    def response_body(\n        self,\n        endpoint: str,\n        body: dict[str, Any],\n        duration_ms: float,\n        cost: float | None = None,\n    ) -> None:\n        \"\"\"Log response body at DEBUG level (truncate large values).\"\"\"\n        truncated_body = self._truncate_body(body)\n        self.logger.debug(\n            \"DataForSEO API response body\",\n            extra={\n                \"endpoint\": endpoint,\n                \"response_body\": truncated_body,\n                \"duration_ms\": round(duration_ms, 2),\n                \"cost\": cost,\n            },\n        )\n\n    def _truncate_body(\n        self, body: dict[str, Any] | list[Any], max_length: int = 500\n    ) -> dict[str, Any] | list[Any] | str:\n        \"\"\"Truncate large values in body for logging.\"\"\"\n        if isinstance(body, list):\n            if len(body) > 10:\n                return f\"[list with {len(body)} items]\"\n            return [self._truncate_body(item, max_length) for item in body[:10]]\n\n        if isinstance(body, dict):\n            result: dict[str, Any] = {}\n            for key, value in body.items():\n                if isinstance(value, str) and len(value) > max_length:\n                    result[key] = value[:max_length] + f\"... (truncated, {len(value)} chars)\"\n                elif isinstance(value, dict):\n                    result[key] = self._truncate_body(value, max_length)\n                elif isinstance(value, list) and len(value) > 10:\n                    result[key] = f\"[list with {len(value)} items]\"\n                else:\n                    result[key] = value\n            return result\n\n        return body\n\n    def cost_usage(\n        self,\n        cost: float,\n        endpoint: str | None = None,\n    ) -> None:\n        \"\"\"Log API cost usage at INFO level.\"\"\"\n        self.logger.info(\n            \"DataForSEO API cost usage\",\n            extra={\n                \"cost\": cost,\n                \"endpoint\": endpoint,\n            },\n        )\n\n    def circuit_state_change(\n        self, previous_state: str, new_state: str, failure_count: int\n    ) -> None:\n        \"\"\"Log circuit breaker state change at WARNING level.\"\"\"\n        self.logger.warning(\n            \"DataForSEO circuit breaker state changed\",\n            extra={\n                \"previous_state\": previous_state,\n                \"new_state\": new_state,\n                \"failure_count\": failure_count,\n            },\n        )\n\n    def circuit_open(self, failure_count: int, recovery_timeout: float) -> None:\n        \"\"\"Log circuit breaker opening at ERROR level.\"\"\"\n        self.logger.error(\n            \"DataForSEO circuit breaker opened - API calls disabled\",\n            extra={\n                \"failure_count\": failure_count,\n                \"recovery_timeout_seconds\": recovery_timeout,\n            },\n        )\n\n    def circuit_recovery_attempt(self) -> None:\n        \"\"\"Log circuit breaker recovery attempt at INFO level.\"\"\"\n        self.logger.info(\"DataForSEO circuit breaker attempting recovery\")\n\n    def circuit_closed(self) -> None:\n        \"\"\"Log circuit breaker closing at INFO level.\"\"\"\n        self.logger.info(\"DataForSEO circuit breaker closed - API calls restored\")\n\n    def graceful_fallback(self, operation: str, reason: str) -> None:\n        \"\"\"Log graceful fallback when DataForSEO is unavailable.\"\"\"\n        self.logger.info(\n            \"DataForSEO unavailable, using fallback\",\n            extra={\n                \"operation\": operation,\n                \"reason\": reason,\n            },\n        )\n\n    def keyword_search_start(\n        self,\n        keywords: list[str],\n        location_code: int,\n    ) -> None:\n        \"\"\"Log keyword search operation start at INFO level.\"\"\"\n        keywords_preview = keywords[:5]\n        if len(keywords) > 5:\n            keywords_str = f\"{keywords_preview}... ({len(keywords)} total)\"\n        else:\n            keywords_str = str(keywords_preview)\n        self.logger.info(\n            \"Starting DataForSEO keyword search\",\n            extra={\n                \"keywords\": keywords_str,\n                \"keyword_count\": len(keywords),\n                \"location_code\": location_code,\n            },\n        )\n\n    def keyword_search_complete(\n        self,\n        keyword_count: int,\n        duration_ms: float,\n        success: bool,\n        results_count: int = 0,\n        cost: float | None = None,\n    ) -> None:\n        \"\"\"Log keyword search operation completion.\"\"\"\n        level = logging.INFO if success else logging.WARNING\n        self.logger.log(\n            level,\n            \"DataForSEO keyword search completed\" if success else \"DataForSEO keyword search failed\",\n            extra={\n                \"keyword_count\": keyword_count,\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": success,\n                \"results_count\": results_count,\n                \"cost\": cost,\n            },\n        )\n\n    def serp_search_start(\n        self,\n        keyword: str,\n        location_code: int,\n        search_engine: str = \"google\",\n    ) -> None:\n        \"\"\"Log SERP search operation start at INFO level.\"\"\"\n        self.logger.info(\n            \"Starting DataForSEO SERP search\",\n            extra={\n                \"keyword\": keyword[:100],\n                \"location_code\": location_code,\n                \"search_engine\": search_engine,\n            },\n        )\n\n    def serp_search_complete(\n        self,\n        keyword: str,\n        duration_ms: float,\n        success: bool,\n        results_count: int = 0,\n        cost: float | None = None,\n    ) -> None:\n        \"\"\"Log SERP search operation completion.\"\"\"\n        level = logging.INFO if success else logging.WARNING\n        self.logger.log(\n            level,\n            \"DataForSEO SERP search completed\" if success else \"DataForSEO SERP search failed\",\n            extra={\n                \"keyword\": keyword[:100],\n                \"duration_ms\": round(duration_ms, 2),\n                \"success\": success,\n                \"results_count\": results_count,\n                \"cost\": cost,\n            },\n        )\n\n    def batch_start(\n        self,\n        batch_index: int,\n        batch_size: int,\n        total_batches: int,\n        total_items: int,\n        operation: str = \"keyword_search\",\n    ) -> None:\n        \"\"\"Log batch processing start at DEBUG level.\"\"\"\n        self.logger.debug(\n            f\"Starting DataForSEO batch {batch_index + 1}/{total_batches}\",\n            extra={\n                \"batch_index\": batch_index,\n                \"batch_size\": batch_size,\n                \"total_batches\": total_batches,\n                \"total_items\": total_items,\n                \"operation\": operation,\n            },\n        )\n\n    def batch_complete(\n        self,\n        batch_index: int,\n        batch_size: int,\n        total_batches: int,\n        success_count: int,\n        duration_ms: float,\n        cost: float | None = None,\n        operation: str = \"keyword_search\",\n    ) -> None:\n        \"\"\"Log batch processing completion at INFO level.\"\"\"\n        self.logger.info(\n            f\"DataForSEO batch {batch_index + 1}/{total_batches} complete\",\n            extra={\n                \"batch_index\": batch_index,\n                \"batch_size\": batch_size,\n                \"total_batches\": total_batches,\n                \"success_count\": success_count,\n                \"duration_ms\": round(duration_ms, 2),\n                \"cost\": cost,\n                \"operation\": operation,\n            },\n        )\n\n\n# Singleton DataForSEO logger\ndataforseo_logger = DataForSEOLogger()\n","numLines":2075,"startLine":1,"totalLines":2075}}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01WfxuGWXm8Yr1BNUzwtfnAE","type":"message","role":"assistant","content":[{"type":"text","text":"I see the issue. The logging uses `extra={}` dict for structured logging fields, but these don't appear in the simple text log format unless the formatter is set up to include them. The standard formatter just shows the message text. I need to check the log records directly via their `__dict__` or extra attributes. Let me update the tests to check the log records properly."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":25244,"cache_read_input_tokens":65758,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":25244},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"61d33f32-1bcc-42dc-95a1-9e9fbff07a0a"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01WfxuGWXm8Yr1BNUzwtfnAE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01KuPWZcCaYPKGL92tnvuBZo","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"class TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-123\" in log_text, \"project_id should be logged\"\n        assert \"page-456\" in log_text, \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        log_text = caplog.text\n        assert \"Content hash computed\" in log_text or \"compute_hash\" in log_text\n        assert \"duration_ms\" in log_text, \"Should log timing information\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 1, \"Should have debug log entries\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-789\" in log_text, \"project_id should be logged\"\n        assert \"crawl-new\" in log_text, \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        log_text = caplog.text\n        assert \"duration_ms\" in log_text, \"Should include timing information\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"negative\" in log_text.lower()\n        assert \"new_page_threshold\" in log_text\n        assert \"-5\" in log_text or \"-1\" in log_text  # The rejected value\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text","new_string":"class TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":25244,"cache_read_input_tokens":65758,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":25244},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"75485e07-c6ea-493b-9a33-1bc40f427b67"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01KuPWZcCaYPKGL92tnvuBZo","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"03bee7b7-063b-4d5d-b20a-93a4d754e44c","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"class TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-123\" in log_text, \"project_id should be logged\"\n        assert \"page-456\" in log_text, \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        log_text = caplog.text\n        assert \"Content hash computed\" in log_text or \"compute_hash\" in log_text\n        assert \"duration_ms\" in log_text, \"Should log timing information\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 1, \"Should have debug log entries\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-789\" in log_text, \"project_id should be logged\"\n        assert \"crawl-new\" in log_text, \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        log_text = caplog.text\n        assert \"duration_ms\" in log_text, \"Should include timing information\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"negative\" in log_text.lower()\n        assert \"new_page_threshold\" in log_text\n        assert \"-5\" in log_text or \"-1\" in log_text  # The rejected value\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text","newString":"class TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-123\" in log_text, \"project_id should be logged\"\n        assert \"page-456\" in log_text, \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        log_text = caplog.text\n        assert \"Content hash computed\" in log_text or \"compute_hash\" in log_text\n        assert \"duration_ms\" in log_text, \"Should log timing information\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 1, \"Should have debug log entries\"\n\n        # Check for entity IDs in logs\n        log_text = caplog.text\n        assert \"proj-789\" in log_text, \"project_id should be logged\"\n        assert \"crawl-new\" in log_text, \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        log_text = caplog.text\n        assert \"duration_ms\" in log_text, \"Should include timing information\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"negative\" in log_text.lower()\n        assert \"new_page_threshold\" in log_text\n        assert \"-5\" in log_text or \"-1\" in log_text  # The rejected value\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n        original_monotonic = time.monotonic\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n        assert \"slow-proj\" in log_text\n        assert \"slow-page\" in log_text\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n        assert \"slow-project\" in log_text\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n        assert \"err-proj\" in log_text\n        assert \"err-page\" in log_text\n        assert \"ValueError\" in log_text\n        assert \"Mocked hash error\" in log_text\n        # Should include stack trace\n        assert \"stack_trace\" in log_text or \"Traceback\" in log_text\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n        assert \"err-project\" in log_text\n        assert \"err-crawl\" in log_text\n        assert \"RuntimeError\" in log_text\n        assert \"Bad page access\" in log_text\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text\n","structuredPatch":[{"oldStart":994,"oldLines":6,"newStart":994,"newLines":10,"lines":["     - Capture and display logs from failed tests","     - Include timing information in test reports","     - Log mock/stub invocations for debugging","+","+    Note: The logging module uses `extra={}` dict for structured logging fields.","+    These fields are stored on the LogRecord object but not rendered in simple","+    text output. Tests check LogRecord attributes directly via record.__dict__.","     \"\"\""," ","     def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:"]},{"oldStart":1008,"oldLines":11,"newStart":1012,"newLines":17,"lines":["         debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]","         assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\""," ","-        # Check for entity IDs in logs","-        log_text = caplog.text","-        assert \"proj-123\" in log_text, \"project_id should be logged\"","-        assert \"page-456\" in log_text, \"page_id should be logged\"","+        # Check for entity IDs in log records' extra data","+        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]","+        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\""," ","+        # Extra fields are stored directly on the LogRecord","+        hash_record = compute_hash_logs[0]","+        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"","+        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"","+        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"","+        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"","+","     def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:","         \"\"\"Test that compute_hash logs completion with timing.\"\"\"","         import logging"]},{"oldStart":1022,"oldLines":10,"newStart":1032,"newLines":14,"lines":["             hasher.compute_hash(title=\"Test\", body_text=\"Content\")"," ","         # Check for hash computed log","-        log_text = caplog.text","-        assert \"Content hash computed\" in log_text or \"compute_hash\" in log_text","-        assert \"duration_ms\" in log_text, \"Should log timing information\"","+        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]","+        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\""," ","+        # Check for timing in log record's extra data","+        hash_record = hash_computed_logs[0]","+        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"","+        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"","+","     def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:","         \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"","         import logging"]},{"oldStart":1041,"oldLines":13,"newStart":1055,"newLines":15,"lines":["             )"," ","         # Check for method entry log","-        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]","-        assert len(debug_logs) >= 1, \"Should have debug log entries\"","+        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]","+        assert len(compare_logs) >= 1, \"Should have compare() called log entry\""," ","-        # Check for entity IDs in logs","-        log_text = caplog.text","-        assert \"proj-789\" in log_text, \"project_id should be logged\"","-        assert \"crawl-new\" in log_text, \"crawl_id should be logged\"","+        # Check for entity IDs in log record's extra data","+        compare_record = compare_logs[0]","+        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"","+        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"","+        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"","+        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\""," ","     def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:","         \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\""]},{"oldStart":1073,"oldLines":7,"newStart":1089,"newLines":7,"lines":["         \"\"\"Test that change detection logs include timing information.\"\"\"","         import logging"," ","-        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):","+        with caplog.at_level(logging.INFO, logger=\"change_detection\"):","             detector = ChangeDetector()","             detector.compare(","                 crawl_id=\"new\","]},{"oldStart":1088,"oldLines":9,"newStart":1104,"newLines":14,"lines":["                 ],","             )"," ","-        log_text = caplog.text","-        assert \"duration_ms\" in log_text, \"Should include timing information\"","+        # Check for completion log with timing","+        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]","+        assert len(completed_logs) >= 1, \"Should have completion log\""," ","+        completed_record = completed_logs[0]","+        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"","+        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"","+","     def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:","         \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"","         import logging"]},{"oldStart":1099,"oldLines":11,"newStart":1120,"newLines":20,"lines":["             with pytest.raises(ValueError):","                 ChangeDetector(new_page_threshold=-5)"," ","+        # Check for validation failure log","+        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]","+        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"","+","         log_text = caplog.text","-        assert \"Validation failed\" in log_text or \"negative\" in log_text.lower()","-        assert \"new_page_threshold\" in log_text","-        assert \"-5\" in log_text or \"-1\" in log_text  # The rejected value","+        assert \"Validation failed\" in log_text"," ","+        # Check that field name and value are in log record's extra data","+        validation_record = warning_logs[0]","+        assert hasattr(validation_record, \"field\"), \"Log record should have field\"","+        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"","+        assert hasattr(validation_record, \"value\"), \"Log record should have value\"","+        assert validation_record.value == -5, \"value should be -5\"","+","     def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:","         \"\"\"Test that invalid percentage threshold is logged.\"\"\"","         import logging"]},{"oldStart":1112,"oldLines":6,"newStart":1142,"newLines":10,"lines":["             with pytest.raises(ValueError):","                 ChangeDetector(change_percentage_threshold=2.0)"," ","+        # Check for validation failure log","+        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]","+        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"","+","         log_text = caplog.text","         assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text"," "]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01KVyom7TJj6FWRfQgcKirNo","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me fix the slow operation logging tests and exception handling tests to also check the log record attributes properly:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3842,"cache_read_input_tokens":91002,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3842},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"8811ff52-88c3-4f86-a0f3-0235dfcdf565"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01KVyom7TJj6FWRfQgcKirNo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GeXrnxQJQPDw2NxVgpVKUw","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"class TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n        original_monotonic = time.monotonic\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n        assert \"slow-proj\" in log_text\n        assert \"slow-page\" in log_text\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n        assert \"slow-project\" in log_text","new_string":"class TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"\n        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"\n        assert slow_record.page_id == \"slow-page\", \"page_id should match\"\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-project\", \"project_id should match\""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3842,"cache_read_input_tokens":91002,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3842},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"935c55d0-a543-44a1-9b8e-4633aa69ac2e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GeXrnxQJQPDw2NxVgpVKUw","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"1fa4613d-4d81-41fd-89af-2306de8747ed","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"class TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n        original_monotonic = time.monotonic\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n        assert \"slow-proj\" in log_text\n        assert \"slow-page\" in log_text\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n        assert \"slow-project\" in log_text","newString":"class TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"\n        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"\n        assert slow_record.page_id == \"slow-page\", \"page_id should match\"\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-project\", \"project_id should match\"","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n        original_monotonic = time.monotonic\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n        assert \"slow-proj\" in log_text\n        assert \"slow-page\" in log_text\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n        assert \"slow-project\" in log_text\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n        assert \"err-proj\" in log_text\n        assert \"err-page\" in log_text\n        assert \"ValueError\" in log_text\n        assert \"Mocked hash error\" in log_text\n        # Should include stack trace\n        assert \"stack_trace\" in log_text or \"Traceback\" in log_text\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n        assert \"err-project\" in log_text\n        assert \"err-crawl\" in log_text\n        assert \"RuntimeError\" in log_text\n        assert \"Bad page access\" in log_text\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text\n","structuredPatch":[{"oldStart":1164,"oldLines":7,"newStart":1164,"newLines":6,"lines":[" ","         # Mock time.monotonic to return values indicating slow operation","         call_count = 0","-        original_monotonic = time.monotonic"," ","         def mock_monotonic() -> float:","             nonlocal call_count"]},{"oldStart":1181,"oldLines":9,"newStart":1180,"newLines":16,"lines":[" ","         log_text = caplog.text","         assert \"Slow content hash computation\" in log_text","-        assert \"slow-proj\" in log_text","-        assert \"slow-page\" in log_text"," ","+        # Check log record extra data for entity IDs","+        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]","+        assert len(slow_logs) >= 1, \"Should have slow operation log\"","+        slow_record = slow_logs[0]","+        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"","+        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"","+        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"","+        assert slow_record.page_id == \"slow-page\", \"page_id should match\"","+","     def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:","         \"\"\"Test that slow change detection logs a warning."," "]},{"oldStart":1217,"oldLines":9,"newStart":1223,"newLines":15,"lines":[" ","         log_text = caplog.text","         assert \"Slow change detection\" in log_text","-        assert \"slow-project\" in log_text"," ","+        # Check log record extra data for entity IDs","+        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]","+        assert len(slow_logs) >= 1, \"Should have slow operation log\"","+        slow_record = slow_logs[0]","+        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"","+        assert slow_record.project_id == \"slow-project\", \"project_id should match\""," ","+"," class TestExceptionHandling:","     \"\"\"Tests for exception handling paths.\"\"\""," "]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Aw15WtZSSYpVuDXo3e1g27","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01R5Cayr1N998JyhwGxuoMjB","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"class TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n        assert \"err-proj\" in log_text\n        assert \"err-page\" in log_text\n        assert \"ValueError\" in log_text\n        assert \"Mocked hash error\" in log_text\n        # Should include stack trace\n        assert \"stack_trace\" in log_text or \"Traceback\" in log_text\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n        assert \"err-project\" in log_text\n        assert \"err-crawl\" in log_text\n        assert \"RuntimeError\" in log_text\n        assert \"Bad page access\" in log_text","new_string":"class TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Content hash computation failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-proj\", \"project_id should match\"\n        assert hasattr(error_record, \"page_id\"), \"Log record should have page_id\"\n        assert error_record.page_id == \"err-page\", \"page_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"ValueError\", \"error_type should be ValueError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Mocked hash error\" in error_record.error_message, \"error_message should contain the error\"\n        # Stack trace should be included\n        assert hasattr(error_record, \"stack_trace\"), \"Log record should have stack_trace\"\n        assert \"Traceback\" in error_record.stack_trace, \"stack_trace should contain traceback\"\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Change detection failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-project\", \"project_id should match\"\n        assert hasattr(error_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert error_record.crawl_id == \"err-crawl\", \"crawl_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"RuntimeError\", \"error_type should be RuntimeError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Bad page access\" in error_record.error_message, \"error_message should contain the error\""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1880,"cache_read_input_tokens":94844,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1880},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"7a531167-c85b-4238-9036-f3fb2b0868a6"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01R5Cayr1N998JyhwGxuoMjB","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"f435fa92-386a-46c3-902a-a0de5828bfb9","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"class TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n        assert \"err-proj\" in log_text\n        assert \"err-page\" in log_text\n        assert \"ValueError\" in log_text\n        assert \"Mocked hash error\" in log_text\n        # Should include stack trace\n        assert \"stack_trace\" in log_text or \"Traceback\" in log_text\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n        assert \"err-project\" in log_text\n        assert \"err-crawl\" in log_text\n        assert \"RuntimeError\" in log_text\n        assert \"Bad page access\" in log_text","newString":"class TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Content hash computation failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-proj\", \"project_id should match\"\n        assert hasattr(error_record, \"page_id\"), \"Log record should have page_id\"\n        assert error_record.page_id == \"err-page\", \"page_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"ValueError\", \"error_type should be ValueError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Mocked hash error\" in error_record.error_message, \"error_message should contain the error\"\n        # Stack trace should be included\n        assert hasattr(error_record, \"stack_trace\"), \"Log record should have stack_trace\"\n        assert \"Traceback\" in error_record.stack_trace, \"stack_trace should contain traceback\"\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Change detection failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-project\", \"project_id should match\"\n        assert hasattr(error_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert error_record.crawl_id == \"err-crawl\", \"crawl_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"RuntimeError\", \"error_type should be RuntimeError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Bad page access\" in error_record.error_message, \"error_message should contain the error\"","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"\n        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"\n        assert slow_record.page_id == \"slow-page\", \"page_id should match\"\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-project\", \"project_id should match\"\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n        assert \"err-proj\" in log_text\n        assert \"err-page\" in log_text\n        assert \"ValueError\" in log_text\n        assert \"Mocked hash error\" in log_text\n        # Should include stack trace\n        assert \"stack_trace\" in log_text or \"Traceback\" in log_text\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n        assert \"err-project\" in log_text\n        assert \"err-crawl\" in log_text\n        assert \"RuntimeError\" in log_text\n        assert \"Bad page access\" in log_text\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text\n","structuredPatch":[{"oldStart":1260,"oldLines":13,"newStart":1260,"newLines":23,"lines":[" ","         log_text = caplog.text","         assert \"Content hash computation failed\" in log_text","-        assert \"err-proj\" in log_text","-        assert \"err-page\" in log_text","-        assert \"ValueError\" in log_text","-        assert \"Mocked hash error\" in log_text","-        # Should include stack trace","-        assert \"stack_trace\" in log_text or \"Traceback\" in log_text"," ","+        # Check log record extra data","+        error_logs = [r for r in caplog.records if \"Content hash computation failed\" in r.message]","+        assert len(error_logs) >= 1, \"Should have error log\"","+        error_record = error_logs[0]","+        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"","+        assert error_record.project_id == \"err-proj\", \"project_id should match\"","+        assert hasattr(error_record, \"page_id\"), \"Log record should have page_id\"","+        assert error_record.page_id == \"err-page\", \"page_id should match\"","+        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"","+        assert error_record.error_type == \"ValueError\", \"error_type should be ValueError\"","+        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"","+        assert \"Mocked hash error\" in error_record.error_message, \"error_message should contain the error\"","+        # Stack trace should be included","+        assert hasattr(error_record, \"stack_trace\"), \"Log record should have stack_trace\"","+        assert \"Traceback\" in error_record.stack_trace, \"stack_trace should contain traceback\"","+","     def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:","         \"\"\"Test that exceptions in compare() are logged with full context."," "]},{"oldStart":1297,"oldLines":12,"newStart":1307,"newLines":21,"lines":[" ","         log_text = caplog.text","         assert \"Change detection failed\" in log_text","-        assert \"err-project\" in log_text","-        assert \"err-crawl\" in log_text","-        assert \"RuntimeError\" in log_text","-        assert \"Bad page access\" in log_text"," ","+        # Check log record extra data","+        error_logs = [r for r in caplog.records if \"Change detection failed\" in r.message]","+        assert len(error_logs) >= 1, \"Should have error log\"","+        error_record = error_logs[0]","+        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"","+        assert error_record.project_id == \"err-project\", \"project_id should match\"","+        assert hasattr(error_record, \"crawl_id\"), \"Log record should have crawl_id\"","+        assert error_record.crawl_id == \"err-crawl\", \"crawl_id should match\"","+        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"","+        assert error_record.error_type == \"RuntimeError\", \"error_type should be RuntimeError\"","+        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"","+        assert \"Bad page access\" in error_record.error_message, \"error_message should contain the error\""," ","+"," class TestSingletonReset:","     \"\"\"Tests for singleton behavior with proper reset."," "]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01VJjUSC6mQE59mstwxLPXqv","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me fix the last failing test for compare_from_dicts logging:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2046,"cache_read_input_tokens":96724,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2046},"output_tokens":6,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"3d6d6a1d-86cd-4612-bba0-e2043b1444c4"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01VJjUSC6mQE59mstwxLPXqv","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GEAvenVvEpqesQsersNMk3","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text","new_string":"    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        # Check for compare_from_dicts log entry with project_id in extra data\n        compare_logs = [r for r in caplog.records if \"compare_from_dicts\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare_from_dicts log entry\"\n\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"dict-project\", \"project_id should match\""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2046,"cache_read_input_tokens":96724,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2046},"output_tokens":6,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"49105587-7eee-4873-8f5a-21f756413126"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GEAvenVvEpqesQsersNMk3","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"6c9ebbc6-f413-4c1f-8b97-ebd9414b9d83","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text","newString":"    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        # Check for compare_from_dicts log entry with project_id in extra data\n        compare_logs = [r for r in caplog.records if \"compare_from_dicts\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare_from_dicts log entry\"\n\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"dict-project\", \"project_id should match\"","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"\n        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"\n        assert slow_record.page_id == \"slow-page\", \"page_id should match\"\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-project\", \"project_id should match\"\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Content hash computation failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-proj\", \"project_id should match\"\n        assert hasattr(error_record, \"page_id\"), \"Log record should have page_id\"\n        assert error_record.page_id == \"err-page\", \"page_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"ValueError\", \"error_type should be ValueError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Mocked hash error\" in error_record.error_message, \"error_message should contain the error\"\n        # Stack trace should be included\n        assert hasattr(error_record, \"stack_trace\"), \"Log record should have stack_trace\"\n        assert \"Traceback\" in error_record.stack_trace, \"stack_trace should contain traceback\"\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Change detection failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-project\", \"project_id should match\"\n        assert hasattr(error_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert error_record.crawl_id == \"err-crawl\", \"crawl_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"RuntimeError\", \"error_type should be RuntimeError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Bad page access\" in error_record.error_message, \"error_message should contain the error\"\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        log_text = caplog.text\n        assert \"dict-project\" in log_text\n","structuredPatch":[{"oldStart":1568,"oldLines":5,"newStart":1568,"newLines":10,"lines":["                 project_id=\"dict-project\",","             )"," ","-        log_text = caplog.text","-        assert \"dict-project\" in log_text","+        # Check for compare_from_dicts log entry with project_id in extra data","+        compare_logs = [r for r in caplog.records if \"compare_from_dicts\" in r.message]","+        assert len(compare_logs) >= 1, \"Should have compare_from_dicts log entry\"","+","+        compare_record = compare_logs[0]","+        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"","+        assert compare_record.project_id == \"dict-project\", \"project_id should match\""]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Xm2BCXconjWY29ZGU7w9jn","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me run the tests again to verify they pass:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":899,"cache_read_input_tokens":98770,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":899},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"09b6d45f-0845-41d6-8e15-69eff3dc7565"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Xm2BCXconjWY29ZGU7w9jn","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GcimLHmJDsyUHyGWrpuZDz","name":"Bash","input":{"command":"python3 -m pytest tests/utils/test_change_detection.py -v --tb=short 2>&1 | tail -50","description":"Run change detection tests again"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":899,"cache_read_input_tokens":98770,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":899},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"f23fe397-03c9-433e-bf6e-bd480683f312"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GcimLHmJDsyUHyGWrpuZDz","type":"tool_result","content":"tests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 47%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 49%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_change_percentage PASSED [ 51%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_with_project_id PASSED [ 52%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts PASSED [ 54%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts_custom_keys PASSED [ 55%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changes_list PASSED [ 56%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_none_hash_treated_as_change PASSED [ 57%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_both_none_hash_is_unchanged PASSED [ 58%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_content_hasher_singleton PASSED [ 59%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_change_detector_singleton PASSED [ 60%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_compute_content_hash PASSED [ 62%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_detect_changes PASSED [ 63%]\ntests/utils/test_change_detection.py::TestChangeType::test_valid_change_types PASSED [ 64%]\ntests/utils/test_change_detection.py::TestChangeType::test_change_type_values PASSED [ 65%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_large_number_of_pages PASSED [ 66%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_unicode_content_hash PASSED [ 67%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 68%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 70%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [ 71%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_method_entry_debug PASSED [ 72%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_hash_computed_debug PASSED [ 73%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_method_entry_debug PASSED [ 74%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_state_transition_info PASSED [ 75%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_with_timing PASSED [ 77%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_validation_failure PASSED [ 78%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_invalid_percentage_threshold PASSED [ 79%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_hash_computation_logs_warning PASSED [ 80%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_change_detection_logs_warning PASSED [ 81%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_content_hasher_exception_logs_error PASSED [ 82%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_change_detector_exception_logs_error PASSED [ 83%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_hasher PASSED [ 85%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_detector PASSED [ 86%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_computed_at_default_is_utc PASSED [ 87%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_to_dict_with_none_computed_at PASSED [ 88%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_unchanged_page_change PASSED [ 89%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_truncates_long_title PASSED [ 90%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_none_title PASSED [ 91%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_to_dict_none_title PASSED [ 93%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_none_content_hash PASSED [ 94%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_empty_headings_list PASSED [ 95%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_headings_list_with_non_string PASSED [ 96%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_fallback_to_markdown_for_body PASSED [ 97%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_missing_url_key_filtered PASSED [ 98%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging PASSED [100%]\n\n============================== 87 passed in 0.12s ==============================","is_error":false}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"3a3b0e74-fcf7-4788-a0cc-bb7d7ced7d09","tool_use_result":{"stdout":"tests/utils/test_change_detection.py::TestChangeDetector::test_compare_changed_pages PASSED [ 45%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_mixed_changes PASSED [ 47%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_new_pages PASSED [ 48%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_new_page_threshold PASSED [ 49%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_by_change_percentage PASSED [ 50%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_significance_below_change_percentage PASSED [ 51%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_with_project_id PASSED [ 52%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts PASSED [ 54%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_from_dicts_custom_keys PASSED [ 55%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_changes_list PASSED [ 56%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_none_hash_treated_as_change PASSED [ 57%]\ntests/utils/test_change_detection.py::TestChangeDetector::test_compare_both_none_hash_is_unchanged PASSED [ 58%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_content_hasher_singleton PASSED [ 59%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_get_change_detector_singleton PASSED [ 60%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_compute_content_hash PASSED [ 62%]\ntests/utils/test_change_detection.py::TestConvenienceFunctions::test_detect_changes PASSED [ 63%]\ntests/utils/test_change_detection.py::TestChangeType::test_valid_change_types PASSED [ 64%]\ntests/utils/test_change_detection.py::TestChangeType::test_change_type_values PASSED [ 65%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_large_number_of_pages PASSED [ 66%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_unicode_content_hash PASSED [ 67%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 68%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 70%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [ 71%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_method_entry_debug PASSED [ 72%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_hash_computed_debug PASSED [ 73%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_method_entry_debug PASSED [ 74%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_state_transition_info PASSED [ 75%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_with_timing PASSED [ 77%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_validation_failure PASSED [ 78%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_invalid_percentage_threshold PASSED [ 79%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_hash_computation_logs_warning PASSED [ 80%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_change_detection_logs_warning PASSED [ 81%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_content_hasher_exception_logs_error PASSED [ 82%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_change_detector_exception_logs_error PASSED [ 83%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_hasher PASSED [ 85%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_detector PASSED [ 86%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_computed_at_default_is_utc PASSED [ 87%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_to_dict_with_none_computed_at PASSED [ 88%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_unchanged_page_change PASSED [ 89%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_truncates_long_title PASSED [ 90%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_none_title PASSED [ 91%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_to_dict_none_title PASSED [ 93%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_none_content_hash PASSED [ 94%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_empty_headings_list PASSED [ 95%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_headings_list_with_non_string PASSED [ 96%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_fallback_to_markdown_for_body PASSED [ 97%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_missing_url_key_filtered PASSED [ 98%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging PASSED [100%]\n\n============================== 87 passed in 0.12s ==============================","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01BTc5MAQ1oLCps6xMfeKFd7","type":"message","role":"assistant","content":[{"type":"text","text":"All 87 tests pass. Now let me check the coverage to see how much we've improved:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1899,"cache_read_input_tokens":99669,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1899},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"37e1698b-7233-4b34-b815-ff69212b4b42"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01BTc5MAQ1oLCps6xMfeKFd7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0175AHnaCd3Q1S7bi26QDLff","name":"Bash","input":{"command":"python3 -m pytest tests/utils/test_change_detection.py -v --cov=app.utils.change_detection --cov-report=term-missing 2>&1 | tail -20","description":"Run tests with coverage report"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1899,"cache_read_input_tokens":99669,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1899},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"27b454c2-3f31-4be7-be8a-c8caa1806ff0"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0175AHnaCd3Q1S7bi26QDLff","type":"tool_result","content":"tests/utils/test_change_detection.py::TestPageChangeAdditional::test_unchanged_page_change PASSED [ 89%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_truncates_long_title PASSED [ 90%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_none_title PASSED [ 91%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_to_dict_none_title PASSED [ 93%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_none_content_hash PASSED [ 94%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_empty_headings_list PASSED [ 95%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_headings_list_with_non_string PASSED [ 96%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_fallback_to_markdown_for_body PASSED [ 97%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_missing_url_key_filtered PASSED [ 98%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging PASSED [100%]\n\n================================ tests coverage ================================\n_______________ coverage: platform darwin, python 3.12.8-final-0 _______________\n\nName                            Stmts   Miss  Cover   Missing\n-------------------------------------------------------------\napp/utils/change_detection.py     207      0   100%\n-------------------------------------------------------------\nTOTAL                             207      0   100%\n============================== 87 passed in 0.09s ==============================","is_error":false}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"4a959c18-e600-44e6-959e-1064e8de8b10","tool_use_result":{"stdout":"tests/utils/test_change_detection.py::TestPageChangeAdditional::test_unchanged_page_change PASSED [ 89%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_truncates_long_title PASSED [ 90%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_none_title PASSED [ 91%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_to_dict_none_title PASSED [ 93%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_none_content_hash PASSED [ 94%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_empty_headings_list PASSED [ 95%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_headings_list_with_non_string PASSED [ 96%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_fallback_to_markdown_for_body PASSED [ 97%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_missing_url_key_filtered PASSED [ 98%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging PASSED [100%]\n\n================================ tests coverage ================================\n_______________ coverage: platform darwin, python 3.12.8-final-0 _______________\n\nName                            Stmts   Miss  Cover   Missing\n-------------------------------------------------------------\napp/utils/change_detection.py     207      0   100%\n-------------------------------------------------------------\nTOTAL                             207      0   100%\n============================== 87 passed in 0.09s ==============================","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TeCYZwnFQmvxR93ZxaoTxA","type":"message","role":"assistant","content":[{"type":"text","text":"Excellent! We've achieved **100% test coverage** for the change detection module. Now let me run the quality checks (typecheck and lint):"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":635,"cache_read_input_tokens":101568,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":635},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"3bae7d17-b643-4b23-9882-153904c7b6ed"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TeCYZwnFQmvxR93ZxaoTxA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017rrbYQSaSo4aBNThyKdx7D","name":"TodoWrite","input":{"todos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"completed","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"in_progress","activeForm":"Running quality checks"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":635,"cache_read_input_tokens":101568,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":635},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"5e4d4cc1-b294-4a1f-9052-2573e7d8678f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017rrbYQSaSo4aBNThyKdx7D","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"4cbe1d91-047c-439b-8f4c-1033ad961a55","tool_use_result":{"oldTodos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"in_progress","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"pending","activeForm":"Running quality checks"}],"newTodos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"completed","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"in_progress","activeForm":"Running quality checks"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_018uhhN4xzCprcR91zqgNe2p","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MWWbipKMHiXVM865xPFXGd","name":"Bash","input":{"command":"python3 -m mypy tests/utils/test_change_detection.py --no-error-summary 2>&1","description":"Type check the test file"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":317,"cache_read_input_tokens":102203,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":317},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"51c9053c-2068-49ed-bf4f-b3c5a259f2f5"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 1\napp/core/logging.py:25:27: error: Name \"jsonlogger.JsonFormatter\" is not\ndefined  [name-defined]\n    class CustomJsonFormatter(jsonlogger.JsonFormatter):\n                              ^\napp/core/redis.py:246:21: error: Incompatible types in \"await\" (actual type\n\"Awaitable[bool] | bool\", expected type \"Awaitable[Any]\")  [misc]\n                        await self._client.ping()\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~","is_error":true,"tool_use_id":"toolu_01MWWbipKMHiXVM865xPFXGd"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"eb7ecc46-9793-482e-bc5d-cb4022a422b4","tool_use_result":"Error: Exit code 1\napp/core/logging.py:25:27: error: Name \"jsonlogger.JsonFormatter\" is not\ndefined  [name-defined]\n    class CustomJsonFormatter(jsonlogger.JsonFormatter):\n                              ^\napp/core/redis.py:246:21: error: Incompatible types in \"await\" (actual type\n\"Awaitable[bool] | bool\", expected type \"Awaitable[Any]\")  [misc]\n                        await self._client.ping()\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01F8bokE6g6hYnHqi6oJjUMA","type":"message","role":"assistant","content":[{"type":"text","text":"Those errors are in other files, not in our test file. Let me check specifically for the test file:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":244,"cache_read_input_tokens":102520,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":244},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"eb721c12-55b4-4b91-be96-c002f51a0aed"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01F8bokE6g6hYnHqi6oJjUMA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TuaARScgRnTLiaZP7JUzff","name":"Bash","input":{"command":"python3 -m mypy tests/utils/test_change_detection.py --ignore-missing-imports 2>&1","description":"Type check the test file with missing imports ignored"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":244,"cache_read_input_tokens":102520,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":244},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"79f204e1-b45d-4ef5-940e-241c9aad161c"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 1\napp/core/logging.py:25:27: error: Name \"jsonlogger.JsonFormatter\" is not\ndefined  [name-defined]\n    class CustomJsonFormatter(jsonlogger.JsonFormatter):\n                              ^\napp/core/redis.py:246:21: error: Incompatible types in \"await\" (actual type\n\"Awaitable[bool] | bool\", expected type \"Awaitable[Any]\")  [misc]\n                        await self._client.ping()\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~\nFound 2 errors in 2 files (checked 1 source file)","is_error":true,"tool_use_id":"toolu_01TuaARScgRnTLiaZP7JUzff"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"d68b0259-0a08-472d-9e68-5686b1112170","tool_use_result":"Error: Exit code 1\napp/core/logging.py:25:27: error: Name \"jsonlogger.JsonFormatter\" is not\ndefined  [name-defined]\n    class CustomJsonFormatter(jsonlogger.JsonFormatter):\n                              ^\napp/core/redis.py:246:21: error: Incompatible types in \"await\" (actual type\n\"Awaitable[bool] | bool\", expected type \"Awaitable[Any]\")  [misc]\n                        await self._client.ping()\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~\nFound 2 errors in 2 files (checked 1 source file)"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_014KHog3nqh7ibUU3mfimRrk","type":"message","role":"assistant","content":[{"type":"text","text":"The type errors are in other files (logging.py and redis.py), not in our test file. Let me check if there are any project-level lint checks:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":289,"cache_read_input_tokens":102764,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":289},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"061c845e-b8ca-4fb8-9ef1-46f4241319db"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_014KHog3nqh7ibUU3mfimRrk","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014rpEf9T3LsVFRL3sQegCeT","name":"Bash","input":{"command":"python3 -m ruff check tests/utils/test_change_detection.py 2>&1","description":"Run ruff linter on test file"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":289,"cache_read_input_tokens":102764,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":289},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"90417bc7-ae96-4a7a-9bc6-dc5d5683c4b5"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 1\nSIM117 Use a single `with` statement with multiple contexts instead of nested `with` statements\n    --> tests/utils/test_change_detection.py:1119:9\n     |\n1117 |           import logging\n1118 |\n1119 | /         with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n1120 | |             with pytest.raises(ValueError):\n     | |___________________________________________^\n1121 |                   ChangeDetector(new_page_threshold=-5)\n     |\nhelp: Combine `with` statements\n\nSIM117 Use a single `with` statement with multiple contexts instead of nested `with` statements\n    --> tests/utils/test_change_detection.py:1141:9\n     |\n1139 |           import logging\n1140 |\n1141 | /         with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n1142 | |             with pytest.raises(ValueError):\n     | |___________________________________________^\n1143 |                   ChangeDetector(change_percentage_threshold=2.0)\n     |\nhelp: Combine `with` statements\n\nI001 [*] Import block is un-sorted or un-formatted\n    --> tests/utils/test_change_detection.py:1395:9\n     |\n1393 |     def test_to_dict_with_none_computed_at(self) -> None:\n1394 |         \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n1395 |         from datetime import datetime, UTC\n     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n1396 |\n1397 |         # Create with explicit computed_at\n     |\nhelp: Organize imports\n\nFound 3 errors.\n[*] 1 fixable with the `--fix` option.","is_error":true,"tool_use_id":"toolu_014rpEf9T3LsVFRL3sQegCeT"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"24ab896e-813c-4873-b967-5240ac9987bc","tool_use_result":"Error: Exit code 1\nSIM117 Use a single `with` statement with multiple contexts instead of nested `with` statements\n    --> tests/utils/test_change_detection.py:1119:9\n     |\n1117 |           import logging\n1118 |\n1119 | /         with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n1120 | |             with pytest.raises(ValueError):\n     | |___________________________________________^\n1121 |                   ChangeDetector(new_page_threshold=-5)\n     |\nhelp: Combine `with` statements\n\nSIM117 Use a single `with` statement with multiple contexts instead of nested `with` statements\n    --> tests/utils/test_change_detection.py:1141:9\n     |\n1139 |           import logging\n1140 |\n1141 | /         with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n1142 | |             with pytest.raises(ValueError):\n     | |___________________________________________^\n1143 |                   ChangeDetector(change_percentage_threshold=2.0)\n     |\nhelp: Combine `with` statements\n\nI001 [*] Import block is un-sorted or un-formatted\n    --> tests/utils/test_change_detection.py:1395:9\n     |\n1393 |     def test_to_dict_with_none_computed_at(self) -> None:\n1394 |         \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n1395 |         from datetime import datetime, UTC\n     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n1396 |\n1397 |         # Create with explicit computed_at\n     |\nhelp: Organize imports\n\nFound 3 errors.\n[*] 1 fixable with the `--fix` option."}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01N87kEqizne2KfUALK77yzB","type":"message","role":"assistant","content":[{"type":"text","text":"Let me fix those lint issues:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":588,"cache_read_input_tokens":103053,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":588},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"587ce107-cdf6-478b-b918-59958da5f70c"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01N87kEqizne2KfUALK77yzB","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013NXbvW7xfS3PPNtvLU2pbn","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)","new_string":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):\n            ChangeDetector(new_page_threshold=-5)"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":588,"cache_read_input_tokens":103053,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":588},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"e07970cd-56b2-4fd1-a660-e3ec897799a7"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013NXbvW7xfS3PPNtvLU2pbn","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"c9fee079-f3db-4897-9e47-1367f78ccd61","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)","newString":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):\n            ChangeDetector(new_page_threshold=-5)","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"\n        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"\n        assert slow_record.page_id == \"slow-page\", \"page_id should match\"\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-project\", \"project_id should match\"\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Content hash computation failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-proj\", \"project_id should match\"\n        assert hasattr(error_record, \"page_id\"), \"Log record should have page_id\"\n        assert error_record.page_id == \"err-page\", \"page_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"ValueError\", \"error_type should be ValueError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Mocked hash error\" in error_record.error_message, \"error_message should contain the error\"\n        # Stack trace should be included\n        assert hasattr(error_record, \"stack_trace\"), \"Log record should have stack_trace\"\n        assert \"Traceback\" in error_record.stack_trace, \"stack_trace should contain traceback\"\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Change detection failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-project\", \"project_id should match\"\n        assert hasattr(error_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert error_record.crawl_id == \"err-crawl\", \"crawl_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"RuntimeError\", \"error_type should be RuntimeError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Bad page access\" in error_record.error_message, \"error_message should contain the error\"\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        # Check for compare_from_dicts log entry with project_id in extra data\n        compare_logs = [r for r in caplog.records if \"compare_from_dicts\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare_from_dicts log entry\"\n\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"dict-project\", \"project_id should match\"\n","structuredPatch":[{"oldStart":1116,"oldLines":9,"newStart":1116,"newLines":8,"lines":["         \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"","         import logging"," ","-        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):","-            with pytest.raises(ValueError):","-                ChangeDetector(new_page_threshold=-5)","+        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):","+            ChangeDetector(new_page_threshold=-5)"," ","         # Check for validation failure log","         warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Y16WqxzsxhPBN9tCkBW7xa","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XRzP5176Ay84WVFKY2KM6A","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)","new_string":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):\n            ChangeDetector(change_percentage_threshold=2.0)"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":272,"cache_read_input_tokens":103641,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":272},"output_tokens":24,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"db2261df-febc-490d-93a9-677b54fb6edb"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XRzP5176Ay84WVFKY2KM6A","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"57247d9e-e348-4b89-9d9b-0a9311ff29ad","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)","newString":"        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):\n            ChangeDetector(change_percentage_threshold=2.0)","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):\n            ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            with pytest.raises(ValueError):\n                ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"\n        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"\n        assert slow_record.page_id == \"slow-page\", \"page_id should match\"\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-project\", \"project_id should match\"\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Content hash computation failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-proj\", \"project_id should match\"\n        assert hasattr(error_record, \"page_id\"), \"Log record should have page_id\"\n        assert error_record.page_id == \"err-page\", \"page_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"ValueError\", \"error_type should be ValueError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Mocked hash error\" in error_record.error_message, \"error_message should contain the error\"\n        # Stack trace should be included\n        assert hasattr(error_record, \"stack_trace\"), \"Log record should have stack_trace\"\n        assert \"Traceback\" in error_record.stack_trace, \"stack_trace should contain traceback\"\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Change detection failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-project\", \"project_id should match\"\n        assert hasattr(error_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert error_record.crawl_id == \"err-crawl\", \"crawl_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"RuntimeError\", \"error_type should be RuntimeError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Bad page access\" in error_record.error_message, \"error_message should contain the error\"\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        # Check for compare_from_dicts log entry with project_id in extra data\n        compare_logs = [r for r in caplog.records if \"compare_from_dicts\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare_from_dicts log entry\"\n\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"dict-project\", \"project_id should match\"\n","structuredPatch":[{"oldStart":1137,"oldLines":9,"newStart":1137,"newLines":8,"lines":["         \"\"\"Test that invalid percentage threshold is logged.\"\"\"","         import logging"," ","-        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):","-            with pytest.raises(ValueError):","-                ChangeDetector(change_percentage_threshold=2.0)","+        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):","+            ChangeDetector(change_percentage_threshold=2.0)"," ","         # Check for validation failure log","         warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_017L157DcC8Yk1dx8C3bnLEY","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CxxLxcgSDkESoCrmyjZHen","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","old_string":"    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC","new_string":"    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import UTC, datetime"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":268,"cache_read_input_tokens":103913,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":268},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"596af751-236b-497a-8537-10dccc1535b9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CxxLxcgSDkESoCrmyjZHen","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"6c9a183b-c35d-49d4-9930-136cd3a04eef","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/backend/tests/utils/test_change_detection.py","oldString":"    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC","newString":"    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import UTC, datetime","originalFile":"\"\"\"Unit tests for change detection algorithm.\n\nTests cover:\n- ContentHasher: Content hash computation with various inputs\n- ChangeDetector: Page comparison and change detection\n- ChangeSummary: Summary generation and significance detection\n- Edge cases: Empty inputs, None values, large datasets\n\"\"\"\n\nimport pytest\n\nfrom app.utils.change_detection import (\n    VALID_CHANGE_TYPES,\n    ChangeDetector,\n    ChangeSummary,\n    ChangeType,\n    ContentHasher,\n    PageChange,\n    PageSnapshot,\n    compute_content_hash,\n    detect_changes,\n    get_change_detector,\n    get_content_hasher,\n)\n\n\nclass TestContentHasher:\n    \"\"\"Tests for ContentHasher class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        hasher = ContentHasher()\n        assert hasher.max_content_length == 5000\n\n    def test_init_custom_length(self) -> None:\n        \"\"\"Test initialization with custom content length.\"\"\"\n        hasher = ContentHasher(max_content_length=1000)\n        assert hasher.max_content_length == 1000\n\n    def test_compute_hash_all_fields(self) -> None:\n        \"\"\"Test hash computation with all fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body content\",\n        )\n\n        assert result is not None\n        assert len(result) == 32  # MD5 hex digest length\n        assert result.isalnum()\n\n    def test_compute_hash_empty_fields(self) -> None:\n        \"\"\"Test hash computation with empty fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash()\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_none_fields(self) -> None:\n        \"\"\"Test hash computation with None fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=None,\n            h1=None,\n            meta_description=None,\n            body_text=None,\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_partial_fields(self) -> None:\n        \"\"\"Test hash computation with some fields.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_consistency(self) -> None:\n        \"\"\"Test that same input produces same hash.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n        hash2 = hasher.compute_hash(\n            title=\"Title\",\n            h1=\"Heading\",\n            meta_description=\"Description\",\n            body_text=\"Body\",\n        )\n\n        assert hash1 == hash2\n\n    def test_compute_hash_different_inputs(self) -> None:\n        \"\"\"Test that different inputs produce different hashes.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Title A\")\n        hash2 = hasher.compute_hash(title=\"Title B\")\n\n        assert hash1 != hash2\n\n    def test_compute_hash_whitespace_normalization(self) -> None:\n        \"\"\"Test whitespace normalization in hashing.\"\"\"\n        hasher = ContentHasher(include_whitespace=False)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_case_normalization(self) -> None:\n        \"\"\"Test case normalization in hashing.\"\"\"\n        hasher = ContentHasher()\n        hash1 = hasher.compute_hash(title=\"Test Title\")\n        hash2 = hasher.compute_hash(title=\"test title\")\n\n        assert hash1 == hash2\n\n    def test_compute_hash_body_truncation(self) -> None:\n        \"\"\"Test body text truncation.\"\"\"\n        hasher = ContentHasher(max_content_length=100)\n        long_body = \"x\" * 1000\n\n        # Hash with truncated body\n        hash1 = hasher.compute_hash(body_text=long_body)\n\n        # Hash with exactly max length body\n        hash2 = hasher.compute_hash(body_text=\"x\" * 100)\n\n        assert hash1 == hash2\n\n    def test_compute_hash_with_ids(self) -> None:\n        \"\"\"Test hash computation with logging IDs.\"\"\"\n        hasher = ContentHasher()\n        result = hasher.compute_hash(\n            title=\"Test\",\n            project_id=\"proj-123\",\n            page_id=\"page-456\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict(self) -> None:\n        \"\"\"Test hash computation from dictionary.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"h1\": \"Test Heading\",\n            \"meta_description\": \"Test description\",\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result == expected\n\n    def test_compute_hash_from_dict_alternative_keys(self) -> None:\n        \"\"\"Test hash computation from dict with alternative keys.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"page_title\": \"Test Title\",\n            \"heading\": \"Test Heading\",\n            \"description\": \"Test description\",\n            \"content\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_from_dict_with_headings_list(self) -> None:\n        \"\"\"Test hash from dict with headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [\"First Heading\", \"Second Heading\"],\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_compute_hash_preserves_whitespace_when_enabled(self) -> None:\n        \"\"\"Test that whitespace is preserved when include_whitespace=True.\"\"\"\n        hasher = ContentHasher(include_whitespace=True)\n        hash1 = hasher.compute_hash(title=\"Test  Title\")\n        hash2 = hasher.compute_hash(title=\"Test Title\")\n\n        assert hash1 != hash2\n\n\nclass TestPageSnapshot:\n    \"\"\"Tests for PageSnapshot dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic PageSnapshot creation.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n        )\n\n        assert snapshot.url == \"https://example.com/page\"\n        assert snapshot.content_hash == \"abc123\"\n        assert snapshot.title is None\n        assert snapshot.page_id is None\n\n    def test_full_creation(self) -> None:\n        \"\"\"Test PageSnapshot with all fields.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        assert snapshot.title == \"Test Page\"\n        assert snapshot.page_id == \"page-456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=\"Test Page\",\n            page_id=\"page-456\",\n        )\n\n        result = snapshot.to_dict()\n\n        assert result[\"url\"] == \"https://example.com/page\"\n        assert result[\"content_hash\"] == \"abc123\"\n        assert result[\"title\"] == \"Test Page\"\n        assert result[\"page_id\"] == \"page-456\"\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in dict.\"\"\"\n        long_title = \"x\" * 200\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"abc123\",\n            title=long_title,\n        )\n\n        result = snapshot.to_dict()\n\n        assert len(result[\"title\"]) == 100\n\n\nclass TestPageChange:\n    \"\"\"Tests for PageChange dataclass.\"\"\"\n\n    def test_new_page_change(self) -> None:\n        \"\"\"Test PageChange for new page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/new\",\n            change_type=ChangeType.NEW,\n            new_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.NEW\n        assert change.old_hash is None\n        assert change.new_hash == \"abc123\"\n\n    def test_removed_page_change(self) -> None:\n        \"\"\"Test PageChange for removed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/removed\",\n            change_type=ChangeType.REMOVED,\n            old_hash=\"abc123\",\n        )\n\n        assert change.change_type == ChangeType.REMOVED\n        assert change.old_hash == \"abc123\"\n        assert change.new_hash is None\n\n    def test_changed_page_change(self) -> None:\n        \"\"\"Test PageChange for changed page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/changed\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old123\",\n            new_hash=\"new456\",\n        )\n\n        assert change.change_type == ChangeType.CHANGED\n        assert change.old_hash == \"old123\"\n        assert change.new_hash == \"new456\"\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.CHANGED,\n            old_hash=\"old\",\n            new_hash=\"new\",\n            title=\"Test\",\n            page_id=\"page-123\",\n        )\n\n        result = change.to_dict()\n\n        assert result[\"change_type\"] == \"changed\"\n        assert result[\"url\"] == \"https://example.com/page\"\n\n\nclass TestChangeSummary:\n    \"\"\"Tests for ChangeSummary dataclass.\"\"\"\n\n    def test_basic_creation(self) -> None:\n        \"\"\"Test basic ChangeSummary creation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"crawl-new\",\n            compared_to=\"crawl-old\",\n        )\n\n        assert summary.crawl_id == \"crawl-new\"\n        assert summary.compared_to == \"crawl-old\"\n        assert summary.computed_at is not None\n\n    def test_total_pages(self) -> None:\n        \"\"\"Test total_pages property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            changed_pages=3,\n            unchanged_pages=10,\n        )\n\n        assert summary.total_pages == 18  # 5 + 3 + 10\n\n    def test_total_changes(self) -> None:\n        \"\"\"Test total_changes property.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=3,\n        )\n\n        assert summary.total_changes == 10  # 5 + 2 + 3\n\n    def test_change_percentage(self) -> None:\n        \"\"\"Test change_percentage calculation.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=10,\n            unchanged_pages=90,\n        )\n\n        assert summary.change_percentage == 0.1  # 10%\n\n    def test_change_percentage_zero_pages(self) -> None:\n        \"\"\"Test change_percentage with no comparable pages.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            changed_pages=0,\n            unchanged_pages=0,\n        )\n\n        assert summary.change_percentage == 0.0\n\n    def test_to_dict(self) -> None:\n        \"\"\"Test conversion to dictionary.\"\"\"\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_pages=5,\n            removed_pages=2,\n            changed_pages=10,\n            unchanged_pages=83,\n            new_page_urls=[\"url1\", \"url2\"],\n            removed_page_urls=[\"url3\"],\n            changed_page_urls=[\"url4\", \"url5\"],\n            is_significant=True,\n        )\n\n        result = summary.to_dict()\n\n        assert result[\"crawl_id\"] == \"new\"\n        assert result[\"compared_to\"] == \"old\"\n        assert result[\"summary\"][\"new_pages\"] == 5\n        assert result[\"summary\"][\"removed_pages\"] == 2\n        assert result[\"summary\"][\"changed_pages\"] == 10\n        assert result[\"summary\"][\"unchanged_pages\"] == 83\n        assert result[\"is_significant\"] is True\n        assert result[\"total_pages\"] == 98\n        assert result[\"total_changes\"] == 17\n\n    def test_to_dict_truncates_urls(self) -> None:\n        \"\"\"Test that URL lists are truncated in dict.\"\"\"\n        urls = [f\"url{i}\" for i in range(200)]\n        summary = ChangeSummary(\n            crawl_id=\"new\",\n            compared_to=\"old\",\n            new_page_urls=urls,\n        )\n\n        result = summary.to_dict()\n\n        assert len(result[\"new_page_urls\"]) == 100\n\n\nclass TestChangeDetector:\n    \"\"\"Tests for ChangeDetector class.\"\"\"\n\n    def test_init_default(self) -> None:\n        \"\"\"Test default initialization.\"\"\"\n        detector = ChangeDetector()\n        assert detector.new_page_threshold == 5\n        assert detector.change_percentage_threshold == 0.10\n\n    def test_init_custom_thresholds(self) -> None:\n        \"\"\"Test initialization with custom thresholds.\"\"\"\n        detector = ChangeDetector(\n            new_page_threshold=10,\n            change_percentage_threshold=0.20,\n        )\n\n        assert detector.new_page_threshold == 10\n        assert detector.change_percentage_threshold == 0.20\n\n    def test_init_invalid_new_page_threshold(self) -> None:\n        \"\"\"Test initialization with invalid new_page_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"non-negative\"):\n            ChangeDetector(new_page_threshold=-1)\n\n    def test_init_invalid_change_percentage_threshold(self) -> None:\n        \"\"\"Test initialization with invalid change_percentage_threshold.\"\"\"\n        with pytest.raises(ValueError, match=\"between 0.0 and 1.0\"):\n            ChangeDetector(change_percentage_threshold=1.5)\n\n    def test_compare_empty_crawls(self) -> None:\n        \"\"\"Test comparison of empty crawls.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 0\n\n    def test_compare_all_new_pages(self) -> None:\n        \"\"\"Test comparison where all pages are new.\"\"\"\n        detector = ChangeDetector()\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.new_pages == 2\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert \"url1\" in result.new_page_urls\n        assert \"url2\" in result.new_page_urls\n\n    def test_compare_all_removed_pages(self) -> None:\n        \"\"\"Test comparison where all pages are removed.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=[],\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 2\n        assert \"url1\" in result.removed_page_urls\n        assert \"url2\" in result.removed_page_urls\n\n    def test_compare_unchanged_pages(self) -> None:\n        \"\"\"Test comparison with unchanged pages.\"\"\"\n        detector = ChangeDetector()\n\n        pages = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=pages,\n            current_pages=pages,\n        )\n\n        assert result.new_pages == 0\n        assert result.removed_pages == 0\n        assert result.changed_pages == 0\n        assert result.unchanged_pages == 2\n\n    def test_compare_changed_pages(self) -> None:\n        \"\"\"Test comparison with changed pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2-modified\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 2\n        assert result.unchanged_pages == 0\n        assert \"url1\" in result.changed_page_urls\n        assert \"url2\" in result.changed_page_urls\n\n    def test_compare_mixed_changes(self) -> None:\n        \"\"\"Test comparison with mixed changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 1\n        assert \"new\" in result.new_page_urls\n        assert \"removed\" in result.removed_page_urls\n        assert \"changed\" in result.changed_page_urls\n\n    def test_significance_by_new_pages(self) -> None:\n        \"\"\"Test significance detection by new page count.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(5)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is True\n\n    def test_significance_below_new_page_threshold(self) -> None:\n        \"\"\"Test non-significance when below new page threshold.\"\"\"\n        detector = ChangeDetector(new_page_threshold=5)\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(4)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=current,\n        )\n\n        assert result.is_significant is False\n\n    def test_significance_by_change_percentage(self) -> None:\n        \"\"\"Test significance detection by change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.10)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% = threshold\n        assert result.is_significant is True\n\n    def test_significance_below_change_percentage(self) -> None:\n        \"\"\"Test non-significance when below change percentage.\"\"\"\n        detector = ChangeDetector(change_percentage_threshold=0.20)\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i == 0 else f\"hash{i}\")\n            for i in range(10)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # 1 changed out of 10 = 10% < 20% threshold\n        assert result.is_significant is False\n\n    def test_compare_with_project_id(self) -> None:\n        \"\"\"Test comparison with project_id for logging.\"\"\"\n        detector = ChangeDetector()\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=[],\n            current_pages=[],\n            project_id=\"proj-123\",\n        )\n\n        assert result.crawl_id == \"new\"\n\n    def test_compare_from_dicts(self) -> None:\n        \"\"\"Test comparison from dictionary representations.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\", \"title\": \"Page 1\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url2\", \"content_hash\": \"hash2\", \"title\": \"Page 2\", \"id\": \"id2\"},\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1-modified\", \"title\": \"Page 1 Updated\", \"id\": \"id1\"},\n            {\"normalized_url\": \"url3\", \"content_hash\": \"hash3\", \"title\": \"Page 3\", \"id\": \"id3\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.removed_pages == 1\n        assert result.changed_pages == 1\n        assert result.unchanged_pages == 0\n\n    def test_compare_from_dicts_custom_keys(self) -> None:\n        \"\"\"Test comparison from dicts with custom keys.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"url\": \"url1\", \"hash\": \"hash1\"},\n        ]\n\n        current = [\n            {\"url\": \"url1\", \"hash\": \"hash1-modified\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n            url_key=\"url\",\n            hash_key=\"hash\",\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_changes_list(self) -> None:\n        \"\"\"Test that changes list contains all changes.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2\"),\n            PageSnapshot(url=\"removed\", content_hash=\"hash3\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"unchanged\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"changed\", content_hash=\"hash2-modified\"),\n            PageSnapshot(url=\"new\", content_hash=\"hash4\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Should have 4 changes (1 new, 1 removed, 1 changed, 1 unchanged)\n        assert len(result.changes) == 4\n\n        # Check change types\n        change_types = {c.change_type for c in result.changes}\n        assert ChangeType.NEW in change_types\n        assert ChangeType.REMOVED in change_types\n        assert ChangeType.CHANGED in change_types\n        assert ChangeType.UNCHANGED in change_types\n\n    def test_compare_none_hash_treated_as_change(self) -> None:\n        \"\"\"Test that None hash vs non-None hash is detected as change.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 1\n\n    def test_compare_both_none_hash_is_unchanged(self) -> None:\n        \"\"\"Test that both None hashes is treated as unchanged.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=None),\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.unchanged_pages == 1\n\n\nclass TestConvenienceFunctions:\n    \"\"\"Tests for convenience functions.\"\"\"\n\n    def test_get_content_hasher_singleton(self) -> None:\n        \"\"\"Test that get_content_hasher returns singleton.\"\"\"\n        hasher1 = get_content_hasher()\n        hasher2 = get_content_hasher()\n\n        assert hasher1 is hasher2\n\n    def test_get_change_detector_singleton(self) -> None:\n        \"\"\"Test that get_change_detector returns singleton.\"\"\"\n        detector1 = get_change_detector()\n        detector2 = get_change_detector()\n\n        assert detector1 is detector2\n\n    def test_compute_content_hash(self) -> None:\n        \"\"\"Test compute_content_hash function.\"\"\"\n        result = compute_content_hash(\n            title=\"Test Title\",\n            h1=\"Test Heading\",\n            meta_description=\"Test description\",\n            body_text=\"Test body\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_detect_changes(self) -> None:\n        \"\"\"Test detect_changes function.\"\"\"\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\"),\n            PageSnapshot(url=\"url2\", content_hash=\"hash2\"),\n        ]\n\n        result = detect_changes(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.new_pages == 1\n        assert result.changed_pages == 1\n\n\nclass TestChangeType:\n    \"\"\"Tests for ChangeType enum.\"\"\"\n\n    def test_valid_change_types(self) -> None:\n        \"\"\"Test VALID_CHANGE_TYPES constant.\"\"\"\n        assert \"new\" in VALID_CHANGE_TYPES\n        assert \"removed\" in VALID_CHANGE_TYPES\n        assert \"changed\" in VALID_CHANGE_TYPES\n        assert \"unchanged\" in VALID_CHANGE_TYPES\n\n    def test_change_type_values(self) -> None:\n        \"\"\"Test ChangeType enum values.\"\"\"\n        assert ChangeType.NEW.value == \"new\"\n        assert ChangeType.REMOVED.value == \"removed\"\n        assert ChangeType.CHANGED.value == \"changed\"\n        assert ChangeType.UNCHANGED.value == \"unchanged\"\n\n\nclass TestEdgeCases:\n    \"\"\"Tests for edge cases and boundary conditions.\"\"\"\n\n    def test_large_number_of_pages(self) -> None:\n        \"\"\"Test comparison with large number of pages.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        current = [\n            PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}-modified\" if i < 100 else f\"hash{i}\")\n            for i in range(1000)\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        assert result.changed_pages == 100\n        assert result.unchanged_pages == 900\n\n    def test_unicode_content_hash(self) -> None:\n        \"\"\"Test hash computation with unicode content.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"日本語タイトル\",\n            h1=\"中文标题\",\n            meta_description=\"Descripción en español\",\n            body_text=\"Контент на русском языке\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_special_characters_in_content(self) -> None:\n        \"\"\"Test hash computation with special characters.\"\"\"\n        hasher = ContentHasher()\n\n        result = hasher.compute_hash(\n            title=\"Title with <html> & \\\"quotes\\\"\",\n            body_text=\"Body with\\n\\ttabs and newlines\",\n        )\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_duplicate_urls_in_pages(self) -> None:\n        \"\"\"Test comparison handles duplicate URLs (last one wins).\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1\"),\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Duplicate\n        ]\n\n        current = [\n            PageSnapshot(url=\"url1\", content_hash=\"hash1-dup\"),  # Matches second\n        ]\n\n        result = detector.compare(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Last duplicate wins, so should be unchanged\n        assert result.unchanged_pages == 1\n\n    def test_empty_url_filtered_in_compare_from_dicts(self) -> None:\n        \"\"\"Test that empty URLs are filtered in compare_from_dicts.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"normalized_url\": \"\", \"content_hash\": \"hash2\"},  # Empty URL\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Empty URL page should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n\nclass TestErrorLogging:\n    \"\"\"Tests for ERROR LOGGING REQUIREMENTS verification.\n\n    ERROR LOGGING REQUIREMENTS:\n    - Ensure test failures include full assertion context\n    - Log test setup/teardown at DEBUG level\n    - Capture and display logs from failed tests\n    - Include timing information in test reports\n    - Log mock/stub invocations for debugging\n\n    Note: The logging module uses `extra={}` dict for structured logging fields.\n    These fields are stored on the LogRecord object but not rendered in simple\n    text output. Tests check LogRecord attributes directly via record.__dict__.\n    \"\"\"\n\n    def test_content_hasher_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ContentHasher logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"proj-123\", page_id=\"page-456\")\n\n        # Check for method entry log\n        debug_logs = [r for r in caplog.records if r.levelno == logging.DEBUG]\n        assert len(debug_logs) >= 2, \"Should have at least 2 debug log entries (init + compute_hash)\"\n\n        # Check for entity IDs in log records' extra data\n        compute_hash_logs = [r for r in debug_logs if \"compute_hash\" in r.message]\n        assert len(compute_hash_logs) >= 1, \"Should have compute_hash log entry\"\n\n        # Extra fields are stored directly on the LogRecord\n        hash_record = compute_hash_logs[0]\n        assert hasattr(hash_record, \"project_id\"), \"Log record should have project_id\"\n        assert hash_record.project_id == \"proj-123\", \"project_id should be logged\"\n        assert hasattr(hash_record, \"page_id\"), \"Log record should have page_id\"\n        assert hash_record.page_id == \"page-456\", \"page_id should be logged\"\n\n    def test_content_hasher_logs_hash_computed_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that compute_hash logs completion with timing.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", body_text=\"Content\")\n\n        # Check for hash computed log\n        hash_computed_logs = [r for r in caplog.records if \"Content hash computed\" in r.message]\n        assert len(hash_computed_logs) >= 1, \"Should have 'Content hash computed' log entry\"\n\n        # Check for timing in log record's extra data\n        hash_record = hash_computed_logs[0]\n        assert hasattr(hash_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(hash_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_method_entry_debug(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that ChangeDetector logs method entry at DEBUG level.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[],\n                current_pages=[],\n                project_id=\"proj-789\",\n            )\n\n        # Check for method entry log\n        compare_logs = [r for r in caplog.records if \"compare() called\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare() called log entry\"\n\n        # Check for entity IDs in log record's extra data\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"proj-789\", \"project_id should be logged\"\n        assert hasattr(compare_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert compare_record.crawl_id == \"crawl-new\", \"crawl_id should be logged\"\n\n    def test_change_detector_logs_state_transition_info(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection completion is logged at INFO level (state transition).\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"crawl-new\",\n                previous_crawl_id=\"crawl-old\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1-modified\")],\n            )\n\n        # Check for INFO level completion log\n        info_logs = [r for r in caplog.records if r.levelno == logging.INFO]\n        assert len(info_logs) >= 1, \"Should have INFO log entry for state transition\"\n\n        log_text = caplog.text\n        assert \"Change detection completed\" in log_text\n\n    def test_change_detector_logs_with_timing(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that change detection logs include timing information.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.INFO, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n                current_pages=[\n                    PageSnapshot(url=f\"url{i}\", content_hash=f\"hash{i}\")\n                    for i in range(100)\n                ],\n            )\n\n        # Check for completion log with timing\n        completed_logs = [r for r in caplog.records if \"Change detection completed\" in r.message]\n        assert len(completed_logs) >= 1, \"Should have completion log\"\n\n        completed_record = completed_logs[0]\n        assert hasattr(completed_record, \"duration_ms\"), \"Log record should have duration_ms\"\n        assert isinstance(completed_record.duration_ms, float), \"duration_ms should be a float\"\n\n    def test_change_detector_logs_validation_failure(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that validation failures are logged with field names and rejected values.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):\n            ChangeDetector(new_page_threshold=-5)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text\n\n        # Check that field name and value are in log record's extra data\n        validation_record = warning_logs[0]\n        assert hasattr(validation_record, \"field\"), \"Log record should have field\"\n        assert validation_record.field == \"new_page_threshold\", \"field should be new_page_threshold\"\n        assert hasattr(validation_record, \"value\"), \"Log record should have value\"\n        assert validation_record.value == -5, \"value should be -5\"\n\n    def test_change_detector_logs_invalid_percentage_threshold(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test that invalid percentage threshold is logged.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"), pytest.raises(ValueError):\n            ChangeDetector(change_percentage_threshold=2.0)\n\n        # Check for validation failure log\n        warning_logs = [r for r in caplog.records if r.levelno == logging.WARNING]\n        assert len(warning_logs) >= 1, \"Should have WARNING log entry\"\n\n        log_text = caplog.text\n        assert \"Validation failed\" in log_text or \"change_percentage_threshold\" in log_text\n\n\nclass TestSlowOperationLogging:\n    \"\"\"Tests for slow operation warning logs (>1 second).\"\"\"\n\n    def test_slow_hash_computation_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow hash computation logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            hasher.compute_hash(title=\"Test\", project_id=\"slow-proj\", page_id=\"slow-page\")\n\n        log_text = caplog.text\n        assert \"Slow content hash computation\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow content hash\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-proj\", \"project_id should match\"\n        assert hasattr(slow_record, \"page_id\"), \"Log record should have page_id\"\n        assert slow_record.page_id == \"slow-page\", \"page_id should match\"\n\n    def test_slow_change_detection_logs_warning(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that slow change detection logs a warning.\n\n        Uses monkeypatch to simulate a slow operation by making time.monotonic\n        return values that indicate >1 second elapsed.\n        \"\"\"\n        import logging\n        import time\n\n        # Mock time.monotonic to return values indicating slow operation\n        call_count = 0\n\n        def mock_monotonic() -> float:\n            nonlocal call_count\n            call_count += 1\n            if call_count == 1:\n                return 0.0  # Start time\n            return 2.0  # End time (2 seconds elapsed, >1000ms threshold)\n\n        monkeypatch.setattr(time, \"monotonic\", mock_monotonic)\n\n        with caplog.at_level(logging.WARNING, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare(\n                crawl_id=\"slow-crawl\",\n                previous_crawl_id=\"prev-crawl\",\n                previous_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                current_pages=[PageSnapshot(url=\"url1\", content_hash=\"hash1\")],\n                project_id=\"slow-project\",\n            )\n\n        log_text = caplog.text\n        assert \"Slow change detection\" in log_text\n\n        # Check log record extra data for entity IDs\n        slow_logs = [r for r in caplog.records if \"Slow change detection\" in r.message]\n        assert len(slow_logs) >= 1, \"Should have slow operation log\"\n        slow_record = slow_logs[0]\n        assert hasattr(slow_record, \"project_id\"), \"Log record should have project_id\"\n        assert slow_record.project_id == \"slow-project\", \"project_id should match\"\n\n\nclass TestExceptionHandling:\n    \"\"\"Tests for exception handling paths.\"\"\"\n\n    def test_content_hasher_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compute_hash are logged with full context.\n\n        This tests the error path in ContentHasher.compute_hash() for exception handling.\n        \"\"\"\n        import hashlib\n        import logging\n\n        # Mock hashlib.md5 to raise an exception\n        def mock_md5(*args: object, **kwargs: object) -> None:\n            raise ValueError(\"Mocked hash error\")\n\n        monkeypatch.setattr(hashlib, \"md5\", mock_md5)\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            hasher = ContentHasher()\n            with pytest.raises(ValueError, match=\"Mocked hash error\"):\n                hasher.compute_hash(\n                    title=\"Test\",\n                    project_id=\"err-proj\",\n                    page_id=\"err-page\",\n                )\n\n        log_text = caplog.text\n        assert \"Content hash computation failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Content hash computation failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-proj\", \"project_id should match\"\n        assert hasattr(error_record, \"page_id\"), \"Log record should have page_id\"\n        assert error_record.page_id == \"err-page\", \"page_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"ValueError\", \"error_type should be ValueError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Mocked hash error\" in error_record.error_message, \"error_message should contain the error\"\n        # Stack trace should be included\n        assert hasattr(error_record, \"stack_trace\"), \"Log record should have stack_trace\"\n        assert \"Traceback\" in error_record.stack_trace, \"stack_trace should contain traceback\"\n\n    def test_change_detector_exception_logs_error(self, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\"Test that exceptions in compare() are logged with full context.\n\n        This tests the error path in ChangeDetector.compare() for exception handling.\n        \"\"\"\n        import logging\n\n        # Create a mock that raises when trying to access url attribute\n        class BadPage:\n            @property\n            def url(self) -> str:\n                raise RuntimeError(\"Bad page access\")\n\n            @property\n            def content_hash(self) -> str:\n                return \"hash\"\n\n        with caplog.at_level(logging.ERROR, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            with pytest.raises(RuntimeError, match=\"Bad page access\"):\n                detector.compare(\n                    crawl_id=\"err-crawl\",\n                    previous_crawl_id=\"prev\",\n                    previous_pages=[BadPage()],  # type: ignore[list-item]\n                    current_pages=[],\n                    project_id=\"err-project\",\n                )\n\n        log_text = caplog.text\n        assert \"Change detection failed\" in log_text\n\n        # Check log record extra data\n        error_logs = [r for r in caplog.records if \"Change detection failed\" in r.message]\n        assert len(error_logs) >= 1, \"Should have error log\"\n        error_record = error_logs[0]\n        assert hasattr(error_record, \"project_id\"), \"Log record should have project_id\"\n        assert error_record.project_id == \"err-project\", \"project_id should match\"\n        assert hasattr(error_record, \"crawl_id\"), \"Log record should have crawl_id\"\n        assert error_record.crawl_id == \"err-crawl\", \"crawl_id should match\"\n        assert hasattr(error_record, \"error_type\"), \"Log record should have error_type\"\n        assert error_record.error_type == \"RuntimeError\", \"error_type should be RuntimeError\"\n        assert hasattr(error_record, \"error_message\"), \"Log record should have error_message\"\n        assert \"Bad page access\" in error_record.error_message, \"error_message should contain the error\"\n\n\nclass TestSingletonReset:\n    \"\"\"Tests for singleton behavior with proper reset.\n\n    Following codebase patterns: reset module-level singleton before and after tests.\n    \"\"\"\n\n    def test_singleton_reset_hasher(self) -> None:\n        \"\"\"Test that hasher singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_hasher = None\n\n        # Get fresh instance\n        hasher1 = get_content_hasher()\n        assert hasher1 is not None\n\n        # Reset again\n        change_detection_module._default_hasher = None\n\n        # Get another fresh instance\n        hasher2 = get_content_hasher()\n        assert hasher2 is not None\n\n        # These should be different instances because we reset\n        # (But in normal operation they would be the same)\n        assert hasher1 is not hasher2\n\n        # Cleanup\n        change_detection_module._default_hasher = None\n\n    def test_singleton_reset_detector(self) -> None:\n        \"\"\"Test that detector singleton can be reset for isolated tests.\"\"\"\n        import app.utils.change_detection as change_detection_module\n\n        # Reset singleton\n        change_detection_module._default_detector = None\n\n        # Get fresh instance\n        detector1 = get_change_detector()\n        assert detector1 is not None\n\n        # Reset again\n        change_detection_module._default_detector = None\n\n        # Get another fresh instance\n        detector2 = get_change_detector()\n        assert detector2 is not None\n\n        # These should be different instances because we reset\n        assert detector1 is not detector2\n\n        # Cleanup\n        change_detection_module._default_detector = None\n\n\nclass TestChangeSummaryAdditional:\n    \"\"\"Additional tests for ChangeSummary to improve coverage.\"\"\"\n\n    def test_computed_at_default_is_utc(self) -> None:\n        \"\"\"Test that computed_at defaults to current UTC time.\"\"\"\n        from datetime import UTC\n\n        summary = ChangeSummary(crawl_id=\"test\", compared_to=\"old\")\n\n        assert summary.computed_at is not None\n        assert summary.computed_at.tzinfo is not None or summary.computed_at.tzinfo == UTC\n\n    def test_to_dict_with_none_computed_at(self) -> None:\n        \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"\n        from datetime import datetime, UTC\n\n        # Create with explicit computed_at\n        computed = datetime.now(UTC)\n        summary = ChangeSummary(\n            crawl_id=\"test\",\n            compared_to=\"old\",\n            computed_at=computed,\n        )\n\n        result = summary.to_dict()\n        assert result[\"computed_at\"] == computed.isoformat()\n\n\nclass TestPageChangeAdditional:\n    \"\"\"Additional tests for PageChange to improve coverage.\"\"\"\n\n    def test_unchanged_page_change(self) -> None:\n        \"\"\"Test PageChange for unchanged page.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/unchanged\",\n            change_type=ChangeType.UNCHANGED,\n            old_hash=\"same123\",\n            new_hash=\"same123\",\n            title=\"Unchanged Page\",\n            page_id=\"page-unchanged\",\n        )\n\n        assert change.change_type == ChangeType.UNCHANGED\n        assert change.old_hash == change.new_hash\n\n    def test_to_dict_truncates_long_title(self) -> None:\n        \"\"\"Test that long titles are truncated in PageChange.to_dict().\"\"\"\n        long_title = \"x\" * 200\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=long_title,\n        )\n\n        result = change.to_dict()\n        assert len(result[\"title\"]) == 100\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        change = PageChange(\n            url=\"https://example.com/page\",\n            change_type=ChangeType.NEW,\n            new_hash=\"hash123\",\n            title=None,\n        )\n\n        result = change.to_dict()\n        assert result[\"title\"] is None\n\n\nclass TestPageSnapshotAdditional:\n    \"\"\"Additional tests for PageSnapshot.\"\"\"\n\n    def test_to_dict_none_title(self) -> None:\n        \"\"\"Test to_dict with None title.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=\"hash123\",\n            title=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"title\"] is None\n\n    def test_none_content_hash(self) -> None:\n        \"\"\"Test PageSnapshot with None content_hash.\"\"\"\n        snapshot = PageSnapshot(\n            url=\"https://example.com/page\",\n            content_hash=None,\n        )\n\n        result = snapshot.to_dict()\n        assert result[\"content_hash\"] is None\n\n\nclass TestHashFromDictEdgeCases:\n    \"\"\"Additional edge case tests for compute_hash_from_dict.\"\"\"\n\n    def test_empty_headings_list(self) -> None:\n        \"\"\"Test hash from dict with empty headings list.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [],  # Empty list\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_headings_list_with_non_string(self) -> None:\n        \"\"\"Test hash from dict with non-string heading.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"headings\": [123, \"Second Heading\"],  # First is int\n            \"body_text\": \"Test body\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        assert result is not None\n        assert len(result) == 32\n\n    def test_fallback_to_markdown_for_body(self) -> None:\n        \"\"\"Test hash from dict using markdown key for body_text.\"\"\"\n        hasher = ContentHasher()\n\n        data = {\n            \"title\": \"Test Title\",\n            \"markdown\": \"# Markdown content\",\n        }\n\n        result = hasher.compute_hash_from_dict(data)\n\n        # Should match hash with body_text as markdown content\n        expected = hasher.compute_hash(\n            title=\"Test Title\",\n            body_text=\"# Markdown content\",\n        )\n\n        assert result == expected\n\n\nclass TestCompareFromDictsEdgeCases:\n    \"\"\"Additional edge case tests for compare_from_dicts.\"\"\"\n\n    def test_missing_url_key_filtered(self) -> None:\n        \"\"\"Test that pages without url_key are filtered.\"\"\"\n        detector = ChangeDetector()\n\n        previous = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n            {\"content_hash\": \"hash2\"},  # Missing url key\n        ]\n\n        current = [\n            {\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"},\n        ]\n\n        result = detector.compare_from_dicts(\n            crawl_id=\"new\",\n            previous_crawl_id=\"old\",\n            previous_pages=previous,\n            current_pages=current,\n        )\n\n        # Page without url should be filtered out\n        assert result.unchanged_pages == 1\n        assert result.removed_pages == 0\n\n    def test_with_project_id_logging(self, caplog: pytest.LogCaptureFixture) -> None:\n        \"\"\"Test compare_from_dicts logs with project_id.\"\"\"\n        import logging\n\n        with caplog.at_level(logging.DEBUG, logger=\"change_detection\"):\n            detector = ChangeDetector()\n            detector.compare_from_dicts(\n                crawl_id=\"new\",\n                previous_crawl_id=\"old\",\n                previous_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                current_pages=[{\"normalized_url\": \"url1\", \"content_hash\": \"hash1\"}],\n                project_id=\"dict-project\",\n            )\n\n        # Check for compare_from_dicts log entry with project_id in extra data\n        compare_logs = [r for r in caplog.records if \"compare_from_dicts\" in r.message]\n        assert len(compare_logs) >= 1, \"Should have compare_from_dicts log entry\"\n\n        compare_record = compare_logs[0]\n        assert hasattr(compare_record, \"project_id\"), \"Log record should have project_id\"\n        assert compare_record.project_id == \"dict-project\", \"project_id should match\"\n","structuredPatch":[{"oldStart":1390,"oldLines":7,"newStart":1390,"newLines":7,"lines":[" ","     def test_to_dict_with_none_computed_at(self) -> None:","         \"\"\"Test to_dict handles None computed_at gracefully.\"\"\"","-        from datetime import datetime, UTC","+        from datetime import UTC, datetime"," ","         # Create with explicit computed_at","         computed = datetime.now(UTC)"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01W9DECED74tEsqQnLxgeMF5","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013J9nT3eprjHGaZfHtdypdN","name":"Bash","input":{"command":"python3 -m ruff check tests/utils/test_change_detection.py 2>&1","description":"Re-run ruff linter on test file"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":271,"cache_read_input_tokens":104181,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":271},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"e8774dd9-4a1a-4664-ba33-e0268b27fd3f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013J9nT3eprjHGaZfHtdypdN","type":"tool_result","content":"All checks passed!","is_error":false}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"4540f43e-0ed0-4e06-9a02-3997b6d25bc2","tool_use_result":{"stdout":"All checks passed!","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Rhhsm7aQGZRHxr4osdS2bA","type":"message","role":"assistant","content":[{"type":"text","text":"Let me run the tests one more time to ensure everything still passes after the lint fixes:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":314,"cache_read_input_tokens":104452,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":314},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"85c62c93-d602-4f38-be09-e168fb8e3f52"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Rhhsm7aQGZRHxr4osdS2bA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MJ4dPWT4xajyHnS7B5USy8","name":"Bash","input":{"command":"python3 -m pytest tests/utils/test_change_detection.py -v --tb=short 2>&1 | tail -30","description":"Final test run after lint fixes"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":314,"cache_read_input_tokens":104452,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":314},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"9ae791e6-3286-49cb-940a-4a848d28234b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MJ4dPWT4xajyHnS7B5USy8","type":"tool_result","content":"tests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 68%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 70%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [ 71%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_method_entry_debug PASSED [ 72%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_hash_computed_debug PASSED [ 73%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_method_entry_debug PASSED [ 74%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_state_transition_info PASSED [ 75%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_with_timing PASSED [ 77%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_validation_failure PASSED [ 78%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_invalid_percentage_threshold PASSED [ 79%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_hash_computation_logs_warning PASSED [ 80%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_change_detection_logs_warning PASSED [ 81%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_content_hasher_exception_logs_error PASSED [ 82%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_change_detector_exception_logs_error PASSED [ 83%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_hasher PASSED [ 85%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_detector PASSED [ 86%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_computed_at_default_is_utc PASSED [ 87%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_to_dict_with_none_computed_at PASSED [ 88%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_unchanged_page_change PASSED [ 89%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_truncates_long_title PASSED [ 90%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_none_title PASSED [ 91%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_to_dict_none_title PASSED [ 93%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_none_content_hash PASSED [ 94%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_empty_headings_list PASSED [ 95%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_headings_list_with_non_string PASSED [ 96%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_fallback_to_markdown_for_body PASSED [ 97%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_missing_url_key_filtered PASSED [ 98%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging PASSED [100%]\n\n============================== 87 passed in 0.11s ==============================","is_error":false}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"81f7eef9-6a0b-4de4-b2b6-4264c0532ece","tool_use_result":{"stdout":"tests/utils/test_change_detection.py::TestEdgeCases::test_special_characters_in_content PASSED [ 68%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_duplicate_urls_in_pages PASSED [ 70%]\ntests/utils/test_change_detection.py::TestEdgeCases::test_empty_url_filtered_in_compare_from_dicts PASSED [ 71%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_method_entry_debug PASSED [ 72%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_content_hasher_logs_hash_computed_debug PASSED [ 73%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_method_entry_debug PASSED [ 74%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_state_transition_info PASSED [ 75%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_with_timing PASSED [ 77%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_validation_failure PASSED [ 78%]\ntests/utils/test_change_detection.py::TestErrorLogging::test_change_detector_logs_invalid_percentage_threshold PASSED [ 79%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_hash_computation_logs_warning PASSED [ 80%]\ntests/utils/test_change_detection.py::TestSlowOperationLogging::test_slow_change_detection_logs_warning PASSED [ 81%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_content_hasher_exception_logs_error PASSED [ 82%]\ntests/utils/test_change_detection.py::TestExceptionHandling::test_change_detector_exception_logs_error PASSED [ 83%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_hasher PASSED [ 85%]\ntests/utils/test_change_detection.py::TestSingletonReset::test_singleton_reset_detector PASSED [ 86%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_computed_at_default_is_utc PASSED [ 87%]\ntests/utils/test_change_detection.py::TestChangeSummaryAdditional::test_to_dict_with_none_computed_at PASSED [ 88%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_unchanged_page_change PASSED [ 89%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_truncates_long_title PASSED [ 90%]\ntests/utils/test_change_detection.py::TestPageChangeAdditional::test_to_dict_none_title PASSED [ 91%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_to_dict_none_title PASSED [ 93%]\ntests/utils/test_change_detection.py::TestPageSnapshotAdditional::test_none_content_hash PASSED [ 94%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_empty_headings_list PASSED [ 95%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_headings_list_with_non_string PASSED [ 96%]\ntests/utils/test_change_detection.py::TestHashFromDictEdgeCases::test_fallback_to_markdown_for_body PASSED [ 97%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_missing_url_key_filtered PASSED [ 98%]\ntests/utils/test_change_detection.py::TestCompareFromDictsEdgeCases::test_with_project_id_logging PASSED [100%]\n\n============================== 87 passed in 0.11s ==============================","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01CK3pM1onY7v8rZsYKQxTRm","type":"message","role":"assistant","content":[{"type":"text","text":"All 87 tests pass. Now let me update the todo list and progress file:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1219,"cache_read_input_tokens":104766,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1219},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"cbc9490c-dd27-484b-a605-dceeeaeaca5b"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01CK3pM1onY7v8rZsYKQxTRm","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Dva2BXsH3v43ZPrUE3i12z","name":"TodoWrite","input":{"todos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"completed","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"completed","activeForm":"Running quality checks"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1219,"cache_read_input_tokens":104766,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1219},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"4ecd75c6-0ce3-44cd-8175-9acbb2584789"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Dva2BXsH3v43ZPrUE3i12z","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"25a3c889-c1ed-4707-8ca8-e858dc6e8d28","tool_use_result":{"oldTodos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"completed","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"in_progress","activeForm":"Running quality checks"}],"newTodos":[{"content":"Review existing change detection tests for completeness","status":"completed","activeForm":"Reviewing existing change detection tests"},{"content":"Add tests for slow operation warning logging","status":"completed","activeForm":"Adding tests for slow operation warning logging"},{"content":"Add tests for exception handling paths","status":"completed","activeForm":"Adding tests for exception handling paths"},{"content":"Add tests to verify ERROR LOGGING REQUIREMENTS","status":"completed","activeForm":"Adding tests to verify ERROR LOGGING REQUIREMENTS"},{"content":"Run tests to verify all pass","status":"completed","activeForm":"Running tests to verify all pass"},{"content":"Run quality checks (typecheck, lint)","status":"completed","activeForm":"Running quality checks"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_019EByTKPX8RdvngBXPJw8yL","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01H1x64ewZc6w4urePKBee7b","name":"Read","input":{"file_path":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":302,"cache_read_input_tokens":105985,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":302},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"0a5ef268-0c58-4320-95be-f73afc6045b6"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01H1x64ewZc6w4urePKBee7b","type":"tool_result","content":"     1→# Ralph Progress Log\n     2→\n     3→This file tracks progress across iterations. Agents update this file\n     4→after each iteration and it's included in prompts for context.\n     5→\n     6→## Codebase Patterns (Study These First)\n     7→\n     8→### Testing Patterns\n     9→- **Test file structure**: Tests organized by service/util in `tests/services/` and `tests/utils/`\n    10→- **Fixture pattern**: Per-class fixtures with `@pytest.fixture`, mock external clients with `AsyncMock`\n    11→- **Async testing**: Use `@pytest.mark.asyncio` decorator (auto mode enabled in pyproject.toml)\n    12→- **Mocking strategy**: Patch at module import source (`app.services.xxx.get_xxx_service`) not at usage point\n    13→- **Singleton testing**: Reset module-level `_xxx_service = None` before and after tests\n    14→- **Error logging verification**: Use `caplog` fixture with `caplog.at_level(logging.DEBUG/INFO)`\n    15→- **Coverage target**: 80% minimum, tests achieve 87% overall\n    16→\n    17→- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n    18→\n    19→---\n    20→\n    21→## 2026-02-01 - client-onboarding-v2-c3y.72\n    22→- What was implemented: Comprehensive unit tests for brand config synthesis service\n    23→- Files changed:\n    24→  - `backend/tests/services/test_brand_config.py` - New test file with 50 tests covering:\n    25→    - SynthesisResult dataclass validation\n    26→    - Exception classes (BrandConfigServiceError, ValidationError, NotFoundError, SynthesisError)\n    27→    - Schema merging (_merge_v2_schemas) with partial overrides\n    28→    - Document parsing (_parse_documents) with base64, failures, exceptions\n    29→    - V2 schema synthesis (synthesize_v2_schema) success, validation, Claude errors, JSON parsing\n    30→    - Synthesize and save (synthesize_and_save) create/update flows, DB errors\n    31→    - CRUD operations (get, list, update, delete) with ownership validation\n    32→    - Service factory function (get_brand_config_service)\n    33→    - Service initialization and helper methods\n    34→- **Learnings:**\n    35→  - Patterns discovered:\n    36→    - Brand config service uses dependency injection for Claude client and document parser\n    37→    - Synthesis result includes token usage metrics (input_tokens, output_tokens, request_id)\n    38→    - Schema merging uses deep merge for nested dicts (colors, typography, etc.)\n    39→    - Test coverage achieved: 93% for app.services.brand_config\n    40→  - Gotchas encountered:\n    41→    - Service logs use `extra={\"filename\": ...}` which conflicts with Python's reserved LogRecord attribute\n    42→    - Must patch `app.services.brand_config.logger` in tests that trigger warning/debug paths using filename\n    43→    - BrandConfigSynthesisRequest requires brand_name, tests with empty string need to use pydantic validation path\n    44→    - Markdown code block handling (`\\`\\`\\`json`) is tested for Claude responses\n    45→\n    46→---\n    47→\n    48→## 2026-02-01 - client-onboarding-v2-c3y.63\n    49→- What was implemented: Verified and fixed existing comprehensive unit tests for fan-out and categorization logic\n    50→- Files changed:\n    51→  - `backend/tests/utils/test_url_categorizer.py` - Fixed edge case test for relative paths without leading `/`\n    52→- **Learnings:**\n    53→  - Patterns discovered:\n    54→    - Tests already existed and were comprehensive (208 total tests for these modules)\n    55→    - Fan-out tests in `test_paa_enrichment.py` - TestEnrichKeywordFanout class\n    56→    - Categorization tests in `test_category_service.py`, `test_paa_categorization.py`, `test_url_categorizer.py`\n    57→    - Test docstrings include ERROR LOGGING REQUIREMENTS verification notes\n    58→  - Gotchas encountered:\n    59→    - `urlparse` treats relative paths without leading `/` ambiguously - they should have `/` prefix\n    60→    - Use `python3` directly (no poetry wrapper needed in this environment)\n    61→    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n    62→---\n    63→\n    64→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"c0453ed5-3ff2-41d0-8507-646f2c24c8b8","tool_use_result":{"type":"text","file":{"filePath":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md","content":"# Ralph Progress Log\n\nThis file tracks progress across iterations. Agents update this file\nafter each iteration and it's included in prompts for context.\n\n## Codebase Patterns (Study These First)\n\n### Testing Patterns\n- **Test file structure**: Tests organized by service/util in `tests/services/` and `tests/utils/`\n- **Fixture pattern**: Per-class fixtures with `@pytest.fixture`, mock external clients with `AsyncMock`\n- **Async testing**: Use `@pytest.mark.asyncio` decorator (auto mode enabled in pyproject.toml)\n- **Mocking strategy**: Patch at module import source (`app.services.xxx.get_xxx_service`) not at usage point\n- **Singleton testing**: Reset module-level `_xxx_service = None` before and after tests\n- **Error logging verification**: Use `caplog` fixture with `caplog.at_level(logging.DEBUG/INFO)`\n- **Coverage target**: 80% minimum, tests achieve 87% overall\n\n- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.72\n- What was implemented: Comprehensive unit tests for brand config synthesis service\n- Files changed:\n  - `backend/tests/services/test_brand_config.py` - New test file with 50 tests covering:\n    - SynthesisResult dataclass validation\n    - Exception classes (BrandConfigServiceError, ValidationError, NotFoundError, SynthesisError)\n    - Schema merging (_merge_v2_schemas) with partial overrides\n    - Document parsing (_parse_documents) with base64, failures, exceptions\n    - V2 schema synthesis (synthesize_v2_schema) success, validation, Claude errors, JSON parsing\n    - Synthesize and save (synthesize_and_save) create/update flows, DB errors\n    - CRUD operations (get, list, update, delete) with ownership validation\n    - Service factory function (get_brand_config_service)\n    - Service initialization and helper methods\n- **Learnings:**\n  - Patterns discovered:\n    - Brand config service uses dependency injection for Claude client and document parser\n    - Synthesis result includes token usage metrics (input_tokens, output_tokens, request_id)\n    - Schema merging uses deep merge for nested dicts (colors, typography, etc.)\n    - Test coverage achieved: 93% for app.services.brand_config\n  - Gotchas encountered:\n    - Service logs use `extra={\"filename\": ...}` which conflicts with Python's reserved LogRecord attribute\n    - Must patch `app.services.brand_config.logger` in tests that trigger warning/debug paths using filename\n    - BrandConfigSynthesisRequest requires brand_name, tests with empty string need to use pydantic validation path\n    - Markdown code block handling (`\\`\\`\\`json`) is tested for Claude responses\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.63\n- What was implemented: Verified and fixed existing comprehensive unit tests for fan-out and categorization logic\n- Files changed:\n  - `backend/tests/utils/test_url_categorizer.py` - Fixed edge case test for relative paths without leading `/`\n- **Learnings:**\n  - Patterns discovered:\n    - Tests already existed and were comprehensive (208 total tests for these modules)\n    - Fan-out tests in `test_paa_enrichment.py` - TestEnrichKeywordFanout class\n    - Categorization tests in `test_category_service.py`, `test_paa_categorization.py`, `test_url_categorizer.py`\n    - Test docstrings include ERROR LOGGING REQUIREMENTS verification notes\n  - Gotchas encountered:\n    - `urlparse` treats relative paths without leading `/` ambiguously - they should have `/` prefix\n    - Use `python3` directly (no poetry wrapper needed in this environment)\n    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n","numLines":64,"startLine":1,"totalLines":64}}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01LsG47izCTgKJ6z4YFMJFNR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01K9YiVU3LosvmRsWpzTWnva","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md","old_string":"    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n","new_string":"    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.105\n- What was implemented: Comprehensive unit tests for change detection module\n- Files changed:\n  - `backend/tests/utils/test_change_detection.py` - Enhanced test file from 62 to 87 tests covering:\n    - ContentHasher: hash computation, normalization, truncation, dict parsing\n    - PageSnapshot, PageChange, ChangeSummary: dataclasses and serialization\n    - ChangeDetector: page comparison, significance detection, custom thresholds\n    - Convenience functions: singletons, compute_content_hash, detect_changes\n    - Edge cases: unicode, special chars, duplicates, large datasets\n    - ERROR LOGGING REQUIREMENTS: method entry/exit logging, entity IDs, timing\n    - Slow operation warnings: >1 second threshold logging with monkeypatch\n    - Exception handling: full error context logging with stack traces\n    - Singleton reset: module-level singleton isolation for tests\n- **Learnings:**\n  - Patterns discovered:\n    - Logging uses `extra={}` dict for structured fields - not in text output, check LogRecord attributes directly\n    - Use `hasattr(record, 'field')` and `record.field` to verify extra log data\n    - Monkeypatch `time.monotonic` to test slow operation warning paths\n    - Exception handling logs include `error_type`, `error_message`, `stack_trace` fields\n    - Test coverage achieved: 100% for app.utils.change_detection\n  - Gotchas encountered:\n    - caplog.text doesn't include extra={} fields - must check record.__dict__ or record.field_name\n    - Combine context managers: `with caplog.at_level(...), pytest.raises(...)` not nested\n    - Import order matters for ruff: `from datetime import UTC, datetime` (alphabetical)\n---\n\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1361,"cache_read_input_tokens":106287,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1361},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"aa6b1ce4-13b6-4731-99df-00fa33267de3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01K9YiVU3LosvmRsWpzTWnva","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"3a671bd2-4297-4dfc-8a36-026ea73ca491","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md","oldString":"    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n","newString":"    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.105\n- What was implemented: Comprehensive unit tests for change detection module\n- Files changed:\n  - `backend/tests/utils/test_change_detection.py` - Enhanced test file from 62 to 87 tests covering:\n    - ContentHasher: hash computation, normalization, truncation, dict parsing\n    - PageSnapshot, PageChange, ChangeSummary: dataclasses and serialization\n    - ChangeDetector: page comparison, significance detection, custom thresholds\n    - Convenience functions: singletons, compute_content_hash, detect_changes\n    - Edge cases: unicode, special chars, duplicates, large datasets\n    - ERROR LOGGING REQUIREMENTS: method entry/exit logging, entity IDs, timing\n    - Slow operation warnings: >1 second threshold logging with monkeypatch\n    - Exception handling: full error context logging with stack traces\n    - Singleton reset: module-level singleton isolation for tests\n- **Learnings:**\n  - Patterns discovered:\n    - Logging uses `extra={}` dict for structured fields - not in text output, check LogRecord attributes directly\n    - Use `hasattr(record, 'field')` and `record.field` to verify extra log data\n    - Monkeypatch `time.monotonic` to test slow operation warning paths\n    - Exception handling logs include `error_type`, `error_message`, `stack_trace` fields\n    - Test coverage achieved: 100% for app.utils.change_detection\n  - Gotchas encountered:\n    - caplog.text doesn't include extra={} fields - must check record.__dict__ or record.field_name\n    - Combine context managers: `with caplog.at_level(...), pytest.raises(...)` not nested\n    - Import order matters for ruff: `from datetime import UTC, datetime` (alphabetical)\n---\n\n","originalFile":"# Ralph Progress Log\n\nThis file tracks progress across iterations. Agents update this file\nafter each iteration and it's included in prompts for context.\n\n## Codebase Patterns (Study These First)\n\n### Testing Patterns\n- **Test file structure**: Tests organized by service/util in `tests/services/` and `tests/utils/`\n- **Fixture pattern**: Per-class fixtures with `@pytest.fixture`, mock external clients with `AsyncMock`\n- **Async testing**: Use `@pytest.mark.asyncio` decorator (auto mode enabled in pyproject.toml)\n- **Mocking strategy**: Patch at module import source (`app.services.xxx.get_xxx_service`) not at usage point\n- **Singleton testing**: Reset module-level `_xxx_service = None` before and after tests\n- **Error logging verification**: Use `caplog` fixture with `caplog.at_level(logging.DEBUG/INFO)`\n- **Coverage target**: 80% minimum, tests achieve 87% overall\n\n- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.72\n- What was implemented: Comprehensive unit tests for brand config synthesis service\n- Files changed:\n  - `backend/tests/services/test_brand_config.py` - New test file with 50 tests covering:\n    - SynthesisResult dataclass validation\n    - Exception classes (BrandConfigServiceError, ValidationError, NotFoundError, SynthesisError)\n    - Schema merging (_merge_v2_schemas) with partial overrides\n    - Document parsing (_parse_documents) with base64, failures, exceptions\n    - V2 schema synthesis (synthesize_v2_schema) success, validation, Claude errors, JSON parsing\n    - Synthesize and save (synthesize_and_save) create/update flows, DB errors\n    - CRUD operations (get, list, update, delete) with ownership validation\n    - Service factory function (get_brand_config_service)\n    - Service initialization and helper methods\n- **Learnings:**\n  - Patterns discovered:\n    - Brand config service uses dependency injection for Claude client and document parser\n    - Synthesis result includes token usage metrics (input_tokens, output_tokens, request_id)\n    - Schema merging uses deep merge for nested dicts (colors, typography, etc.)\n    - Test coverage achieved: 93% for app.services.brand_config\n  - Gotchas encountered:\n    - Service logs use `extra={\"filename\": ...}` which conflicts with Python's reserved LogRecord attribute\n    - Must patch `app.services.brand_config.logger` in tests that trigger warning/debug paths using filename\n    - BrandConfigSynthesisRequest requires brand_name, tests with empty string need to use pydantic validation path\n    - Markdown code block handling (`\\`\\`\\`json`) is tested for Claude responses\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.63\n- What was implemented: Verified and fixed existing comprehensive unit tests for fan-out and categorization logic\n- Files changed:\n  - `backend/tests/utils/test_url_categorizer.py` - Fixed edge case test for relative paths without leading `/`\n- **Learnings:**\n  - Patterns discovered:\n    - Tests already existed and were comprehensive (208 total tests for these modules)\n    - Fan-out tests in `test_paa_enrichment.py` - TestEnrichKeywordFanout class\n    - Categorization tests in `test_category_service.py`, `test_paa_categorization.py`, `test_url_categorizer.py`\n    - Test docstrings include ERROR LOGGING REQUIREMENTS verification notes\n  - Gotchas encountered:\n    - `urlparse` treats relative paths without leading `/` ambiguously - they should have `/` prefix\n    - Use `python3` directly (no poetry wrapper needed in this environment)\n    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n","structuredPatch":[{"oldStart":61,"oldLines":3,"newStart":61,"newLines":29,"lines":["     - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`"," ---"," ","+## 2026-02-01 - client-onboarding-v2-c3y.105","+- What was implemented: Comprehensive unit tests for change detection module","+- Files changed:","+  - `backend/tests/utils/test_change_detection.py` - Enhanced test file from 62 to 87 tests covering:","+    - ContentHasher: hash computation, normalization, truncation, dict parsing","+    - PageSnapshot, PageChange, ChangeSummary: dataclasses and serialization","+    - ChangeDetector: page comparison, significance detection, custom thresholds","+    - Convenience functions: singletons, compute_content_hash, detect_changes","+    - Edge cases: unicode, special chars, duplicates, large datasets","+    - ERROR LOGGING REQUIREMENTS: method entry/exit logging, entity IDs, timing","+    - Slow operation warnings: >1 second threshold logging with monkeypatch","+    - Exception handling: full error context logging with stack traces","+    - Singleton reset: module-level singleton isolation for tests","+- **Learnings:**","+  - Patterns discovered:","+    - Logging uses `extra={}` dict for structured fields - not in text output, check LogRecord attributes directly","+    - Use `hasattr(record, 'field')` and `record.field` to verify extra log data","+    - Monkeypatch `time.monotonic` to test slow operation warning paths","+    - Exception handling logs include `error_type`, `error_message`, `stack_trace` fields","+    - Test coverage achieved: 100% for app.utils.change_detection","+  - Gotchas encountered:","+    - caplog.text doesn't include extra={} fields - must check record.__dict__ or record.field_name","+    - Combine context managers: `with caplog.at_level(...), pytest.raises(...)` not nested","+    - Import order matters for ruff: `from datetime import UTC, datetime` (alphabetical)","+---","+"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_011U8E5ekndyHecKjNtvPSRZ","type":"message","role":"assistant","content":[{"type":"text","text":"Let me also add the new patterns I discovered to the Codebase Patterns section at the top:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":681,"cache_read_input_tokens":107648,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":681},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"a4036c6f-5d19-46d2-a06a-e4e2284ecbbe"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_011U8E5ekndyHecKjNtvPSRZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HWzUYMmcn2C9kLyyokwW3V","name":"Edit","input":{"replace_all":false,"file_path":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md","old_string":"- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs","new_string":"- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n- **Logging extra verification**: To check structured logging extra fields, use `hasattr(record, 'field')` and `record.field` on LogRecord objects - `caplog.text` doesn't include extra fields\n- **Slow operation testing**: Use `monkeypatch.setattr(time, \"monotonic\", mock_fn)` to test slow operation warning paths by simulating elapsed time >1 second"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":681,"cache_read_input_tokens":107648,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":681},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"e6733815-5ac7-472e-a02c-8cf7088322b8"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HWzUYMmcn2C9kLyyokwW3V","type":"tool_result","content":"The file /Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"6217acd4-fa40-4700-bb37-d7685a19ff2e","tool_use_result":{"filePath":"/Users/mike/Projects/client-onboarding-v2/.ralph-tui/progress.md","oldString":"- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs","newString":"- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n- **Logging extra verification**: To check structured logging extra fields, use `hasattr(record, 'field')` and `record.field` on LogRecord objects - `caplog.text` doesn't include extra fields\n- **Slow operation testing**: Use `monkeypatch.setattr(time, \"monotonic\", mock_fn)` to test slow operation warning paths by simulating elapsed time >1 second","originalFile":"# Ralph Progress Log\n\nThis file tracks progress across iterations. Agents update this file\nafter each iteration and it's included in prompts for context.\n\n## Codebase Patterns (Study These First)\n\n### Testing Patterns\n- **Test file structure**: Tests organized by service/util in `tests/services/` and `tests/utils/`\n- **Fixture pattern**: Per-class fixtures with `@pytest.fixture`, mock external clients with `AsyncMock`\n- **Async testing**: Use `@pytest.mark.asyncio` decorator (auto mode enabled in pyproject.toml)\n- **Mocking strategy**: Patch at module import source (`app.services.xxx.get_xxx_service`) not at usage point\n- **Singleton testing**: Reset module-level `_xxx_service = None` before and after tests\n- **Error logging verification**: Use `caplog` fixture with `caplog.at_level(logging.DEBUG/INFO)`\n- **Coverage target**: 80% minimum, tests achieve 87% overall\n\n- **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.72\n- What was implemented: Comprehensive unit tests for brand config synthesis service\n- Files changed:\n  - `backend/tests/services/test_brand_config.py` - New test file with 50 tests covering:\n    - SynthesisResult dataclass validation\n    - Exception classes (BrandConfigServiceError, ValidationError, NotFoundError, SynthesisError)\n    - Schema merging (_merge_v2_schemas) with partial overrides\n    - Document parsing (_parse_documents) with base64, failures, exceptions\n    - V2 schema synthesis (synthesize_v2_schema) success, validation, Claude errors, JSON parsing\n    - Synthesize and save (synthesize_and_save) create/update flows, DB errors\n    - CRUD operations (get, list, update, delete) with ownership validation\n    - Service factory function (get_brand_config_service)\n    - Service initialization and helper methods\n- **Learnings:**\n  - Patterns discovered:\n    - Brand config service uses dependency injection for Claude client and document parser\n    - Synthesis result includes token usage metrics (input_tokens, output_tokens, request_id)\n    - Schema merging uses deep merge for nested dicts (colors, typography, etc.)\n    - Test coverage achieved: 93% for app.services.brand_config\n  - Gotchas encountered:\n    - Service logs use `extra={\"filename\": ...}` which conflicts with Python's reserved LogRecord attribute\n    - Must patch `app.services.brand_config.logger` in tests that trigger warning/debug paths using filename\n    - BrandConfigSynthesisRequest requires brand_name, tests with empty string need to use pydantic validation path\n    - Markdown code block handling (`\\`\\`\\`json`) is tested for Claude responses\n\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.63\n- What was implemented: Verified and fixed existing comprehensive unit tests for fan-out and categorization logic\n- Files changed:\n  - `backend/tests/utils/test_url_categorizer.py` - Fixed edge case test for relative paths without leading `/`\n- **Learnings:**\n  - Patterns discovered:\n    - Tests already existed and were comprehensive (208 total tests for these modules)\n    - Fan-out tests in `test_paa_enrichment.py` - TestEnrichKeywordFanout class\n    - Categorization tests in `test_category_service.py`, `test_paa_categorization.py`, `test_url_categorizer.py`\n    - Test docstrings include ERROR LOGGING REQUIREMENTS verification notes\n  - Gotchas encountered:\n    - `urlparse` treats relative paths without leading `/` ambiguously - they should have `/` prefix\n    - Use `python3` directly (no poetry wrapper needed in this environment)\n    - Coverage module path syntax uses dots not slashes: `--cov=app.services.paa_enrichment`\n---\n\n## 2026-02-01 - client-onboarding-v2-c3y.105\n- What was implemented: Comprehensive unit tests for change detection module\n- Files changed:\n  - `backend/tests/utils/test_change_detection.py` - Enhanced test file from 62 to 87 tests covering:\n    - ContentHasher: hash computation, normalization, truncation, dict parsing\n    - PageSnapshot, PageChange, ChangeSummary: dataclasses and serialization\n    - ChangeDetector: page comparison, significance detection, custom thresholds\n    - Convenience functions: singletons, compute_content_hash, detect_changes\n    - Edge cases: unicode, special chars, duplicates, large datasets\n    - ERROR LOGGING REQUIREMENTS: method entry/exit logging, entity IDs, timing\n    - Slow operation warnings: >1 second threshold logging with monkeypatch\n    - Exception handling: full error context logging with stack traces\n    - Singleton reset: module-level singleton isolation for tests\n- **Learnings:**\n  - Patterns discovered:\n    - Logging uses `extra={}` dict for structured fields - not in text output, check LogRecord attributes directly\n    - Use `hasattr(record, 'field')` and `record.field` to verify extra log data\n    - Monkeypatch `time.monotonic` to test slow operation warning paths\n    - Exception handling logs include `error_type`, `error_message`, `stack_trace` fields\n    - Test coverage achieved: 100% for app.utils.change_detection\n  - Gotchas encountered:\n    - caplog.text doesn't include extra={} fields - must check record.__dict__ or record.field_name\n    - Combine context managers: `with caplog.at_level(...), pytest.raises(...)` not nested\n    - Import order matters for ruff: `from datetime import UTC, datetime` (alphabetical)\n---\n\n","structuredPatch":[{"oldStart":15,"oldLines":6,"newStart":15,"newLines":8,"lines":[" - **Coverage target**: 80% minimum, tests achieve 87% overall"," "," - **Logging extra key gotcha**: Service code using `extra={\"filename\": ...}` conflicts with Python logging's reserved LogRecord attributes - tests should patch the logger when testing paths that trigger these logs","+- **Logging extra verification**: To check structured logging extra fields, use `hasattr(record, 'field')` and `record.field` on LogRecord objects - `caplog.text` doesn't include extra fields","+- **Slow operation testing**: Use `monkeypatch.setattr(time, \"monotonic\", mock_fn)` to test slow operation warning paths by simulating elapsed time >1 second"," "," ---"," "]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01PJrW4zUHMEEAwmniFMvydD","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me verify the final test count and coverage one more time:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":375,"cache_read_input_tokens":108329,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":375},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"cd053c7d-839a-4e32-b39b-258215091783"}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01PJrW4zUHMEEAwmniFMvydD","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UikbBHWTGxcnBJc6KP9JSV","name":"Bash","input":{"command":"python3 -m pytest tests/utils/test_change_detection.py -v --cov=app.utils.change_detection --cov-report=term-missing 2>&1 | grep -E \"(passed|TOTAL|Name)\"","description":"Final test count and coverage verification"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":375,"cache_read_input_tokens":108329,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":375},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"cd658961-dee0-4f42-b0b8-fa7dad72bc63"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UikbBHWTGxcnBJc6KP9JSV","type":"tool_result","content":"Name                            Stmts   Miss  Cover   Missing\nTOTAL                             207      0   100%\n============================== 87 passed in 0.10s ==============================","is_error":false}]},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"cbaf3694-9095-4b45-a7ef-d47d9730c1b4","tool_use_result":{"stdout":"Name                            Stmts   Miss  Cover   Missing\nTOTAL                             207      0   100%\n============================== 87 passed in 0.10s ==============================","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-5-20251101","id":"msg_01SoBsJJLt6MbJiq5tipbgth","type":"message","role":"assistant","content":[{"type":"text","text":"The task is complete. Let me provide a summary:\n\n## Summary\n\n**Task: client-onboarding-v2-c3y.105 - Write unit tests for change detection**\n\n**What was implemented:**\n- Enhanced the existing test file `backend/tests/utils/test_change_detection.py` from 62 tests to 87 tests\n- Achieved **100% test coverage** (up from 96%)\n\n**New test classes added:**\n1. **TestErrorLogging** (7 tests) - Verifies ERROR LOGGING REQUIREMENTS:\n   - Method entry/exit logging at DEBUG level\n   - Entity IDs (project_id, page_id, crawl_id) in log records\n   - Timing information (duration_ms) in log records\n   - Validation failure logging with field names and rejected values\n   - State transitions logged at INFO level\n\n2. **TestSlowOperationLogging** (2 tests) - Tests slow operation warning paths:\n   - Slow content hash computation (>1 second threshold)\n   - Slow change detection (>1 second threshold)\n   - Uses monkeypatch to simulate elapsed time\n\n3. **TestExceptionHandling** (2 tests) - Tests exception handling with full context:\n   - ContentHasher.compute_hash() error path with stack trace\n   - ChangeDetector.compare() error path with entity IDs\n\n4. **TestSingletonReset** (2 tests) - Tests singleton behavior:\n   - ContentHasher singleton reset for isolated tests\n   - ChangeDetector singleton reset for isolated tests\n\n5. **Additional edge case classes** (12 tests) - Additional coverage:\n   - ChangeSummary, PageChange, PageSnapshot additional tests\n   - Hash from dict edge cases (empty headings, non-string, markdown fallback)\n   - compare_from_dicts edge cases (missing url key, logging)\n\n**Files changed:**\n- `backend/tests/utils/test_change_detection.py` - Added 25 new tests\n\n<promise>COMPLETE</promise>"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":197,"cache_read_input_tokens":108704,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":197},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","uuid":"5ab119e4-ceaf-401e-bfae-fa4169a70789"}
{"type":"result","subtype":"success","is_error":false,"duration_ms":306752,"duration_api_ms":296024,"num_turns":36,"result":"The task is complete. Let me provide a summary:\n\n## Summary\n\n**Task: client-onboarding-v2-c3y.105 - Write unit tests for change detection**\n\n**What was implemented:**\n- Enhanced the existing test file `backend/tests/utils/test_change_detection.py` from 62 tests to 87 tests\n- Achieved **100% test coverage** (up from 96%)\n\n**New test classes added:**\n1. **TestErrorLogging** (7 tests) - Verifies ERROR LOGGING REQUIREMENTS:\n   - Method entry/exit logging at DEBUG level\n   - Entity IDs (project_id, page_id, crawl_id) in log records\n   - Timing information (duration_ms) in log records\n   - Validation failure logging with field names and rejected values\n   - State transitions logged at INFO level\n\n2. **TestSlowOperationLogging** (2 tests) - Tests slow operation warning paths:\n   - Slow content hash computation (>1 second threshold)\n   - Slow change detection (>1 second threshold)\n   - Uses monkeypatch to simulate elapsed time\n\n3. **TestExceptionHandling** (2 tests) - Tests exception handling with full context:\n   - ContentHasher.compute_hash() error path with stack trace\n   - ChangeDetector.compare() error path with entity IDs\n\n4. **TestSingletonReset** (2 tests) - Tests singleton behavior:\n   - ContentHasher singleton reset for isolated tests\n   - ChangeDetector singleton reset for isolated tests\n\n5. **Additional edge case classes** (12 tests) - Additional coverage:\n   - ChangeSummary, PageChange, PageSnapshot additional tests\n   - Hash from dict edge cases (empty headings, non-string, markdown fallback)\n   - compare_from_dicts edge cases (missing url key, logging)\n\n**Files changed:**\n- `backend/tests/utils/test_change_detection.py` - Added 25 new tests\n\n<promise>COMPLETE</promise>","session_id":"68e7088e-8bc8-4723-a729-b2d4a1a42419","total_cost_usd":2.3844424999999996,"usage":{"input_tokens":34,"cache_creation_input_tokens":93634,"cache_read_input_tokens":2602640,"output_tokens":18989,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":93634,"ephemeral_5m_input_tokens":0}},"modelUsage":{"claude-opus-4-5-20251101":{"inputTokens":34,"outputTokens":18989,"cacheReadInputTokens":2602640,"cacheCreationInputTokens":93634,"webSearchRequests":0,"costUSD":2.3614274999999996,"contextWindow":200000,"maxOutputTokens":64000},"claude-haiku-4-5-20251001":{"inputTokens":21275,"outputTokens":348,"cacheReadInputTokens":0,"cacheCreationInputTokens":0,"webSearchRequests":0,"costUSD":0.023014999999999997,"contextWindow":200000,"maxOutputTokens":64000}},"permission_denials":[],"uuid":"1e1c97a9-8fb1-4473-9227-e7f7f4bcc133"}
